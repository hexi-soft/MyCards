The Linguist's Guide to Statistics 
Don't Panic 
Brigitte Krenn 
Universitat des Saarlandes 
FR 8.7, Computerlinguistik 
Postfach 1150 
D-66041 Saarbrucken 
Germany 
krenn@coli.uni-sb.de 
Christer Samuelsson 
Bell Laboratories 
600 Mountain Ave 
Room 2D-339 
Murray Hill, NJ 07974 
USA 
christer@research.bell-labs.com 
http://www.coli.uni-sb.de/fkrenn,christerg 
December 19, 1997

c  Brigitte Krenn and Christer Samuelsson, 1994, 1995, 1996, 1997.

i 
In a review of Eugene Charniak's book \Statistical Language Learning" in 
Computational Linguistics Vol. 21, No. 1 of March, 1995, David Magerman 
writes:
The $64,000 question in computational linguistics these days 
is: What should I read to learn about statistical natural language 
processing? I have been asked this question over and 
over, and each time I have given basically the same reply: there 
is no text that addresses this topic directly, and the best one 
can do is nd a good probability-theory textbook and a good 
information-theory textbook, and supplement those texts with 
an assortment of conference papers and journal articles. 
   
The overriding concern should be to learn (and teach) the mathematical 
underpinnings of the statistical techniques used in 
this eld. The eld of statistical NLP is very young, and the 
foundations are still being laid. Deep knowledge of the basic 
machinery is far more valuable than the details of the most 
recent unproven ideas. 
So let's get down to it!

ii

Contents 
1 Basic Statistics 1 
1.1 The Stability of the Relative Frequency : : : : : : : : : : : 1 
1.2 Elementary Probability Theory : : : : : : : : : : : : : : : : 1 
1.2.1 Sample Space : : : : : : : : : : : : : : : : : : : : : : 1 
1.2.2 Probability Measures : : : : : : : : : : : : : : : : : : 2 
1.2.3 Independence : : : : : : : : : : : : : : : : : : : : : : 3 
1.2.4 Conditional Probabilities : : : : : : : : : : : : : : : 3 
1.2.5 Bayesian Inversion : : : : : : : : : : : : : : : : : : : 4 
1.2.6 Partitions : : : : : : : : : : : : : : : : : : : : : : : : 5 
1.2.7 Combinatorics : : : : : : : : : : : : : : : : : : : : : 6 
1.3 Stochastic Variables : : : : : : : : : : : : : : : : : : : : : : 8 
1.3.1 Distribution Function : : : : : : : : : : : : : : : : : 8 
1.3.2 Discrete and Continuous Stochastic Variables : : : : 9 
1.3.3 Frequency Function : : : : : : : : : : : : : : : : : : 10 
1.3.4 Expectation Value : : : : : : : : : : : : : : : : : : : 12 
1.3.5 Variance : : : : : : : : : : : : : : : : : : : : : : : : : 13 
1.3.6 Moments and Moment-Generating Functions : : : : 14 
1.4 Two-dimensional Stochastic Variables : : : : : : : : : : : : 14 
1.4.1 Distribution Function : : : : : : : : : : : : : : : : : 14 
1.4.2 Frequency Function : : : : : : : : : : : : : : : : : : 15 
1.4.3 Independence : : : : : : : : : : : : : : : : : : : : : : 16 
1.4.4 Functions of Stochastic Variables : : : : : : : : : : : 16 
1.4.5 Higher Dimensions : : : : : : : : : : : : : : : : : : : 18 
1.5 Selected Probability Distributions : : : : : : : : : : : : : : : 18 
1.5.1 Binomial Distribution : : : : : : : : : : : : : : : : : 18 
1.5.2 Normal Distribution : : : : : : : : : : : : : : : : : : 20 
1.5.3 Other Distributions : : : : : : : : : : : : : : : : : : 23 
1.5.4 Distribution Tables : : : : : : : : : : : : : : : : : : : 24 
1.6 Some Theoretical Results : : : : : : : : : : : : : : : : : : : 25 
1.7 Estimation : : : : : : : : : : : : : : : : : : : : : : : : : : : 27 
1.7.1 Random Samples : : : : : : : : : : : : : : : : : : : : 27 
1.7.2 Estimators : : : : : : : : : : : : : : : : : : : : : : : 28 
1.7.3 Maximum-Likelihood Estimators : : : : : : : : : : : 29 
1.7.4 Sucient Statistic : : : : : : : : : : : : : : : : : : : 31 
1.7.5 Condence Intervals : : : : : : : : : : : : : : : : : : 32 
1.7.6 Hypothesis Testing and Signicance : : : : : : : : : 35 
1.8 Further Reading : : : : : : : : : : : : : : : : : : : : : : : : 37 
iii

iv CONTENTS 
2 Applied Statistics 39 
2.1 Markov Models : : : : : : : : : : : : : : : : : : : : : : : : : 39 
2.1.1 Stochastic Processes : : : : : : : : : : : : : : : : : : 39 
2.1.2 Markov Chains and the Markov Property : : : : : : 40 
2.1.3 Markov Models : : : : : : : : : : : : : : : : : : : : : 42 
2.1.4 Hidden Markov Models : : : : : : : : : : : : : : : : 43 
2.1.5 Calculating P(O) : : : : : : : : : : : : : : : : : : : : 44 
2.1.6 Finding the Optimal State Sequence : : : : : : : : : 47 
2.1.7 Parameter Estimation for HMMs : : : : : : : : : : : 48 
2.1.8 Further reading : : : : : : : : : : : : : : : : : : : : : 50 
2.2 Elementary Information Theory : : : : : : : : : : : : : : : : 50 
2.2.1 Entropy : : : : : : : : : : : : : : : : : : : : : : : : : 50 
2.2.2 Related Information Measures : : : : : : : : : : : : 52 
2.2.3 Noiseless Coding : : : : : : : : : : : : : : : : : : : : 55 
2.2.4 More on Information Theory : : : : : : : : : : : : : 59 
2.3 Multivariate Analysis : : : : : : : : : : : : : : : : : : : : : : 59 
2.4 Sparse Data : : : : : : : : : : : : : : : : : : : : : : : : : : : 59 
3 Basic Corpus Linguistics 61 
3.1 Empirical Evaluation : : : : : : : : : : : : : : : : : : : : : : 61 
3.1.1 Contingency Tables : : : : : : : : : : : : : : : : : : 61 
3.1.2 Important Measures on Contingency Tables : : : : : 62 
3.1.3 Extended Contingency Table Model : : : : : : : : : 63 
3.1.4 Measures, Extended : : : : : : : : : : : : : : : : : : 63 
3.1.5 Loose Attempts on Measuring System Eciency : : 64 
3.1.6 Empirical Evaluation of Part-of-Speech Taggers: A 
Case Study : : : : : : : : : : : : : : : : : : : : : : : 64 
3.2 Corpora : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 65 
3.2.1 Types of Corpora : : : : : : : : : : : : : : : : : : : : 66 
3.2.2 Test Suites versus Corpora : : : : : : : : : : : : : : 67 
3.2.3 Tokenization : : : : : : : : : : : : : : : : : : : : : : 68 
3.2.4 Training and Testing : : : : : : : : : : : : : : : : : : 69 
3.2.5 Tagsets : : : : : : : : : : : : : : : : : : : : : : : : : 69 
4 Stochastic Grammars 73 
4.1 Some Formal Language Theory : : : : : : : : : : : : : : : : 73 
4.1.1 Context-free Grammars : : : : : : : : : : : : : : : : 73 
4.1.2 Derivations : : : : : : : : : : : : : : : : : : : : : : : 74 
4.1.3 Trees : : : : : : : : : : : : : : : : : : : : : : : : : : : 74 
4.1.4 Parse Trees : : : : : : : : : : : : : : : : : : : : : : : 75 
4.2 Stochastic Context-free Grammars : : : : : : : : : : : : : : 76 
4.3 A Parser for SCFGs : : : : : : : : : : : : : : : : : : : : : : 78 
4.4 Parameter Estimation for SCFGs : : : : : : : : : : : : : : : 80 
4.4.1 The Inside and Outside Variables : : : : : : : : : : : 81 
4.4.2 Deriving the Reestimation Equations : : : : : : : : : 82 
4.5 Adding Probabilistic Context to SCFGs : : : : : : : : : : : 84 
4.6 Theoretical Probability Losses : : : : : : : : : : : : : : : : : 86 
4.7 Stochastic Tree-Substitution Grammars : : : : : : : : : : : 88 
4.8 Stochastic History-Based Grammars : : : : : : : : : : : : : 89 
4.9 Lexicalization of Stochastic Grammars : : : : : : : : : : : : 90 
4.9.1 Stochastic Dependency Grammar and Related Approaches 
: : : : : : : : : : : : : : : : : : : : : : : : : 90 
4.10 Probabilistic LR Parsing : : : : : : : : : : : : : : : : : : : : 92 
4.10.1 Basic LR Parsing : : : : : : : : : : : : : : : : : : : : 92

CONTENTS v 
4.10.2 LR-Parsed Example : : : : : : : : : : : : : : : : : : 93 
4.10.3 LR-Table Compilation : : : : : : : : : : : : : : : : : 95 
4.10.4 Generalized LR Parsing : : : : : : : : : : : : : : : : 96 
4.10.5 Adding Probabilities : : : : : : : : : : : : : : : : : : 97 
4.10.6 Probabilistic GLR Parsing : : : : : : : : : : : : : : : 98 
4.11 Scoring : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 99 
5 Selected Topics in Statistical NLP 101 
5.1 Statistical Part-of-Speech Tagging : : : : : : : : : : : : : : 101 
5.1.1 In short : : : : : : : : : : : : : : : : : : : : : : : : : 101 
5.1.2 Linguistic Background : : : : : : : : : : : : : : : : : 101 
5.1.3 Basic Statistical PoS tagging : : : : : : : : : : : : : 101 
5.1.4 Suggested Reading : : : : : : : : : : : : : : : : : : : 103 
5.2 Statistical Machine Translation : : : : : : : : : : : : : : : : 103 
5.2.1 In short : : : : : : : : : : : : : : : : : : : : : : : : : 103 
5.2.2 Suggested Reading : : : : : : : : : : : : : : : : : : : 104 
5.3 Statistical Language Learning : : : : : : : : : : : : : : : : : 104 
5.4 Structural Ambiguity and Semantic Classes : : : : : : : : : 104 
5.4.1 Linguistic Background : : : : : : : : : : : : : : : : : 104 
5.4.2 Association Models : : : : : : : : : : : : : : : : : : : 105 
5.4.3 Suggested Reading : : : : : : : : : : : : : : : : : : : 108 
5.5 Word Sense Disambiguation : : : : : : : : : : : : : : : : : : 108 
5.5.1 Phenomena : : : : : : : : : : : : : : : : : : : : : : : 108 
5.5.2 Parameter Estimation : : : : : : : : : : : : : : : : : 108 
5.5.3 Disambiguation Model : : : : : : : : : : : : : : : : : 108 
5.5.4 Suggested Reading : : : : : : : : : : : : : : : : : : : 109 
5.6 Lexical Knowledge Acquisition : : : : : : : : : : : : : : : : 109 
A Desiderata 111 
B Tools 113 
B.1 Simple Unix Commands : : : : : : : : : : : : : : : : : : : : 113 
B.2 Split up a text using tr : : : : : : : : : : : : : : : : : : : : : 114 
B.3 Sort word list: sort, uniq : : : : : : : : : : : : : : : : : : : : 114 
B.4 Merge counts for upper and lower case: tr, sort, uniq : : : : 115 
B.5 Count lines, words, characters: wc : : : : : : : : : : : : : : 115 
B.6 Display the rst n lines of a le: sed : : : : : : : : : : : : : 115 
B.7 Find lines: grep, egrep : : : : : : : : : : : : : : : : : : : : : 116 
B.8 n-grams: tail, paste : : : : : : : : : : : : : : : : : : : : : : : 116 
B.9 Manipulation of lines and elds: awk : : : : : : : : : : : : : 116 
C Some Calculus 119 
C.1 Numbers : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 119 
C.1.1 Natural Numbers : : : : : : : : : : : : : : : : : : : : 119 
C.1.2 Integers : : : : : : : : : : : : : : : : : : : : : : : : : 119 
C.1.3 Rational Numbers : : : : : : : : : : : : : : : : : : : 119 
C.1.4 Real Numbers : : : : : : : : : : : : : : : : : : : : : : 120 
C.1.5 Complex Numbers : : : : : : : : : : : : : : : : : : : 121 
C.1.6 Algebraic and Transcendental Numbers : : : : : : : 121 
C.2 The Very Basics : : : : : : : : : : : : : : : : : : : : : : : : 122 
C.2.1 Exponentials : : : : : : : : : : : : : : : : : : : : : : 122 
C.2.2 Roots : : : : : : : : : : : : : : : : : : : : : : : : : : 123 
C.2.3 The Exponential Function : : : : : : : : : : : : : : : 124 
C.2.4 Logarithms for Beginners : : : : : : : : : : : : : : : 125

vi CONTENTS 
C.2.5 Factorial : : : : : : : : : : : : : : : : : : : : : : : : : 127 
C.2.6 Sequences : : : : : : : : : : : : : : : : : : : : : : : : 128 
C.2.7 Series : : : : : : : : : : : : : : : : : : : : : : : : : : 129 
C.3 On the Number e, the Exponential Function and the Natural 
Logarithm : : : : : : : : : : : : : : : : : : : : : : : : : : : : 132 
D Tagsets 133 
D.1 Word Level Tagsets : : : : : : : : : : : : : : : : : : : : : : : 133 
D.1.1 Representation of Word Level Tags : : : : : : : : : : 133 
D.1.2 Mapping between Linguistic Descriptions and Tagsets 134 
D.1.3 Tagsets and the Representation of Ambiguity : : : : 134 
D.1.4 Minimal Criteria for the Development of PoS-Tagsets 134 
D.2 Tagsets for English Text Corpora : : : : : : : : : : : : : : : 135 
D.2.1 The Susanne Tagset : : : : : : : : : : : : : : : : : : 135 
D.2.2 The Penn Treebank : : : : : : : : : : : : : : : : : : 141 
D.2.3 The Constraint Grammar Tagset : : : : : : : : : : : 146 
D.3 Tagsets for German Text Corpora : : : : : : : : : : : : : : 152 
D.3.1 The Stuttgart-Tubingen Tagset : : : : : : : : : : : : 152 
E Optimization Theory, Wild-West Style 155 
E.1 Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : 155 
E.2 Constrained Optimization in Rn : : : : : : : : : : : : : : : 156 
E.3 Numerical Analysis : : : : : : : : : : : : : : : : : : : : : : : 158

Preface 
This compendium is the result of the authors' attempts at teaching courses 
on statistical approaches in Computational Linguistics and Natural Language 
Processing, and it is continuously evolving and undergoing revision. 
Although we have already put considerable eort into writing, correcting 
and updating this compendium, there are numerous errors and omissions in 
it that we hope to deal with in the nearest future. The most recent release 
of this compendium, in the form of a uuencoded gzipped PostScript le, 
stat_cl.ps.gz.uu, or as an ordinary PostScript le, stat_cl.ps, can be 
retrieved from one of the authors' WWW homepage at 
http://www.coli.uni-sb.de/fchrister,krenng 
The ambition is to cover most statistical, stochastic and probabilistic 
approaches in the eld. As will be obvious by inspecting the table of contents, 
this is by no means yet the case. 
The rst three chapters have a distinct text-book character: Chapter 1 
provides the necessary prerequisites in Probability Theory and Statistics, 
Chapter 2 describes some statistical models that are much in use in the eld, 
and Chapter 3 constitutes an introduction to Corpus Linguistics. Chapters 
4 and 5 discuss how statistical models and techniques are applied to 
various task in Computational Linguistics and Natural Language Processing, 
relating the material presented in the previous chapters to relevant 
scientic articles. 
Feel free to use this compendium, but please do acknowledge the source. 
Also, any comments or suggestions to improvements are more than welcome, 
and are most likely to enhance future versions of the compendium. 
We are already greatly indebted for this to Rens Bod, Bob Carpenter, John 
Carroll, Ted Dunning, Jussi Karlgren, Kimmo Koskenniemi, David Magerman, 
David Milward, Joakim Nivre, Khalil Sima'an, Atro Voutilainen and 
numerous students at the University of the Saarland, Uppsala University, 
the University Helsinki and to course participants at the ESSLLI-97 summer 
school in Aix-en-Provence. Special credit is due to Thorsten Brants, 
who wrote the rst version of the section on Hidden Markov Models, and 
to our online mathematician Ake H. Samuelsson. Parts of the compendium 
are used in a web-based introductory course on statistical natural language 
processing which is set up by Joakim Nivre at Goteborg University. It can 
be accessed via http://www.ling.gu.se/nivre/kurser/wwwstat/. 
Saarbrucken New York 
December 1997 
Brigitte Krenn Christer Samuelsson 
vii

viii CONTENTS

Chapter 1 
Basic Statistics 
1.1 The Stability of the Relative Frequency 
Although an elegant theory in its own right, the main reason that statistics 
has grown as popular as it has is something that is known as \The stability 
of the relative frequency". This is the empirical observation that there is 
some structure also in random processes. If we for example ip a coin a 
large number of times, we will note that in approximately half the cases 
the outcome is \heads" and in approximately half the cases it is \tails". If 
we ip a coin a small number of times, say only once, twice or three times, 
this is not consistently the case. 
The proportion of times a certain outcome occurs is called the relative 
frequency of the outcome. If nu is the number of times the outcome u occurs 
in n trials, then 
nu 
n 
is the relative frequency of u. The relative frequency 
is often denoted fn. 
Empirically, there seems to be some number which the relative frequency 
stabilizes around after a large number of trials. A fundamental assumption 
in statistics is that such numbers exist. These numbers are called probabilities. 
1.2 Elementary Probability Theory 
Introductory presentations of statistics often disguise probability theory in 
set theory, and this is no exception. 
1.2.1 Sample Space 
The sample space is a set of elementary outcomes. An event is a subset of 
the sample space. Sample spaces are often denoted 
, and events are often 
called A, B, C, etc. Let's get this grounded with an example: 
Example: For a normal die1, the six elementary outcomes are 
One, Two, Three, Four, Five and Six, and thus the sample space 

 is the set fOne;Two; Three; Four; Five; Sixg. The events are 
various subsets of this set, e.g., \the outcome is One", fOneg; 
\the outcome is less than four", fOne;Two; Threeg; the outcome 
is odd, fOne; Three; Fiveg; etc. In fact, there are 26 = 64 
1\Die" is the singular form of \dice", a sort of mechanical random generators used in 
games like Craps, Monopoly and Fia-med-knu. 
1

2 CHAPTER 1. BASIC STATISTICS 
dierent subsets of 
, i.e., there are 64 distinct events in 
, 
including the empty set ; and 
 itself, see Section 1.2.7. 
Statisticians are a kind of mathematicians, and like to distinguish between 
for example the outcome One, which is a basic element, and the event 
fOneg, which is a set consisting of one element. We will try to keep this 
up for a while. 
1.2.2 Probability Measures 
A probability measure P is a function from events in the sample space 
, 
i.e., from the set of subsets of 
, 2 to the set of real numbers in [0; 1] that 
has the following properties: 
1) 0  P(A)  1 for each event A  
 
2) P(
) = 1 
3) A \ B = ; ) P(A [ B) = P(A) + P(B) 
Intuitively, the total mass of 1 is distributed throughout the set 
 by the 
function P. This will assign some particular mass to each subset A of 
. 
This mass is the probability of event A, denoted P(A). A \ B = ; means 
that A and B are disjoint, i.e., that they have no common element. 
Example: For a fair (unbiased) die, where as we recall the 
sample space is the set fOne;Two; Three; Four; Five; Sixg, the 
mass 1 is evenly and justly divided among the six dierent singleton 
sets. Thus P(fOneg) = P(fTwog) = P(fThreeg) = 
P(fFourg) = P(fFiveg) = P(fSixg) = 
1
6
. If A is the event of 
the outcome being divisible by two, i.e., the subset fTwo; Four; Sixg, 
and if B is the event of the outcome being divisible by three, 
i.e., the subset fThree; Sixg, then P(A) = 
1
2 
and P(B) = 
1
3
. 
A loaded die, used by cheaters, would not have the probability 
mass as evenly distributed, and could for example assign 
P(fSixg) a substantially larger value than 
1
6
. 
Some immediate corollaries that are worth remembering fall out from 
the denition of a probability measure: 
a) P(B n A) = P(B)  P(A \ B) 
b) A  B ) P(A)  P(B) 
c) P(  A) = 1 P(A) 
d) P(;) = 0 
e) P(A [ B) = P(A) + P(B)  P(A \ B) 
B n A denotes the dierence set B minus A, i.e., the set of elements in B 
that are not members of A.  A denotes the complement of A, i.e., 
 n A. 
Proofs: 
a) B = (B n A) [ (A \ B) ; (B n A) \ (A \ B) = ; ) 
P(B) = P((B n A) [ (A \ B)) = P(B n A) + P(A \ B) 
2This is called the power set of 
 and is denoted 2
.

1.2. ELEMENTARY PROBABILITY THEORY 3 
b) A  B ) A \ B = A 
0  P(B n A) = P(B)  P(A \ B) = P(B)  P(A) 
c)  A = 
n A ; A \
 = A ) 
P( A
) = P(
 n A) = P(
)  P(A \
) = 1 P(A) 
d) A = A [ ; ; A \ ; = ; ) 
P(A) = P(A [ ;) = P(A) + P(;) 
e) A [ B = A [ (B n A) ; A \ (B n A) = ; ) 
P(A [ B) = P(A [ (B n A)) = P(A) + P(B n A) = 
= P(A) + P(B)  P(A \ B) 
2 
Note how a) follows from 3) and how a) is used to prove b), c) and e). 
However, d) follows more easily from 3) directly. 
1.2.3 Independence 
The probability of two events A and B both occurring is the probability of 
the intersection of the sets A and B, P(A \ B). 
Two events A and B are said to be independent i3 
P(A \ B) = P(A)  P(B) (1.1) 
Intuitively, this means that the probability of A and B occurring simultaneously 
can be established directly from the individual probabilities of A 
and B. 
Example: To continue the example of the fair die, with the 
events A of the outcome being divisible by two and B of the 
outcome being divisible by three, we note that P(A \ B) = 
P(fSixg) = 
1
6
, and that P(A)  P(B) = 
1
2  
1
3 
= 
1
6
. Thus A and 
B are independent. 
Let C be the event of the outcome being divisible by four and 
note that P(C) = P(fFourg) = 
1
6
. Intuitively, the property 
of being divisible by four is related to the property of being 
divisible by two, and if we calculate on the one hand P(A\C) = 
P(fFourg) = 
1
6
, and on the other hand P(A)  P(C) = 
1
2  
1
6 
= 
1 
12
, we see that A and C are not independent. 
1.2.4 Conditional Probabilities 
P(A j B) is a so-called conditional probability , namely the probability of 
event A given that event B has occurred, and is dened as 
P(A j B) = 
P(A \ B) 
P(B) 
(1.2) 
This is the updated probability of A once we have learned that B has 
occurred. P(A) is often called the prior probability of A since we have no 
3\I" means if and only if.

4 CHAPTER 1. BASIC STATISTICS 
prior information, while P(A j B) is called the posterior probability of A 
(knowing B), since it is dened posterior to that, that event B occurred. 
Intuitively, after the event B has occurred, an event A is replaced by the 
event A \ B, and the sample space 
 is replaced by the event B, and the 
probabilities are renormalized accordingly, which means dividing by P(B). 
The probability of an event A can change drastically when one learns 
that some other event has occurred. The following example is rather extreme, 
but illustrates this point: 
P(A j A) = 1 
P(A j  A) = 0 
Now, if A and B are independent, then P(A \ B) = P(A)  P(B) and 
thus P(A j B) = 
P(A \ B) 
P(B) 
= 
P(A)  P(B) 
P(B) 
= P(A). This means that no 
new information is gained about A by knowing that B has occurred. 
Example: Again continuing the example of the fair die, with 
the events A of the outcome being divisible by two, B of the 
outcome being divisible by three, and C of the outcome being divisible 
by four, we note that P(A j B) = 
P(A \ B) 
P(B) 
= 
1
61 
3 
= 
1
2 
= 
P(A), as should be, considering that A and B are independent. 
On the other hand, P(A j C) = 
P(A \ C) 
P(C) 
= 
P(fFourg) 
P(fFourg) 
= 1 6= 
P(A) = 
1
2
, since A and C are not independent. 
1.2.5 Bayesian Inversion 
Either by returning to the denition of conditional probability, Eq. (1.2), 
or by noting that if both A and B are to occur, then rst B must occur, 
and then A must occur, or the other way around, we can establish that 
P(B)  P(A j B) = P(A \ B) = P(A)  P(B j A) 
This directly gives us the Bayesian inversion formula4: 
P(A j B) = 
P(A)  P(B j A) 
P(B) 
(1.3) 
This formula relates the probability of an event A conditional on another 
event B, i.e. P(A j B), with the probability of event B conditional on event 
A, P(B j A), and is useful if the former quantity is not easily determined, 
while the latter is. 
Example: We turn again to the example of the fair die, with 
the events A of the outcome being divisible by two, B of the 
outcome being divisible by three, and C of the outcome being 
divisible by four. If we wish to calculate P(C j A), i.e., the 
probability that the outcome is divisible by four given that it 
is divisible by two, we observe that the probability P(A j C) 
4The formula is of almost religious importance to so-called Bayesianists, see [Pearl 
1988], pp. 29{41.

1.2. ELEMENTARY PROBABILITY THEORY 5 
is 1, since divisibility by four implies divisibility by two, and 
recalling that P(A) = 
1
2 
and that P(C) = 
1
6
, we can establish 
that P(C j A) = 
P(C)  P(A j C) 
P(A) 
= 
1
6  1 
1
2 
= 
1
3
. 
1.2.6 Partitions 
Assume that we have a collection fAi : i = 1; : : :;Ng of events (sets) such 
that 

 = 
N [
i=1
Ai 
Ai \ Aj = ; for i 6= j. 
The rst equality means that the sets Aj cover the sample space 
 and 
the second equality states that all events Ai are disjoint. This situation 
is illustrated in Figure 1.1. We say that fAi : i = 1; : : :;Ng constitutes a 
A1 
A2 
A3 A4 
A5 
W 
Figure 1.1: The sets A1 through A5 constitute a partition of 
. 
partition of 
 and we can establish the following formula: 
P(B) = P(B \ 
) = P(B \ 
N [i=1
Ai) = (1.4) 
= P( 
N [
i=1
B \ Ai) = 
N X
i=1 
P(B \ Ai) = 
N X
i=1 
P(B j Ai)  P(Ai) 
This can be seen as follows: Since the sample space 
must contain the event 
B, B equals B\
. Thus the probability P(B) is the same as the probability 
P(B \ 
). As the sample space consists of the union of the disjoint events 
Ai, we have 
 = 
N [
i=1
Ai, and we can thus write P(B \ 
N [
i=1
Ai) instead of 
P(B \ 
). Now, B \ 
N [
i=1
Ai is equivalent to 
N [
i=1
B \ Ai, and we can thus

6 CHAPTER 1. BASIC STATISTICS 
rewrite P(B \ 
N [
i=1
Ai) as P( 
N [
i=1
B \Ai). Furthermore, the probability of the 
union of disjoint events equals the sum of the probability of the individual 
events. Thus P( 
N [
i=1
B \ Ai) = 
N X
i=1 
P(B \ Ai). The last equality of the 
equation follows from the denition of conditional probabilities, Eq. (1.2). 
Equation 1.4 is very useful when there is a natural partition fAig of 

, and it is easy to calculate the probabilities P(Ai) and the conditional 
probabilities P(B j Ai). 
Example: In the example of the fair die, one way one could determine 
the probability of event B, that the outcome is divisible 
by three, is to use the natural partition A1 = fOneg; : : :;A6 = 
fSixg where P(Ai) = 
1
6 
for all i, and where P(B j Ai) is 1 if i 
is divisible by 3 and 0 if it is not. Thus P(B) = 0  
1
6 
+ 0 
1
6 
+ 
1  
1
6 
+ 0 
1
6 
+ 0  
1
6 
+ 1  
1
6 
= 
1
3
. 
1.2.7 Combinatorics 
And now for something completely dierent: This section contains some 
results from elementary combinatorics that will be needed later, e.g., in 
Section 1.5.1. 
The following table summarizes the number of dierent ways k elements 
can be selected from n elements. This is in fact an instance the much 
dreaded \urn and balls" scenario, which has plagued statistics as long as 
anyone can remember. This particular incarnation can be viewed as the 
even more feared \Lotto" scenario where the urn contains n balls numbered 
from 1 to n, and where we will at random draw a ball k times. In doing 
this, we may or may not replace selected elements, i.e., we may or may not 
put the drawn ball back into the urn, and we may or may not be interested 
in the order in which they are selected. This gives us four dierent cases: 
Without replacement With replacement 
Ordered (n)k nk 
Unordered  n
k   n + k  1 
k  
Here 
n! = n  (n  1)  : : :  1 
(n)k = n  (n  1)  : : :  (n  k + 1) = 
n! 
(n  k)! 
nk = 
k factors 
z }| { n  : : :  n 
 n
k  = 
(n)k 
k! 
= 
n! 
(n  k)!k! 
n! is read out \n factorial". By denition 0! = 1. In particular, the number 
of permutations of n elements is (n)n = n!.  n
k  is read out \n over k".

1.2. ELEMENTARY PROBABILITY THEORY 7 
We devote the rest of the section to proofs of these formulas: 
Ordered, without replacement: 
We wish to select k items from n items, where selected items are not replaced, 
and where we are interested in the order in which they are selected. 
First we have n possible choices, then we have n  1 choices left, since 
we have removed one item, etc., down to the kth choice, where we have 
n(k1) = nk+1 alternatives. Thus n(n1) : : : (nk+1) = (n)k 
possibilities in total. 2 
Ordered with replacement: 
We wish to select k items from n items, where selected items are replaced, 
and where we are interested in the order in which they are selected. First 
we have n possible choices, then we have again n possible choices, since we 
put back the selected item, etc., down to the kth choice, where we still have 
n alternatives. Thus nk possibilities in total. 2 
Unordered without replacement: 
We wish to select k items from n items, where selected items are not replaced, 
and where we are not interested in the order in which they are 
selected. We will rst select k items paying attention to the order. Then 
there are (n)k possibilities. For each set of k dierent items, there will 
be k! permutations of them. Thus among these (n)k sequences of k items 
there are for each such set k! sequences that should be considered to be the 
same, since we are not interested in the order in which items are selected. 
We will then factor out this by dividing (n)k by k!. In short: 
#unordered without replacement = 
#ordered without replacement 
#permutations 
= 
(n)k 
k! 
=  n
k . Thus  n
k  possibilities in total. 2 
Unordered with replacement: 
We wish to select k items from n items, where selected items are replaced, 
and where we are not interested in the order in which they are selected. To 
this end, we make a list of the n items, and make a tick beside \i" each 
time we select item i; i = 1; : : :; n: 
k 1 : : : i j : : : n 
n j : : : j j j : : : j 
We can represent this by a sequence of n zeros and k ones, where the 
number of ones immediately preceding the ith zero indicates the number of 
times item i is selected: 
n 
z }| { 0 : : :00 : : :0 
10 : : :11010 : : :10 
| {z } n + k 
Thus, this reduces to selecting k elements (ones) from n+k1 elements without 
replacement, and where the order is not signicant. (The \minus one" 
is since the last digit cannot be a one.) Thus  n + k  1 
k  possibilities 
in total. 2 
The last two proofs used the well-known tactics of mathematicians of 
reducing to Case One. 
Story time: A mathematician is faced with the following scenario: 
A bucket, a well and a house on re. He immediately nds 
the correct solution of using the bucket to fetch water from the

8 CHAPTER 1. BASIC STATISTICS 
well and extinguish the re. Next, the mathematician is faced 
with the scenario of a bucket, a well and a house that is not on 
re. He rapidly sets re to the house and reduces the problem 
to Case One. 
1.3 Stochastic Variables 
A stochastic, or random, variable  is a function from a sample space 
 
to R, the set of real numbers. Thus, if u 2 
, then (u) 2 R. Figure 1.2 
illustrates this. It may seem strange to call a function a variable, but this 
convention will die hard, if ever. There are two dominant conventions for 
denoting stochastic variables | one using Greek letters like ;; ; : : : and 
the other using Roman capital letters X;Y;Z; : : : We will use the former 
convention. 
W 
u 
x(u) R 
Figure 1.2: A random variable is a function from 
 to R. 
Example: The running example of the fair die does not illustrate 
this very well, since there is a mapping form the sample 
space 
 = fOne;Two; Three; Four; Five; Sixg to the subset 
f1; 2; 3; 4; 5; 6g of R that is so obvious, that it seems articial 
to distinguish between the two. However, let us dene the 
stochastic variable  as the function from 
 to R such that 
(One) = 1; (Two) = 2; : : :; (Six) = 6. 
1.3.1 Distribution Function 
Let A be a subset of R, and consider the inverse image of A under , i.e., 
1(A) = fu : (u) 2 Ag  
. We will let P( 2 A) denote the probability 
of this set, i.e., P(1(A)) = P(fu : (u) 2 Ag) = P( 2 A). See Figure 1.3. 
If A is the interval (1; x],5 then the real-valued function F dened by 
F(x) = P(fu : (u)  xg) = P(  x) for all x 2 R 
is called the distribution function of the random variable . Sometimes F is 
denoted F to indicate that it is the distribution function of the particular 
random variable . 
5(a; b) denotes an open interval, while [a; b] denotes a closed interval, i.e., in the former 
case a and b do not belong to the interval, while in the latter case they do.

1.3. STOCHASTIC VARIABLES 9 
x-1(A)
W 
R 
A 
Figure 1.3: P( 2 A) is dened through P(1(A)) = P(fu : (u) 2 Ag). 
Example: In the reappearing example of the fair die, the distribution 
function F is dened by 
F(x) = 
8>
>>>>>>>>>>>><>
>>>>>>>>>>>>: 
0
6 
= 0 for x < 1; 
1
6 
for 1  x < 2; 
2
6 
= 
1
3 
for 2  x < 3; 
3
6 
= 
1
2 
for 3  x < 4; 
4
6 
= 
2
3 
for 4  x < 5; 
5
6 
for 5  x < 6; 
6
6 
= 1 for 6  x: 
The graph of this function is depicted in Figure 1.4. 
Some rather useful corollaries that are worth remembering include: 
a) P( > x) = 1 F(x) 
b) P(a <   b) = F(b)  F(a) 
c) F is a nondecreasing function, i.e. if x1 < x2, then F(x1)  F(x2). 
d) lim 
x!1
F(x) = 0 
e) lim 
x!1
F(x) = 1 
The proofs of a{c) are left as an exercise. 
1.3.2 Discrete and Continuous Stochastic Variables 
The image of the sample space 
 in R under the random variable , i.e., 
the range of , is called the sample space of the stochastic variable  and is

10 CHAPTER 1. BASIC STATISTICS 
1 3 4 5 6 
2
3
4
1
5
1 
6
6
6
6
6 
2 
F(x) 
x 
Figure 1.4: Fair die: Graph of the distribution function. 
denoted 
. In short 
 = (
). 
Stochastic variables come in several varieties. Two common types are 
discrete and continuous stochastic variables: 
 A random variable is discrete i 
 is nite or countable. 
 A random variable is continuous i 
1. F is continuous, and 
2. F is dierentiable with a continuous derivative except in at most 
a nite number of points. 
The attentive reader may have noticed that Calculus is entering into this 
presentation through the back door. This is really necessary for a good 
understanding of the subject, in particular of continuous random variables, 
and we refer those interested in this to some introductory book on calculus 
until we have had time to include an appendix on the topic. 
Example: In the persistent example of the fair die, 
 = 
f1; 2; 3; 4; 5; 6g, and  is thus a discrete random variable. 
1.3.3 Frequency Function 
Another convenient way of characterizing a random variable is by its frequency 
function f: 
 For a discrete random variable, f(x) = P( = x). 
 For a continuous random variable, f(x) = F0(x) = d 
dxF(x). 6 
6F0(x) = d 
dxF(x) = limh!0 
F(x + h)  F(x) 
h 
is the derivative of the function F at 
point x. Intuitively, this is the slope of the curve at point x when F(x) is plotted against 
x.

1.3. STOCHASTIC VARIABLES 11 
Sometimes f is denoted f to indicate that it is the frequency function 
of the particular random variable . The frequency function of a discrete 
stochastic variable is often referred to as the probability function, and the 
frequency function of a continuous stochastic variable is often referred to 
as the probability density function. 
Example: Continuing the perannial example of the fair die, we 
nd that the frequency function f is 
f(1) = f (2) = f (3) = f(4) = f(5) = f(6) = 
1
6 
This is an example of a uniform distribution, i.e., f(x) has the 
same value for all elements in 
 . Note that a uniform distribution 
can only be assigned to a nite or bounded sample space. 
Figure 1.5 shows the graphical representation of the frequency 
function f .
1 3 5 6 
6 
2 
1 
4 
x 
f(x) 
Figure 1.5: Fair die: Graph of the frequency function 
The probabilities of events can be calculated from the frequency function: 
 For a discrete random variable 
P( 2 A) = Xx2A 
f(x) 
and in particular 
P(  x) = F(x) = X i:xix 
f(xi) 
 For a continuous random variable, 
P( 2 A) = ZA 
f(x) dx

12 CHAPTER 1. BASIC STATISTICS 
and in particular 
P(  x) = F(x) = Z x 
1 
f(t) dt 
Since all probabilities must sum to one, we have: 
 For a discrete random variable 
P(
) = Xx2
 
f(x) = 1 
 For a continuous random variable 
P(
) = Z 1 
1 
f(x) dx = 1 
1.3.4 Expectation Value 
The expectation value or statistical mean of a stochastic variable , denoted 
E[], is dened as follows: 
 For a discrete random variable 
E[] = Xx2
 
x  f(x) = Xi 
xi  f(xi) 
 For a continuous random variable 
E[] = Z 1 
1 
x  f(x) dx 
Expectation values are often denoted . The expectation value is the average 
value of the outcome of the random variable weighted by probability, 
indicating the center of gravity of . 
Example: Continuing the notorious example of the fair die, 
E[] = 
6 X
i=1 
i  
1
6 
= 
6  7 
2  
1
6 
= 
7
2 
Note that this is not a possible value for . 
A random variable can be a function of another random variable, i.e., 
 = g(). The expectation value of the random variable  can be calculated 
from the frequency function f of : 
 For a discrete random variable 
E[] = E[g()] = Xx2
 
g(x)  f(x) = Xi 
g(xi)  f(xi) 
 For a continuous random variable, 
E[] = E[g()] = Z 1 
1 
g(x)  f(x) dx 
As we shall soon see, this trick can come in quite handy. 
Example: Continuing the long-lasting example of the fair die, 
if  = 2, then 
E[] = E[2] = 
6 X
i=1 
i2  
1
6 
= 
91 
6

1.3. STOCHASTIC VARIABLES 13 
1 3 4 5 6 
2
3
4
1
5
1 
6
6
6
6
6 
2 7 2 
F(x) 
f(x) 
mean 
y 
x 
Figure 1.6: Fair die: Expectation value (mean) 
1.3.5 Variance 
The variance of a stochastic variable , denoted Var[], is dened as the 
expectation value of (  )2, where  = E[], i.e., Var[] = E[(  )2]. 
 For a discrete random variable, this means that 
Var[] = Xx2

(x  )2  f(x) 
 For a continuous random variable, the corresponding expression is 
Var[] = Z 1 
1
(x  )2  f(x) dx 
Variances are often denoted 2. The variance is a measure of how spread 
out the probability mass is from the center of gravity .  itself is referred 
to as the standard deviation. 
Example: Continuing the inextinguishable example of the fair 
die, 
Var[] = 
6 X
i=1
(i  
7
2
)2  
1
6 
= 
35 
12 
From the point of view of elementary Mechanics, the variance is simply 
the quadratic moment about the center of gravity, and Steiner's Theorem 
from Mechanics can sometimes be useful for calculating Var[]: 
Var[] = E[2]  (E[])2 or (1.5) 
2 = E[2]  2

14 CHAPTER 1. BASIC STATISTICS 
Example: Continuing the eternal example of the fair die, 
Var[] = E[2]  (E[])2 = 
91 
6  
49 
4 
= 
182  147 
12 
= 
35 
12 
which is the same result as when using the denition directly. 
1.3.6 Moments and Moment-Generating Functions 
In general, the expectation value of r, i.e., E[r], is known as the rth 
moment of , denoted r. This means that E[] =  = 1 and that the 
expectation value of the square of , which just gured in Steiner's Theorem, 
is the second moment of , i.e., E[2] = 2. If we instead calculate the 
expectation value of E[()r ], we have the rth central moment of  about 
. We see that the variance is the second central moment of  about . 
The moment-generating function m(t) of  is dened as 
m(t) = E[et] = Xx2
 
extf(x) 
for a discrete random variable and for a continuous random variable as 
m(t) = E[et] = Z 1 
1 
extf(x) dx 
This sum or integral doesn't necessarily converge for any value of t, but if 
it does so for all t around zero, i.e., in h < t < h for some h > 0, we are 
in business. We then have 
dr 
dtrm(t) = Z 1 
1 
xrextf(x) dx 
and if we let t tend to zero, we nd that 
dr 
dtrm(0) = Z 1 
1 
xrf(x) dx = E[r] = r 
So in this sense, m(t) generates all moments of . In view of the series expansion 
of the function ex discussed in Appendix C.3, this is not altogether 
too surprising. 
1.4 Two-dimensional Stochastic Variables 
Let  and  be two random variables dened on the same sample space 

. Then (; ) is a two-dimensional random variable from 
 to 
(;) = 
f((u); (u)) : u 2 
g  R2. Here, R2 = R  R is the Cartesian product 
of the set of real numbers R with itself. This is illustrated in Figure 1.7. 
We can pull o the same sort of stunts with two-dimensional stochastic 
variables as with one-dimensional stochastic variables. 
1.4.1 Distribution Function 
The distribution function of the two-dimensional random variable (; ) is 
dened as follows: 
F(x; y) = P(  x;   y) = P(fu : (u)  x; (u)  yg) for all (x; y) 2 R2 
This is called the joint, or bivariate, distribution of  and  and P( 2 A;  2 B) is called the joint probability of  2 A and  2 B.

1.4. TWO-DIMENSIONAL STOCHASTIC VARIABLES 15 
(u) h 
x(u) 
x(u) (u) h , ( )
R2 
u 
y 
W 
x 
Figure 1.7: A two-dimensional random variable is a function from 
 to R2. 
1.4.2 Frequency Function 
There are discrete and continuous versions of two-dimensional random variables, 
and they all have frequency functions: 
 A two-dimensional random variable (; ) is discrete i 
(;) is nite 
or countable. The frequency function f of (; ) is then dened by 
f(x; y) = P( = x;  = y) = P((; ) = (x; y)) for (x; y) 2 R2 
 A two-dimensional random variable is continuous i the distribution 
function F(x; y) can be written as 
F(x; y) = Z x 
1 Z y 
1 
f(u; v) dv du 
for some non-negative integrable function f. The function f is then 
called the frequency function of (; ). 
Thus, for all two-dimensional stochastic variables we have f(x; y)  0. 
For a continuous variable we also have7 
f(x; y) = 
@2 
@x@y 
F(x; y) and 
Z 1 
1 Z 1 
1 
f(u; v) dv du = 1: 
Furthermore, for A  
(;) 
P((; ) 2 A) = X (x;y)2A 
f(x; y) if (; ) is discrete, and 
P((; ) 2 A) = ZA 
f(x; y) dx dy if (; ) is continuous. 
7The curly derivative @ 
@x is the partial derivative w.r.t. x.

16 CHAPTER 1. BASIC STATISTICS 
Intuitively, a two-dimensional stochastic variable denes a mass distribution 
in the real plane. For a discrete variable, f(x; y) is the mass at point (x; y); 
for a continuous variable, f(x; y) is the density at point (x; y). 
We can recover the frequency functions of either of the individual variables 
by summing, or integrating, over the other: 
If (; ) is discrete, we have 
f(x) = Xy2
 
f(x; y) and f(y) = Xx2
 
f(x; y) 
and if (; ) is continuous, we have 
f(x) = Z 1 
1 
f(x; y) dy and f(y) = Z 1 
1 
f(x; y) dx 
In this context, f is often referred to as the marginal distribution of , and 
similarly for f. 
1.4.3 Independence 
Two stochastic variables  and , dened on the same sample space, are 
said to be independent i 
P( 2 A;  2 B) = P( 2 A)  P( 2 B) (1.6) 
for all subsets A and B of R. 
Not very surprisingly,  and  being independent is equivalent to 
 F(;)(x; y) = F(x)  F(y) for all (x; y) 2 R2; or 
 f(;)(x; y) = f(x)  f(y) for all (x; y) 2 R2. 
1.4.4 Functions of Stochastic Variables 
Finally, we look at functions of two random variables, (u) = g((u); (u)). 
For the expectation value of g(; ) we have 
E[g(; )] = X (x;y)2
(;) 
g(x; y)  f(;)(x; y) if (; ) is discrete 
E[g(; )] = Z 1 
1 Z 1 
1 
g(x; y)  f(;)(x; y) dy dx if (; ) is continuous. 
Let ;  and 1; : : :; n be random variables dened on the same sample 
space 
, and let a be a real number. The following formulas are worth 
memorizing: 
a) E[a  ] = a  E[] 
b) E[ + ] = E[] + E[] 
c) E[1 + : : :+ n] = E[1] + : : :+ E[n] 
d) Var[a  ] = a2  Var[] 
If we in addition to this require that ;  and 1; : : :; n are independent, 
we get the following useful formulas: 
a) E[  ] = E[]  E[] 
b) E[1  : : :  n] = E[1]  : : :  E[n] 
c) Var[ + ] = Var[] +Var[] 
d) Var[1 + : : :+ n] = Var[1] + : : :+Var[n]

1.4. TWO-DIMENSIONAL STOCHASTIC VARIABLES 17 
We will later have reason to pay attention to sums of stochastic variables. 
The frequency function f(+)(x) of the sum + can be derived from 
the joint frequency function f(;)(x; y). In the continuous case we have 
F(+)(x) = P( +   x) = Zu+vx 
f(;)(u; v) du dv = 
= Z 1 
1 Z xu 
1 
f(;)(u; v) dv du 
Now 
f(+)(x) = 
d 
dx
F(+)(x) = 
d 
dx Z 1 
1 Z xu 
1 
f(;)(u; v) dv du = 
= Z 1 
1 
d 
dx Z xu 
1 
f(;)(u; v) dv du = Z 1 
1 
f(;)(u; x  u) du 
In particular, if  and  are independent, we have f(;)(x; y) = f(x)f(y) 
and thus 
f(+)(x) = Z 1 
1 
f(u)f(x  u) du 
Example: Let i : i = 1; 2; : : : be independent random variables 
with frequency function ex. (This particular frequency 
function will be discussed in Section 1.5.3.) Then 
f(1+2)(x) = Z 1 
1 
f1 (u)f2 (x  u) du = 
= Z x 
0 
eue(xu) du = Z x 
0 
2ex du = 
= 2ex Z x 
0 
du = 2xex 
If we have the sum of three variables, we can establish that 
f(1+2+3)(x) = Z 1 
1 
f(1+2)(u)f3 (x  u) du = 
= Z x 
0 
2ueue(xu) du = 3ex Z x 
0 
u du = 
= 3 x2 
2 
ex 
In general we nd that f(1+:::+n)(x) = n xn1 
(n  1)! 
ex 
So what is the expectation value of the maximum of n independent 
observations of the same random variable ? 
Fmax(1;:::;n)(x) = P(max(1; : : :; n)  x) = 
n Y
i=1 
P(i  x) = (F(x))n 
Derivation immediately yields: 
fmax(1;:::;n)(x) = F0 
max(1;:::;n)(x) = n(F(x))n1f(x)

18 CHAPTER 1. BASIC STATISTICS 
Thus we have 
E[max(1; : : :; n)] = Z
 
xfmax(1;:::;n)(x)dx = Z
 
xn(F(x))n1f(x)dx 
If for example  has a uniform distribution on the interval (a; b), i.e., if 
  U(a; b), see Section 1.5.3, then 
E[max(1; : : :; n)] = Z
 
xn(F(x))n1f(x)dx = Z b 
a 
xn(
x  a 
b  a 
)n1 dx 
b  a 
= 
= Z b 
a 
n(
x  a 
b  a 
)ndx+ Z b 
a 
n(
x  a 
b  a 
)n1 a 
b  a
dx = 
= 
n(b  a) 
n+ 1 
[(
x  a 
b  a 
)n+1]b
a + 
na 
n 
[(
x  a 
b  a 
)n]b
a = 
n 
n + 1
(b  a) + a 
1.4.5 Higher Dimensions 
All this can easily be generalized to several variables. 
1.5 Selected Probability Distributions 
Some particular types of stochastic variables pop up quite often, and their 
distributions are given special names. In the following, we will discuss in 
detail some probability distributions, namely the Binomial and Normal (or 
Gaussian) distributions, and present more briey a number of other ones. 
1.5.1 Binomial Distribution 
A common situation is when we repeat an experiment a number of times 
and see how many times some particular outcome occurs. For example, 
assume that we ip a coin a number of times, and count the number of 
times \heads" comes up. 
Let 
 = fu; vg be a basic sample space (where in our example say u is 
\heads") and let  be the number of times u occurs in n independent trials. 
The stochastic variable  is then dened on the sample space 
n factors 
z }| { fu; vg  fu; vg  : : : fu; vg 
and an outcome in this sample space can be described as a string of length 
n over the alphabet fu; vg, e.g., if n = 5, uvvuu. This is mapped to a real 
number, in this case 3, since this string contains three us. Thus, the value 
of  is a natural number between 0 and n, i.e. 
 = f0; 1; : : :; ng, and  is 
a discrete random variable, since this set is nite. 
In general, let p be the probability of an event u. If  is the number of 
times u occurs in n independent trials, then  has a Binomial distribution 
with parameters n and p. This is often denoted   binfn,pg. We will 
now try to establish the frequency function of a stochastic variable with a 
Binomial distribution. Assume that event u has probability p and event v 
has probability q = 1  p. Then for example the sequence uvvuu has probability 
pqqpp = p3q2. In fact, any sequence with three us and two vs has 
probability p3q2. Now, there are ten dierent such strings, i.e., there are 
ten outcomes in fu; vgfu; vgfu; vgfu; vgfu; vg that map to 3, namely 
uuuvv; uuvuv; uuvvu; uvuuv; uvuvu; uvvuu; vuuuv; vuuvu; vuvuu and vvuuu.

1.5. SELECTED PROBABILITY DISTRIBUTIONS 19 
Each of these outcomes has probability p3q2. Thus the probability of getting 
exactly three \heads" in ve ips is 10  (
1
2
)3  (
1
2
)2 = 
10 
32 
= 0:3125. 
Similarly, we can compute the probability of getting exactly zero, one, 
two, four or ve heads in ve independent trials of the coin-ipping experiment. 
Figure 1.8 shows the resulting probabilities, and Figure 1.9 gives a 
graphical representation of the distribution. 
k 0 1 2 3 4 5 P P( = k) 
1 
32 
5 
32 
10 
32 
10 
32 
5 
32 
1 
32 
1 
Figure 1.8: The probability of getting 0; 1; 2; 3; 4; 5 heads in 5 independent 
trials.
P(x=k) 
1 3 4 5 
1
5 
2 
32 
32 
32 10 
0 
k 
Figure 1.9: The probability distribution when ipping a coin ve times. 
In general, any string of k us and nk vs will have probability pkqnk. 
Now, there are  n
k  = 
n! 
(n  k)!k! 
distinct such strings, as explained 
in detail in Section 1.2.7. Thus, the probability P( = k) = f(k) = 
 n
k pkqnk. 
In words, the probability P( = k), dening the frequency function 
f(k), is the product of the number of possible ways of selecting k items 
from n items,  n
k , and the probability of each such possibility, pkqnk. 
This is valid for k = 0; : : :; n. 
This gives us the frequency function of a binomially distributed variable: 
f(k) =  n
k pk(1  p)nk; k 2 f0; 1; : : :; ng; 0 < p< 1 (1.7)

20 CHAPTER 1. BASIC STATISTICS 
This distribution is parameterized by the probability p of the particular 
outcome u, and n, the number of trials. 
It is easy to establish that if a random variable   bin(n,p), then: 
P( 2 
) = 
n X
k=0 n
k pk(1  p)nk = 1 
F(x) = P(  x) = 
x X
k=0 
f(k) = 
x X
k=0 n
k pk(1  p)nk 
E[] = 
n X
k=0 
k   n
k pk(1  p)nk = np 
Var[] = 
n X
k=0
(k  np)2   n
k pk(1  p)nk = np(1  p) 
The rst three equations should come as a surprise to no one. The rst 
equation simply states that the probability of u happening either zero, 
one, two, : : : or n times in n trials is one, i.e., that the probability of the 
entire sample space is one. The second equation expresses the distribution 
function F(x) in terms of the frequency function f(x). The third equation 
states that the average number of times u will happen in n trials is n  p, 
where p is the probability of u. 
If fn is the relative frequency of event u, i.e., fn = 

n
, we have: 
E[fn] = E[ 

n
] = 
1
n
E[] = 
1
n
np = p 
Var[fn] = Var[ 

n
] = 
1 
n2Var[] = 
1 
n2 np(1  p) = 
p(1  p) 
n 
1.5.2 Normal Distribution 
The Normal or Gaussian distribution, named after the German mathematician 
Carl Friedrich Gauss (1777{1855), famous for guring on the 10 
Mark bank notes together with the distribution function below, is probably 
the most important distribution around. This is due to the Central Limit 
Theorem presented in Section 1.6. Due to its importance, a number of 
other probability distributions have been derived from it, e.g., the 2 and 
t distributions, see Section 1.5.3. 
We will not beat around the bush, but give the hard-core denition 
directly: Let  be a normally distributed random variable with expectation 
value  and variance 2, denoted   N(; ). Then the frequency function 
f(x) is 
f(x) = 
1 
p2
e(x)2 
22 ; x2 R 
Nice gure needed here! 
Since 
f(x) > 0; x2 R, and 
Z 1 
1 
1 
p2
e(x)2 
22 dx = Z 1 
1 
1 
p 
et2 
dt = 1

1.5. SELECTED PROBABILITY DISTRIBUTIONS 21 
this is a proper frequency function8, and since the distribution function is 
F(x) = Z x 
1 
1 
p2
e(t)2 
22 dt 
the associated random variable is continuous. 
See Appendix C.3 for a discussion of the number e, the exponential 
function ex, and the natural logarithm function lnx. e(x)2 
22 is simply ex 
with 
(x  )2 
22 instead of x. 
Next, we will prove that the parameters  and 2 are indeed the expectation 
value and the variance of the distribution. 
Proofs: 
E[] = Z 1 
1 
x 
p2
e(x)2 
22 dx = 
= 
1 
p Z 1 
1 
x   
p2 
e( x 
p2 
)2 
dx + 
1 
p Z 1 
1 
 
p2
e( x 
p2 
)2 
dx = 
= 0 + 
1 
p 
 
p2 Z 1 
1 
et2
p2 dt = 
 
p Z 1 
1 
et2 
dt =  
The rst equality is obtained by subtracting and adding the 
same thing. The rst integral is zero, since the integrand g(x) is 
antisymmetric around , i.e., g(y) = g(+y), which can be 
taken as the midpoint of the integration interval (1;1), and 
the contributions cancel out. The second integral is rewritten 
by moving out a bunch of constants from the integral and using 
the variable substitution t = 
x   
p2 
; dt = 
dx 
p2
. 2 
Var[] = Z 1 
1 
(x  )2 
p2 
e( x 
p2 
)2 
dx = 
2 
p Z 1 
1 
2t2et2 
dt = 
= 
2 
p 
[tet2
]1
1 + 
2 
p Z 1 
1 
et2 
dt = 0 + 2 = 2 
First, the integral is rewritten using the same variable substitution 
as in the previous proof. Then the integral is partially 
integrated, reducing it to an integral with a known value. 2 
The distribution dened above is parameterized by the expectation value 
 and the standard deviation . An important special case is when 
 = 0 and  = 1. This is called the Standard Normal distribution. The 
frequency function is denoted (x) and the distribution function (x). In 
this case the random variable is said to be N(0; 1). From any normally 
distributed random variable   N(; ) we can derive another random variable 
 = 
   
  N(0; 1), i.e., that has a Standard Normal distribution. 
We will prove this shortly. We can thus recover the Normal distribution 
function for any mean and standard deviation using a table of the Standard 
Normal distribution. An example of such a table is given in Section 1.5.4. 
If   N(; ); 1  N(1; 1) and 2  N(2; 2) are independent and 
a is a real constant, then 
8Z 1 
1 
et2 
dt = p, see Appendix C.3.

22 CHAPTER 1. BASIC STATISTICS 
1. a  N(a; a), 
2.  + a  N( + a; ) and 
3. 1 + 2  N(1 + 2;p2
1 + 2
2). 
Proofs: We rst note that the expectation values and variances 
are what they are claimed to be by the formulas at the end of 
Section 1.4. These results will however fall out from the proofs 
that the new variables are normally distributed. 
1. 
Fa(x) = P(a  x) = P(  x=a) = F(x=a) = 
= Z x=a 
1 
1 
p2
e(t)2 
22 dt =  u = at 
du = adt  = 
= Z x 
1 
1 
p2
e
( ua
)2 
22 
du 
a 
= Z x 
1 
1 
ap2
e
(ua)2 
2(a)2 du = 
= Z x 
1 
1 
0p2
e(u0)2 
202 du 
with 0 = a and 0 = a. 2 
2. 
F+a(x) = P( + a  x) = P(  x  a) = F(x  a) = 
= Z xa 
1 
1 
p2
e(t)2 
22 dt =  u = t + a 
du = dt  = 
= Z x 
1 
1 
p2
e(u(+a))2 
22 du = 
= Z x 
1 
1 
p2
e(u0 )2 
22 du 
with 0 =  + a. 2 
3. 
f1+2 (x) = 
= Z 1 
1 
1 
1p2
e
(u1)2 
22
1  
1 
2p2
e
(xu2)2 
22
2 du = : : : = 
= 
1 
0p2
e(x0 )2 
202 
with 0 = 1 + 2 and 02 = 2
1 + 2
2. 2 
This in turn means that if i  N(i; i); i = 1; : : :; n are independent and 
ai; i = 0; : : :; n are real constants, then 
a0 + 
n X
i=1 
aii  N(0; 0) 
0 = a0 + 
n X
i=1 
aii 
02 = 
n X
i=1 
a2i
2
i

1.5. SELECTED PROBABILITY DISTRIBUTIONS 23 
In words: Any linear combination of independent normally distributed random 
variables has a Normal distribution. In particular, this proves that if 
  N(; ), then 
   
  N(0; 1). 
1.5.3 Other Distributions 
This section lists a number of other common distributions. The Uniform 
distribution on a nite set and on an interval pop up quite often. The Poisson 
and Exponential distributions are included mainly for reference, and 
will only feature occasionally in the following. The 2 and t distributions 
are used for estimating the parameters of normally distributed stochastic 
variables, as described in Section 1.7. 
 If the probability mass of a discrete random variable is evenly divided 
on a nite set fxi : i = 1; : : :;Mg, we have a Uniform distribution on 
a nite set; cf. our example of the fair die. This is obviously a discrete 
distribution. 
f(x) = 
1 
M
; x 2 fxi : i = 1; : : :;Mg 
E[] = 
1 
M Xi 
xi 
Var[] = 
1 
M
(Xi 
x2i
 (Xi 
xi)2) 
 If the probability mass of a continuous random variable is evenly 
spread out on a nite interval (a; b), we have a Uniform distribution 
on an interval, denoted   U(a; b). This is obviously a continuous 
distribution. 
f(x) = 
1 
b  a 
x 2 (a; b) 
E[] = 
a + b 
2 
Var[] = 
(b  a)2 
12 
 The Poisson distribution models a stream of impulses where the impulse 
probability is dierentially proportional to the interval length, 
but where impulses in disjoint intervals are independent, and where 
we have translational invariance. 9  is the number of impulses in an 
interval of length t where c is the (dierential) proportionality factor, 
and  = ct. Thus, this is a discrete distribution. 
f(x) = 
x 
x! 
ex x 2 N;  > 0 
E[] =  
Var[] =  
 The Exponential distribution models rst order decay, e.g., radioactive 
decay. Here,  is the life span of an atom. Thus, this is a continuous 
9insert what translat. invar. means

24 CHAPTER 1. BASIC STATISTICS 
distribution. Incidentally, the time between two impulses in a Poisson 
process is exponentially distributed. 
f(x) = ex x 2 R+; >0 
E[] = 
1
 
Var[] = 
1 
2 
This distribution, parameterized by , is denoted   exp(). 
 The discrete counterpart of the Exponential distribution is the Geometric 
or Pascal distribution over N+: 
p(n) = p(1  p)n1 n 2 N+; 0 < p< 1 
E[] = 
1
p 
Var[] = 
1 
p2 
This distribution is parameterized by p, the probability of  taking 
the value 1. Since this is a probability, we require 0 < p < 1. 
 Let i  N(0; 1); i = 1; : : :; r be independent. Then  = 
r Xi=1 
2
i has a 
2 distribution, read out \chi-square distribution", with r degrees of 
freedom, denoted   2(r). This is a continuous distribution. 
f(x) = 
1 
2r
2 (r
2)
x
r
21ex
2 x 2 R+ 
E[] = r 
Var[] = 2r 
where (x) = Z 1 
0 
tx1et dt; x 2 R+. Forget about the frequency 
function and just remember that a 2-distributed variable with r 
degrees of freedom has the same distribution as the sum of the squares 
of r independent variables with a Standard Normal distribution. 
 Let   N(0; 1) and   2(r) be independent. Then  = 
 
p=r 
has 
a t distribution with r degrees of freedom, denoted   t(r). This is 
a continuous distribution. 
E[] = 0 for r > 1 
Var[] = 
r 
r  2 
for r > 2 
You don't want to know the frequency function. For large rs,  is 
approximately  N(0; 1). 
1.5.4 Distribution Tables 
It is cumbersome to use the frequency functions of the above distributions 
directly for calculations, so tables are compiled that list the solution to the

1.6. SOME THEORETICAL RESULTS 25 
equation F(xp) = p for a number of interesting probabilities p. xp is called 
the p  100% fractile, which means that p  100% of the probability mass is 
associated with values of x that are less than xp. This is done for a variety 
of dierent distribution functions F(x), some of which will be listed in a 
future appendix. 
The following is a short example of a table showing the value of the 
distribution function (x) of the Standard Normal distribution N(0; 1) for 
some positive values of x. For negative values, we can utilize the fact that 
(x) = d 
dx(x) is symmetric around zero, i.e., that (x) = (x). This 
means that (x) = 1  (x) since 
(x) = Z x 
1 
(t) dt = 1 Z 1 
x 
(t) dt = 
= 1  Z x 
1 
(t) dt = 1 Z x 
1 
(t) dt = 1 (x) 
x (x) x (x) x (x) 
0.00 0.500 1.10 0.864 2.10 0.977 
0.10 0.540 1.20 0.885 2.20 0.986 
... 
... 
...
0.90 0.816 1.90 0.971 2.90 0.9981 
1.00 0.841 2.00 0.977 3.00 0.9987 
Of special importance are the value of x when (x) is 0.90, 0.95 and 0.975: 
x (x) 
1.282 0.900 
1.645 0.950 
1.960 0.975 
These are often used to create condence intervals, see Section 1.7.5. 
1.6 Some Theoretical Results 
In this section we will discuss some important theoretical results, namely 
Chebyshev's Inequality, Bernoulli's Theorem and the Central Limit Theorem.
We will start with Chebyshev's Inequality, which states that for any 
random variable  with expectation value  and variance 2 and any t > 0 
we have 
P(j  j  t)  
2 
t2 
The probability mass P(j  j  t) is depicted in Figure 1.10 
Proof: Assuming for convenience that  is discrete, we have 
2 = Xx2

(x  )2  f(x)  X jxjt
(x  )2  f(x)

26 CHAPTER 1. BASIC STATISTICS 
|x-m| > t) 
+t m m-t 
P( 
m 
Figure 1.10: The probability mass P(j  j  t). 
 t2 X jxjt 
f(x) = t2 P(jx j  t) 
The rst inequality comes from summing a positive function 
over a subset of 
, and the second since (x  )2  t2 on 
this subset. The nal equality follows from the denition of the 
frequency function f(x). 2 
Equipped with this tool, we will now continue by discussing more formally 
the stability of the relative frequence presented in the introduction. 
Let   bin(n,p) and let fn be 

n
. Recalling that E[fn] = p and 
Var[fn] = 
p(1  p) 
n 
, and using Chebyshev's Inequality with t =  > 0, we 
can establish that 
P(jfn  pj  )  
p(1  p) 
n2 
We will now use a regular trick in mathematical analysis: Having trapped 
some quantity, in our case P(jfn  pj  ), between 0 and something that 
approaches 0 when some parameter, in this case n, approaches 1, we can 
conclude that the trapped quantity also approaches 0 as the parameter 
approaches 1. This means that 
lim 
n!1
P(jfn  pj  ) = 0 for all  > 0 
We can thus establish that the probability of the relative frequence deviating 
the slightest (i.e., being more than  away) from the probability of the 
event tends to zero as the number of trials tends to innity. This is known 
as Bernoulli's Theorem. 
The practical importance of the normal distribution is mainly due to 
the Central Limit Theorem, which states that if 1; 2; : : : is a sequence of

1.7. ESTIMATION 27 
independent and identically distributed random variables with expectation 
value  and variance 2, then 
lim 
n!1
P(Pn
i=1 i  n 
pn  x) = (x) 
This means that n = 
1
n 
n X
i=1 
i is approximately  N(; =pn) for large 
ns.10 Or to put it more informally: If you take a very large random sample 
from a distribution, then the distribution of the sum is approximately 
normal. We will refrain from proving this theorem. 
The conditions that i be independent and identically distributed can 
be relaxed somewhat, the important thing is that they are not to strongly 
dependent, that each makes some small contribution to the sum, and that 
 and 2 exist. Physical measurements are often disturbed by small, reasonably 
independent uctuations in temperature, humidity, electric elds, 
concentrations of various substances, etc., or random shocks, vibrations and 
the like, and can therefore in view of the Central Limit Theorem often be 
assumed to be normally distributed. 
In speech recognition there is a strong evidence from corpora that phonetic 
features are best described acoustically by normal distributions. Thus 
phonemes are modeled by multidimensional normal distributions.11 
i can be either discrete or continuous. In the former case, using the 
normal distribution to approximate the original one is called a continuum 
approximation. In particular, if   bin(n; p), then  is approximately  N(np;pnp(1  p)). This is known as the Moivre-Laplace Limit Theorem: 
lim 
n!1
P( 
  np 
pnp(1  p)  x) = (x) 
Since it is cumbersome to work out the binomial coecients  n
k  for 
large values of n and k, this is quite useful. Even more useful is replacing 
p with the relative frequency fn almost everywhere, since 
(fn  p)pn 
pfn(1  fn) 
is in fact also approximately  N(0; 1). 
1.7 Estimation 
Statistical estimation is usually reduced to the problem of estimating some 
parameter  of a probability distribution, where the rest of the distribution 
is already known. Also,  is known to belong to some parameter space T. 
1.7.1 Random Samples 
Let i; i = 1; : : :; n be independent stochastic variables with the same distribution 
as the variable . Then (1; : : :; n) is said to be a random sample 
10 n will be discussed in more detail in the following section. 
11Cf. [Dalsgaard 1992] for an approach based on neural nets, and [Young 1993] for a 
HMM-based approach.

28 CHAPTER 1. BASIC STATISTICS 
of . The set of observations of the outcome of the variables in some trial, 
(x1; : : :; xn), is called a statistical material. 
From the random sample, we can create new random variables that are 
functions of the random sample, which we will call sample variables. In 
particular, the two variables n and s2
n are called the sample mean and 
sample variance respectively: 
n = 
1
n 
n X
i=1 
i 
s2
n = 
1 
n  1 
n X
i=1
(i  n)2 
The reason for dividing by n1 rather than n in the latter case will become 
apparent in Section 1.7.5. 
We will here digress slightly to prove the Law of Large Numbers: 
lim 
n!1
P(jn  j  ) = 0 for all  > 0: 
Here it is essential that E[] =  and Var[] = 2 both exist. 
Proof: The proof is very similar to the proof of Bernoulli's 
Theorem. By Chebyshev's Inequality we have that 
P(jn  j  )  
2 
n2 for all  > 0: 
By letting n tend to innity, we can establish the claim. 
To put it in more words: The Law of Large Numbers states that 
the sample mean n of a random sample (1; : : :; n) converges 
in probability to the mean  of the distribution from which the 
random sample is taken, as n increases. 2 
This is good news! It tells us that we can estimate  with any accuracy we 
wish by simply making enough observations. 
1.7.2 Estimators 
A sample variable g(1; : : :; n) that is used to estimate some real parameter 
 is called an estimator. Here  will in general depend on the unknown 
parameter  of the frequency function, and should really be written (). 
The value assigned to  by the statistical material is called an estimate. 
The distinction between estimators and estimates is rather convenient to 
make. 
We want the estimator g(1; : : :; n) to in some way be related to the 
parameter  it is supposed to estimate. It is said to be unbiased i 
E[g(1; : : :; n)] =  
Also, we desire that the larger the random sample, the better the estimator. 
Let gn be real-valued functions from Rn to R for n = 1,2,: : : 
The sequences fg(1; : : :; n)g1n
=1 of estimators is said to be consistent i 
lim 
n!1
P(jg(1; : : :; n)  j  ) = 0 
for each  > 0.

1.7. ESTIMATION 29 
We have that the sample mean n is a unbiased estimator of the expectation 
value  since 
E[n] = E[
1
n 
n X
i=1 
i] = 
1
n 
n X
i=1 
E[i] = 
1
n 
n X
i=1 
 =  
and that 1; : : : ; n form a consistent sequence of estimators in view of the 
Law of Large Numbers. One can further prove that the sample variance 
s2
n is an unbiased estimator of the variance 2, and that s21 
; s22 
; : : : is a 
consistent sequence of estimators of 2 if the fourth central moment of  
about  exists, i.e., if E[(  )4] < 1. 
If for a sequence of unbiased estimators n = g(1; : : :; n) we have that 
limn!1Var[n] = 0, then n is a consistent sequence of estimators. 
Proof: By Chebyshev's Inequality we have that 
P(jn  j  )  
1 
2Var[n] 
The claim follows immediately by letting n tend to innity. 2 
If, in general, we have two unbiased estimators 1 and 2, then 1 is 
said to be more ecient than 2 if Var[1] < Var[2]. 
1.7.3 Maximum-Likelihood Estimators 
So how do we nd good estimators? One method is to use Maximum- 
Likelihood Estimators, MLEs. This method is based on a simple, but important 
idea: 
Choose the alternative that maximizes the probability of the observed 
outcome. 
To this end, we proceed as follows: Let (1; : : :; n) be a random sample 
of a stochastic variable  with frequency function f(x), and let (x1; : : :; xn) 
be the corresponding statistical material. We then dene the likelihood 
function L(x1; : : :; xn; ) as the joint frequency function of 1; : : :; n 
L(x1; : : :; xn; ) = f(1;:::;n);(x1; : : :; xn) = 
n Y
i=1 
f(xi) 
Since the sample is assumed to be random, the joint frequency function is 
simply the product of the individual frequency functions, which in turn are 
the same function, since this is a sample. For a discrete variable, this is 
the probability of the outcome (x1; : : :; xn); for a continuous variable, this 
is the probability density in the point (x1; : : :; xn). 
We will simply choose the value ^ that maximizes likelihood function L: 
max 
 
L(x1; : : :; xn; ) 
Seeing that the logarithm function ln (see Appendix C.3 for a discussion 
of this function) is monotonically increasing on R+, we can equally well 
maximize lnL: 
max 
 
lnL(x1; : : :; xn; ) = max 
 
n X
i=1 
ln f(xi)

30 CHAPTER 1. BASIC STATISTICS 
This is convenient as L is a product, which means that lnL is a sum. 
The most important example of a maximum-likelihood estimator is the 
relative frequency fn of an outcome, which is the maximum-likelihood estimator 
of the probability p of that outcome. 
A regular trick for nding the maximum is to inspect points where 
@ 
@ lnL = 0. If we for example happen to know that L is a convex function 
of , this is in fact equivalent to a global maximum for interior points of 
the parameter space T in which  is allowed to vary. 
Nice gure needed here! 
Example: Let (1; : : :; n) be a random sample of   exp(), 
see Section 1.5.3 for the denition of an exponential distribution, 
and let (x1; : : :; xn) be the corresponding statistical material. 
Let , the unknown parameter of the exponential distribution, 
and  the quantity that we wish to estimate, both be . Then 
L(x1; : : :; xn; ) = 
n Y
i=1 
f;(xi) = 
n Y
i=1 
exi = nePn
i=1 
xi 
and 
lnL(x1; : : :; xn; ) = n ln   
n X
i=1 
xi 
Thus 
@ 
@ 
lnL(x1; : : :; xn; ) = 
n
  
n X
i=1 
xi 
which is 0 i 
n
 
= 
n X
i=1 
xi. Let us introduce 
xn = 
1
n 
n Xi=1 
xi 
Then the derivative is zero i  = 1 
xn 
. 
Since L is a convex function of , this is a global maximum. 
Thus the maximum-likelihood estimator of the  parameter of 
the exponential distribution is 1=n. 
Maximum-likelihood estimators can be used to estimate several parameters 
simultaneously. For example, if we want maximum-likelihood estimators 
of both  and 2 for a normally distributed variable , we can 
construct L(x1; : : :; xn; ; ) as usual (using the frequency function for the 
normal distribution), take the logarithm, and nd the values ^ and ^ 2 for 
which simultaneously @ 
@ lnL(x1; : : :; xn; ; ) and @ 
@ lnL(x1; : : :; xn; ; ) 
are zero. This turns out to be when 
^ = 
1
n 
n X
i=1 
xi = xn and ^ 2 = 
1
n 
n X
i=1
(xi  xn)2 
This corresponds to the estimators 
1
n 
n X
i=1 
i and 
1
n 
n X
i=1
(i  n)2

1.7. ESTIMATION 31 
The rst one is simply the sample mean n, which as we recall is an unbiased 
estimator of . However, recalling that the sample variance s2
n is an 
unbiased estimator of 2, i.e., that 
s2
n = 
1 
n  1 
n X
i=1
(i  n)2 and E[s2
n] = 2 
we have that 
E[ 
1
n 
n X
i=1
(i  n)2] = E[
n  1 
n 
1 
n  1 
n X
i=1
(i  n)2] = 
n  1 
n 
E[s2
n] = 
n  1 
n 
2 
This means that the maximum-likelihood estimator for 2 in the normal 
distribution, 
1
n 
n X
i=1
(i  n)2, is not quite unbiased. 
This is an example of the fact that maximum-likelihood estimators are 
not necessarily unbiased. If we return to our previous example, we nd 
that the expectation value of the maximum-likelihood estimator 1=n of 
the  parameter of the exponential distribution does not equal . In fact, 
it equals 
n 
n  1
, so it is also slightly o the mark: 
E[1=n] = Z 1 
1 
n
x
f(1+:::+n)(x) dx = Z 1 
0 
n
x
n xn1 
(n  1)! 
ex dx = 
= 
n 
n  1
 Z 1 
0 
n1 xn2 
(n  2)! 
ex dx = fPartial integration n  1 timesg 
= : : : = 
n 
n  1
[ex]10
= 
n 
n  1
 
In both cases, though, we can construct an unbiased estimator from the 
maximum-likelihood estimator by multiplying with a scaling factor. This 
is actually quite often the case. 
1.7.4 Sucient Statistic 
A sample variable g(1; : : :; n) is said to be a statistic if it does not depend 
on any unknown parameter. For example, if   N(; ), where  is not 
known, then the sample mean n is a statistic, whereas n   isn't, since 
the latter depends on the unknown parameter . 
A statistic g(1; : : :; n) is a sucient statistic for the unknown parameter(
s)  i the conditional distribution given g(1; : : :; n) = s does not 
depend on . Intuitively, this means that as soon as we've xed the value 
of the sample variable (in this case to s), the distribution of the sample 
does no longer depend on  and there is thus no further information to be 
extracted about  from the sample. 
A practically more useful denition of sucient statistic is that the 
frequency function of the random sample, which is identical to the likelihood 
function of the previous section, can be written as a product of two functions 
h1 and h2, one that depends only on the statistical material, and one that 
depends on , but only on the statistical material through the function g 
of the sample variable: 
L(x1; : : :; xn; ) = 
n Y
i=1 
f(xi) = 
= f(1;:::;n)(x1; : : :; xn; ) = h1(g(x1; : : :; xn); )  h2(x1; : : :; xn)

32 CHAPTER 1. BASIC STATISTICS 
The entire point is that the equation g(x1; : : :; xn) = s partitions the 
range of the random sample (i.e., 
n 
) into subsets, one for each value of s, 
that can be viewed as equivalent w.r.t. the parameter , thus condensing 
the statistical material without losing information about . Thus, we have 
revealed the innermost secret of sample variables: They condense data. 
For example, if nu is the number of times some outcome u occurs in n 
trials, then nu is a sucient statistic for the purpose of determining the 
unknown parameter p, the probability of the outcome. Thus no information 
about p is contained in the particular sequence of outcomes of the trials, 
only in the number of times u occurred. So whereas the size of the range 
of the random sample is 2n, the size of the range of the sample variable nu 
is n+1, which nonetheless contains the same amount of information about 
p | a tremendous reduction! 
There does not necessarily exist a sucient statistic, but there is always 
a set of jointly sucient statistics, which is a set of statistics: 
Denition: Let (1; : : :; n) be a random sample and let  be 
the unknown parameter(s). The sample variables g1(1; : : :; n); 
: : :; gr(1; : : :; n) are said to constitute a set of jointly sucient 
statistics i they do not depend on any unknown parameters 
and the conditional distribution given gk(1; : : :; n) = sk, for 
k = 1; : : :; r, does not depend on . 
Why there always is one? Set gk(1; : : :; n) = k for k = 1; : : :; n. Although 
this won't condense the data, the random sample itself is trivially jointly 
sucient, since once we've specied the outcome, its probability is 1, and 
thus does not depend on . 
A set of jointly sucient statistics is said to be minimal sucient if no 
other set of sucient statistics condenses the data more. 
Example: The sample mean n and the sample variance s2
n, 
n = 
1
n 
n X
i=1 
i 
s2
n = 
1 
n  1 
n X
i=1
(i  n)2 
constitute a minimal sucient statistics for the parameters  = 
h; i of the normal distribution. This means that we can't condense 
the statistical material more without losing information 
about  or . 
1.7.5 Condence Intervals 
We have until now only spoken of point estimates of parameters. With 
Chebyshev's Inequality, where we are in essence estimating the probability 
of a random variable taking a value that is \far out", cf. Figure 1.10, we are 
nibbling at the idea of interval estimates. The basic idea with a condence 
interval is that we can give a lower or an upper bound, or both, for the 
parameter , and we can indicate how condent we are that  is in fact 
contained in this interval. 
Let 1 = g1(1; : : :; n) and 2 = g2(1; : : :; n) be two sample variables 
such that: 
P(1 <  < 2) = p

1.7. ESTIMATION 33 
We then call (1; 2) a condence interval with condence degree p. If 
P(1 > ) = P( > 2), the interval is said to be symmetric. This means 
that we have cut o equally much of the probability mass on the left side 
as on the right side. 
Yoga time: The parameter  is a (real) number, and associated 
with no uncertainty or stochastic behavior.  does not take a 
value inside the interval with probability p. It is either inside the 
interval or outside. If it is inside, our method for determining 
the bounds on it worked. If it isn't, we goofed. The condence 
degree p only tells us how often we will in average succeed in 
establishing a correct interval using this method. That's all. For 
example, if we conduct measurements on c0, the speed of light in 
vacuum, and establish that with 99 % percent condence degree 
299; 792; 457 m/s < c0 < 299; 792; 459 m/s, this is our guess at 
what c0 is. In most contemporary theories of Physics, c0 has 
some xed value, and is not subject to random variation. 
In the rest of this section, we will assume that   N(; ) and construct 
condence intervals for  and . 
Estimating  with known  
As established in Section 1.5.2, 
n X
i=1 
i  N(n;pn). Thus n = 
1
n 
n X
i=1 
i  
N(; =pn), or 
n   
 
pn  N(0; 1). Utilizing this fact we have: 
P(x1 < 
n   
 
pn < x2) = (x2)  (x1) 
Rearranging this gives us 
P(n  x2 
 
pn 
<  < n  x1 
 
pn
) = (x2)  (x1) 
This is usually written 
n  x2 
 
pn 
<  < n  x1 
 
pn 
(p) 
where p = (x2)  (x1). In particular, if we want a symmetric interval 
with condence degree p, we choose x = x2 = x1 such that (x) = 
1  (1  p)=2 = (1 + p)=2, resulting in 
n  x 
 
pn 
<  < n + x 
 
pn 
(p) 
Estimating 2 with known  
Let v2n
= 
1
n 
n X
i=1
(i  )2. Then 
nv2n 
2 = 
n X
i=1
( 
i   
 
)2 is the sum of n squares 
of independent normally distributed random variables with expectation 
value 0 and variance 1, i.e., 
nv2n 
2 = Xu2i
, where ui = 
i   
  N(0; 1). 
According to Section 1.5.3, such a random variable has a 2 distribution 
with n degrees of freedom, i.e., we have that 
nv2n 
2  2(n), and thus 
P(x1 < 
nv2n 
2 < x2) = F(x2)  F(x1) = p2  p1

34 CHAPTER 1. BASIC STATISTICS 
where F(x) is the distribution function of the 2 distribution with n degrees 
of freedom, and F(x1) = p1; F(x2) = p2. This gives the following condence 
interval with p = p2  p1: 
nv2n 
x2 
< 2 < 
nv2n 
x1 
(p) 
Estimating 2 with unknown  
If we do not know , we instead use the sample variance s2
n = 
1 
n  1 
n X
i=1
(i 
n)2 to construct 
(n  1)s2
n 
2 = 
n X
i=1
( 
i  n 
 
)2, which can also be written 
as the sum of n squares of normally distributed random variables with 
expectation value 0 and variance 1, i.e., 
(n  1)s2
n 
2 = Xu2i
, where ui = 
i  n 
  N(0; 1). Unfortunately, these uis are not independent since, for 
example, they sum to zero: 
n X
i=1 
ui = 
n X
i=1 
i  n 
 
= 
n X
i=1 
i 
  
n X
i=1 
n 
 
= n 
n 
  n 
n 
 
= 0 
One can however prove that 
n X
i=1 
i  n 
 
can be rewritten as 
n1 
Xi=1 
w2
i where wi 
are indeed independent and  N(0; 1), and we then have that 
(n  1)s2
n 
2  
2(n  1). 
We can thus establish that 
P(x1 < 
(n  1)s2
n 
2 < x2) = F(x2)  F(x1) = p2  p1 
where F(x) is the distribution function of the 2 distribution with n  1 
degrees of freedom, and F(x1) = p1; F(x2) = p2. This gives the following 
condence interval with p = p2  p1: 
(n  1)s2
n 
x2 
< 2 < 
(n  1)s2
n 
x1 
(p) 
Estimating  with unknown  
Of course, it is not generally the case that we know  but not . When 
both are unknown, we can instead use the fact that 
(n  1)s2
n 
2  2(n1) 
and that 
n   
 
pn  N(0; 1). Now let 
sn = ps2
n 
Then 
n   
sn 
pn  t(n  1) and we can establish the analogous result 
n  t 
sn pn 
<  < n + t 
sn pn 
(p) 
where F is the t distribution function with n  1 degrees of freedom, and 
F(t) = 1  (1  p)=2 = (1 + p)=2.

1.7. ESTIMATION 35 
Worked Example: Given the following temperature measurements, 
x1 x2 x3 x4 x5 x6 
20.1 C 19.7 C 19.9 C 20.2 C 19.8 C 20.0 C 
estimate a symmetric condence interval for the temperature. 
We will assume a normal distribution, i.e., i  N(; ) for 
i = 1; : : : ; 6. We introduce the sample variables 6 and s26
: 
6 = 
1
6 
6 X
i=1 
i 
s26
= 
1
5 
6 X
i=1
(i  6)2 
From this with can construct a new sample variable with a t 
distribution with ve degrees of freedom: 
6   
ps26 
=6  t(5) 
Thus, the following is a symmetric condence interval 
 = x6  tp^s26 
p6 
(p) 
where t is determined from the t-distribution function with 5 
degrees of freedom and the desired condence degree p. Let us 
assume that we want p to be 0.95: 
F(t) = 1  (1  p)=2 = 1=2 + p=2 = 1=2 + 0:95=2 = 0:975 
From the t-distribution table we nd t = 2:571 and from the 
data we calculate 
x6 = 
1
6 
6 X
i=1 
xi = 19:95 C 
^s26
= 
1
5 
6 X
i=1
(xi  x6)2 = 0:035 (C)2 
tp^s26 
p6 
= 0:196 C < 0:20 C 
This yields us 
 = 19:95 C  0:20 C (0:95) 
1.7.6 Hypothesis Testing and Signicance 
Assume that we wish to prove something, e.g. that some gambler is using 
a loaded die with a distinct bias towards (or against) the outcome Six. 
We then throw the die a large number of times and observe the relative 
frequency of the outcome Six. If this deviates considerably from 
1
6
, we 
conclude that the die is loaded. 
How can we get this more mathematically well-founded? Well, having 
just learned a lot about condence intervals, we will use similar techniques.

36 CHAPTER 1. BASIC STATISTICS 
We create an estimator with a known distribution that depends on p, the 
probability of Six, in much the same way that we would when constructing 
a condence interval. In this case, we know that nfn  bin(n; np) and 
that thus P(x1 < nfn < x2) = F(x2)  F(x1), where F(x) is the binomial 
distribution function. To simplify the calculations for large values of n, we 
can instead use that 
(fn  p)pn 
pp(1  p) 
is approximately  N(0; 1) to establish 
that 
P(p  1:96rp(1  p) 
n 
< fn < p+ 1:96rp(1  p) 
n 
)  0:95 
As we recall from Section 1.5.4, 1.96 is the 97.5-percent fractile of the 
Standard Normal distribution. Since the condence interval is symmetric, 
this means that we cut o 2.5 percent of the probability mass in both ends, 
leaving 95 percent. 
Under the assumption that p is in fact 
1
6
, this means that 
P(
1
6  
0:73 
pn 
< fn < 
1
6 
+ 
0:73 
pn 
)  0:95 
If the observed relative frequence is not in this interval, we argue that the 
observed outcome is so unlikely if p were in fact 
1
6
, that we can conclude 
that p is not 
1
6
. If for example we throw the die 100 times and it shows Six 
in 25 of them, the observed relative frequency 0.25 is not contained in this 
interval, and we conclude that the die is in fact loaded, and our gambler 
has had it. 
This is an example of a hypothesis test. The basic line of reasoning to 
extract from this is that if the thing we want to prove false were really true, 
then the observed data would be very unlikely. More formally, we assume 
that a null hypothesis H0, that we want to prove false, is really true. We 
then calculate the probability of as extreme a statistical material as the one 
observed, given that H0 is indeed true. Now, if this probability is less then 
some predened value p, we can discard the null hypothesis at signicance 
level p. In the above example, the signicance level is 5%. The signicance 
level is related to the condence degree in that a method for constructing 
a condence interval with condence degree 1p can be used to construct 
a hypothesis test with signicance level p. 
Note that if we cannot discard the null hypothesis, this doesn't mean 
that we may conclude that it is true. It may very well be false or highly 
unlikely, but the available statistical material does not allow us to discard 
it at the desired signicance level. 
Yoga time: Again, the null hypothesis H0 is associated with 
no uncertainty or stochastic behavior. It is either true or false. 
If we decide to discard H0 and it is indeed false, the method 
worked. If it is in fact true, we goofed. If we do not decide to 
discard H0, we have not made any mistake, but we have on the 
other hand not gained any new knowledge about the world. The 
signicance level p only gives us an indication of how often we 
will wrongly conclude that the null hypothesis is false. That's 
all.

1.8. FURTHER READING 37 
Note that the signicance level and the condence degree are not symmetrical: 
A condence interval with 0 % condence degree will never contain 
the estimated parameter, but while a hypothesis test with signicance 
level 100 % would be a very crappy test indeed, this does not exclude the 
possibility that the null hypothesis may in fact be wrong. 
1.8 Further Reading 
For other literature covering the material presented in this chapter, as well 
as additional material, we strongly recommend [DeGroot 1975]. Another 
good text book is [Mood et al 1974]. We plan to extend the section on 
estimation considerably. Estimation and hypothesis testing are related to 
Multivariate Analysis, see Section 2.3 when this has been written.

38 CHAPTER 1. BASIC STATISTICS

Chapter 2 
Applied Statistics 
In this chapter we will apply the theory of stochastic variables presented in 
the previous chapter to stochastic processes and information theory. There 
is however a slight complication stemming from the fact that the theoretical 
apparatus of the previous chapter assumes that the range of the random 
variables are numbers. Here, they will be nite sets, but not necessarily 
subsets of the natural numbers. This can be dealt with by constructing 
a mapping between the set of outcomes of the random variable and the 
natural numbers. In particular, since the range of any random variable 
is nite, it will always be of the form fx1; : : :; xNg. We will thus tacitly 
assume the mapping xi ; i and merrily apply our high-powered theoretical 
machinery. 
2.1 Markov Models 
This section is partly due to Thorsten Brants. Here we will briey discuss 
stochastic processes in general, before turning to Markov chains, Markov 
models and hidden Markov models. We will then proceed to discuss how to 
eciently calculate the involved probabilities, how to determine the most 
likely state sequence using the Viterbi algorithm, and how to nd an optimal 
parameter setting using Baum-Welch reestimation. 
2.1.1 Stochastic Processes 
A stochastic or random process is a sequence 1; 2; : : : of random variables 
based on the same sample space 
. The possible outcomes of the random 
variables are called the set of possible states of the process. The process 
will be said to be in state t at time t. Note that the random variables are 
in general not independent. In fact, the interesting thing about stochastic 
processes is the dependence between the random variables. 
Example: Returning to the imperishable example of the unbiased 
die, the sequence of outcomes when repeatedly casting 
the die is a stochastic process with discrete random variables 
and a discrete time parameter. 
Example: Let us consider another simple example | the telephone 
example. Here we have three telephone lines, and at 
any given moment 0, 1, 2 or 3 of them can be busy. Once every 
minute we will observe how many of them are busy. This will be 
39

40 CHAPTER 2. APPLIED STATISTICS 
a random variable with 
 = f0; 1; 2; 3g. Let 1 be the number 
of busy lines at the rst observation time, 2 the number busy 
lines at the second observation time, etc. The sequence of the 
number of busy lines then forms a random process with discrete 
random variables and a discrete time parameter. 
We will here only consider stochastic processes with a discrete time 
parameter and a nite sample space. Measuring the voltage in one of the 
telephone lines every minute, instead of counting the number of busy lines, 
in the previous example would be an example of a random process with 
continuous random variables and a discrete time parameter. Continuously 
monitoring the number of busy lines at every point in time would be an 
example of a random process with a continuous time parameter and a nite 
sample space. 
To fully characterize a random process with a discrete time parameter 
t, we need to specify 
1. the probability P(1 = xj) of each outcome xj for the rst observation, 
i.e., the initial state 1 and 
2. for each subsequent observation (state) t+1 : t = 1; 2; : : : the conditional 
probabilities P(t+1 = xit+1 j 1 = xi1; : : :; t = xit). 
To avoid getting into a lot of mathematical trouble with dening distributions 
over innite sequences of random variables, we can terminate after 
some nite number of steps T, and instead use our knowledge of multidimensional 
stochastic variables. 
2.1.2 Markov Chains and the Markov Property 
A Markov chain is a special type of stochastic process where the probability 
of the next state conditional on the entire sequence of previous states up 
to the current state is in fact only dependent on the current state. This is 
called the Markov property and can be stated as: 
P(t+1 = xit+1 j 1 = xi1; : : :; t = xit) = P(t+1 = xit+1 j t = xit) 
This means that the probability of a Markov chain 1; 2; : : : can be 
calculated as: 
P(1 = xi1; : : :; t = xit) = 
= P(1 = xi1)  P(2 = xi2 j 1 = xi1)  : : :  P(t = xit j t1 = xit1) 
The conditional probabilities P(t+1 = xit+1 j t = xit) are called the 
transition probabilities of the Markov chain. A nite Markov chain must 
at each time be in one of a nite number of states. 
Example: We can turn the telephone example with 0, 1, 2 or 
3 busy lines into a (nite) Markov chain by assuming that the 
number of busy lines will depend only on the number of lines 
that were busy the last time we observed them, and not on the 
previous history. 
Consider a Markov chain with n states s1; : : :; sn. Let pij denote the 
transition probability from State si to State sj , i.e., P(t+1 = sj j t = si). 
The transition matrix for this Markov process is then dened as 
P = 24 
p11    p1n 
         pn1    pnn 
35 
; pij  0; 
n X
j=1 
pij = 1; i = 1; : : :; n

2.1. MARKOV MODELS 41 
In general, a matrix with these properties is called a stochastic matrix . 
Example: The stochastic process derived from the immortal 
example of the unbiased die, can be described by a six-by-six 
matrix where each element is 1=6. 
Example: In the telephone example, a possible transition matrix 
could for example be: 
s0 s1 s2 s3 
P = 
s0 
s1 
s2 
s3 
2664 
0:2 0:5 0:2 0:1 
0:3 0:4 0:2 0:1 
0:1 0:3 0:4 0:2 
0:1 0:1 0:3 0:4 
3775
Assume that currently, all three lines are busy. What is then 
the probability that at the next point in time exactly one line 
is busy? The element in Row 3, Column 1 (p31) is 0.1, and 
thus p(1j3) = 0:1. (Note that we have numbered the rows and 
columns 0 through 3.) 
We can now determine the transition matrix for several steps. The 
following is the transition matrix for two steps: 
p(2) 
ij = P(t+2 = sj j t = si) 
= Pn
r=1 P(t+1 = sr \ t+2 = sj j t = si) 
= Pn
r=1 P(t+1 = sr j t = si)  P(t+2 = sj j t+1 = sr) 
= Pn
r=1 pir  prj 
The element p(2) 
ij can be determined by matrix multiplication. This is the 
element in Row i, Column j of the matrix P2 = PP. In the same way, we 
can determine the transition probabilities for several steps. In general, the 
transition matrix for t steps is Pt. 
Example: Returning to the telephone example: Assuming that 
currently all three lines are busy, what is the probability of exactly 
one line being busy after two steps in time? 
s0 s1 s2 s3 
P2 = PP = 
s0 
s1 
s2 
s3 
2664 
0:22 0:37 0:25 0:15 
0:21 0:38 0:25 0:15 
0:17 0:31 0:30 0:20 
0:12 0:22 0:28 0:24 
3775
) p(2) 
31 = 0:22 
A vector v = [v1; : : :; vn] with vi  0; i = 1; : : :; n and Pn
i=1 vi = 1 is 
called a probability vector. The probability vector that determines the state 
probabilities of the observations of the rst element (state) of a Markov 
chain, i.e., where vi = P(1 = si), is called an initial probability vector. 
The initial probability vector and the transition matrix together determine 
the probability of the chain being in a particular state at a particular point 
in time. This probability can be calculated by multiplying the vector v with 
the matrix P the appropriate number of times. If p(t)(si) is the probability 
of the chain being in state si at time t, i.e., after t  1 steps, then 
[p(t)(s1); : : :; p(t)(sn)] = vPt1

42 CHAPTER 2. APPLIED STATISTICS 
Example: Returning to the telephone example: Let v = [0:5; 0:3; 0:2; 0:0]. 
What is then the probability that after two steps exactly two 
lines are busy? 
s0 s1 s2 s3 
vP2 = vPP =  0:21 0:43 0:24 0:12  ) p(2 = s2) = 0:24. 
2.1.3 Markov Models 
Let us attribute each state in a Markov chain with a nite set of signals. 
After each transition, one of the signals associated with the current state 
is emitted. Thus, we introduce a new sequence of random variables t; t = 
1; :::; T representing the signal emitted at time t. This determines a Markov 
model. 
A Markov model consists of: 
 a nite set of states 
 = fs1; : : :; sng; 
 an signal alphabet  = f1; : : :; mg; 
 an n  n state transition matrix P = [pij] where pij = P(t+1 = sj j t = si); 
 an n  m signal matrix A = [aij], which for each state-signal pair 
determines the probability aij = p(t = j j t = si) that signal j 
will be emitted given that the current state is si; 
 and an initial vector v = [v1; : : :; vn] where vi = P(1 = si). 
The probability of reaching some particular state at some particular time 
is determined just as in the case of the Markov chain. The probability that 
signal j will be emitted in the current state si is thus precisely the element 
aij of the signal matrix. We have thus made a second Markov assumption, 
namely that the probability of a particular signal being emitted does only 
depend on the current state, and not on the sequence of pervious ones. 
From this it follows that the probability of the model being in state si 
and emitting signal j at time t is 
p(t)(si; j) = p(t)(si)  p(t = j j t = si) 
where p(t)(si) is the ith element of the vector vPt1. The probability that 
signal j will be emitted at time t is then: 
p(t)(j) = 
n X
i=1 
p(t)(si; j) = 
n X
i=1 
p(t)(si)  p(t = j j t = si) 
Thus if p(t)(j) is the probability of the model emitting signal j at time 
t, i.e., after t  1 steps, then 
[p(t)(1); : : :; p(t)(m)] = vPt1A 
The Markov models described this far are of rst order, i.e., the probability 
of the next state depends only on the current state. In an nth order 
Markov model, this transition probability depends on the way the current 
state was reached, and in particular on the n  1 previous states. This

2.1. MARKOV MODELS 43 
means that we need to specify the probabilities P(sit j sitn; : : :; sit1 ). A 
Markov model of higher (nth) order can always be reduced to an equivalent 
rst-order Markov model by expanding out each possible sequence of 
n states in the old Markov model to a state in the new Markov model. 
2.1.4 Hidden Markov Models 
If it is not possible to observe the sequence of states 1; : : :; T of a Markov 
model, but only the sequence 1; : : :; T of emitted signals, the model is 
called a hidden Markov model (an HMM). 
Let O 2  be a known sequence of observed signals and let S 2 
 be 
the unknown sequence of states in which O was emitted. Our best guess 
at S is the sequence maximizing 
max 
S 
P(S j O) 
From Bayes' inversion formula (Eq. 1.3), redisplayed here for your convenience, 
P(S j O) = 
P(O j S)  P(S) 
P(O) 
we see that this S will also maximize 
max 
S 
P(O j S)  P(S) 
since P(O) does not depend on the choice of S. Here P(O j S) is called the 
signal model and P(S) is called the language model. Maximizing P(O j S) 
alone would be the maximum-likelihood estimate of S, see Section 1.7.3. 
Prototypical tasks to which hidden Markov models are applied include 
the following. Given a sequence of signals O = (i1; : : :; iT ): 
 Estimate the probability of observing this particular signal sequence. 
 Determine the most probable state sequence that can give rise to this 
signal sequence. 
 Determine the set of model parameters  = (P;A; v) maximizing the 
probability of this signal sequence. 
Part-of-speech tagging and speech recognition are examples of the second. 
Language identication would be an example of the rst. The third problem 
is of interest when creating a hidden Markov model. 
Example: Part-of-speech tagging. The set of observable signals 
are the words of an input text. The states are the set of 
tags that are to be assigned to the words of input text. The 
task consists in nding the most probable sequence of states 
that explains the observed words. This will assign a particular 
state to each signal, i.e., a tag to each word. 
Example: Speech recognition. The set of observable signals 
are (some representation of the) acoustic signals. The states 
are the possible words that these signals could arise from. The 
task consists in nding the most probable sequence of words 
that explains the observed acoustic signals. This is a slightly 
more complicated situation, since the acoustic signals do not 
stand in a one-to-one correspondence with the words.

44 CHAPTER 2. APPLIED STATISTICS 
We can easily specify HMMs at dierent levels. For example, if we have 
an HMM for each phoneme dened in terms of the acoustic signals, an 
HMM for each word dened in terms of the phonemes, and an HMM for 
word pairs (using bigram statistics), we can expand this to a single HMM 
where each acoustic signal in each phoneme in each word pair corresponds 
to a unique state. Of course, this increases the number of states drastically, 
and thus also the search space. 
2.1.5 Calculating P(O) 
We now turn to the problem of actually calculating the probability P(O) 
of the observed signal sequence. Let O = (k1; : : :; kT ) and let S = 
(si1 ; : : :; siT ). Then 
P(O j S) = 
T Y
t=1 
P(t = kt j t = sit) = 
T Y
t=1 
aitkt 
P(S) = vi1  
T Y
t=2 
pit1it 
P(O\ S) = P(O j S)  P(S) = ( 
T Y
t=1 
aitkt)  (vi1  
T Y
t=2 
pit1it) = 
= (ai1k1  vi1)  
T Y
t=2 
pit1it  aitkt 
Putting this together yields us 
P(O) = XS 
P(O\ S) = X si1 ;:::;siT 
(ai1k1  vi1)  
T Y
t=2 
pit1it  aitkt 
Using this formula directly requires O(2TnT ) calculations. Obviously, this 
is not a good idea computationally. 
The forward algorithm 
Instead, we reshue the expression and introduce a set of accumulators, 
so-called forward variables, one for each time t and state i: 
t(i) = P(Ot; t = si) = P(1 = k1; : : :; t = kt; t = si) 
This is the joint probability of being in state si at time t and the observed 
signal sequence Ot = k1; : : :; kt from time 1 to time t. 
Note that 
P(O) = P(1 = k1; : : :; T = kT ) = 
= 
n X
i=1 
P(1 = k1; : : :; T = kT ; T = si) = 
n X
i=1 
T (i) 
that
1(i) = P(1 = k1 ; 1 = si) = aik1  vi for i = 1; : : :; n 
and that 
t+1(j) = [ 
n X
i=1 
t(i)  pij]  ajkt+1 for t = 1; : : :; T  1 ; j = 1; : : :; n

2.1. MARKOV MODELS 45 
since 
P(Ot+1; t+1 = sj) = 
= 
n X
i=1 
p(Ot; t = si)  P(t+1 = kt+1; t+1 = sj j Ot; t = si) = 
= 
n X
i=1 
P(Ot; t = si)  P(t+1 = kt+1 j t+1 = sj)  P(t+1 = sj j t = si) 
The rst equality is exact, while the second one uses the Markov assumptions. 
Calculating P(O) this way is known as the forward algorithm and 
requires O(n2T) calculations, which is a considerable savings. 
The trellis 
An ecient way of representing this is by the use of a trellis, i.e., a graph 
with a node for each state-time pair, where each node at time t is connected 
only to the nodes at times t 1 and t+ 1. Figure 2.1 shows an example of 
a trellis. Each node in the trellis corresponds to being in a particular state 
at a particular time, and can be attributed some appropriate accumulator, 
for example a forward variable. Using the appropriate recurrence equation, 
we can calculate the values of the accumulators for the next time t+1 from 
those of the current time t, or possibly the other way around. 
n 
1 
2
1 
3 
3 
2 T 
e
t
a
t
s 
t e m i 
Figure 2.1: Trellis 
The backward algorithm 
Alternatively, we can dene the set of backward variables: 
t(i) = P(O>t j t = si) = P(t+1 = kt+1; : : :; T = kT j t = si)

46 CHAPTER 2. APPLIED STATISTICS 
This is the probability of the observed signal sequence O>t = kt+1; : : :; kT 
from time t + 1 to time T conditional on being in state si at time t. 
Note that 
P(O) = 
n X
i=1 
P(1 = k1; 1 = si)  P(2 = k2; : : :; T = kT j 1 = si) = 
= 
n X
i=1 
aik1  vi  1(i) 
Let us dene 
T (i) = 1 for i = 1; : : :; n 
and note that 
t(i) = 
n X
j=1 
pij  ajkt+1  t+1(j) for t = 1; : : :; T  1 ; i = 1; : : :; n 
since 
P(O>t j t = si) = 
n X
j=1 
P(O>t; t+1 = sj j t = si) = 
= 
n X
j=1 
P(O>t j t = si; t+1 = sj)  P(t+1 = sj j t = si) = 
= 
n X
j=1 
p(t+1 = kt+1 j t+1 = sj)  P(O>t+1 j t+1 = sj)  P(t+1 = sj j t = si) 
Again, the rst equality is exact, while the second one uses the Markov 
assumptions. Calculating P(O) using the backward variables is called the 
backward algorithm and also requires O(n2T) calculations. 
The forward-backward algorithm 
We can also use a mixture of the forward and backward variables since 
P(O) = 
n X
i=1 
P(O; t = si) = 
n X
i=1 
P(Ot; t = si)  P(O>t j Ot; t = si) = 
= 
n Xi=1 
P(Ot; t = si)  P(O>t j t = si) = 
n X
i=1 
t(i)  t(i) 
Thus we need the forward variables t(i) and backward variables t(i) at 
time t. These can be calculated recursively from the forward variables 
for time 1; : : :; t and the backward variables for time t; : : :; T. This way of 
calculating P(O) is called the forward-backward algorithm and also requires 
O(n2T) calculations. 
In fact, we can dene a set of forward-backward variables: 
t(i) = P(t = si j O) = 
P(O; t = si) 
P(O) 
= 
t(i)  t(i) 
Pn
i=1 t(i)  t(i) 
This is the probability of being in state si at time t conditional on the entire 
observed signal sequence O from time 1 to time T.

2.1. MARKOV MODELS 47 
2.1.6 Finding the Optimal State Sequence 
A rst attempt at nding an optimal sequence of states could be the one 
maximizing the -factors individually: 
si
t = arg max 
1in
t(i) 
This does however not take the dependencies between the random variables 
into account. In particular, the constructed sequence may have zero 
probability, since some transitions may be impossible. In fact, due to the 
Markov assumption(s), this turns out to be the maximum-likelihood estimate 
maxS P(O j S). Instead we want to nd the state sequence that 
maximizes P(O j S)  P(S). To this end we employ a dynamic programming 
technique | the Viterbi algorithm. 
The Viterbi algorithm 
We will instead dene the set of variables 
t(i) = max 
St1 
P(St1; t = si;Ot) = 
= max 
si1 ;:::;sit1 
P(1 = si1 ; : : :; t1 = sit1; t = si;Ot) 
This is the joint probability of the most likely state sequence from time 1 
to time t ending in state si and the observed signal sequence Ot up to 
time t. We also introduce the vector  t(i) which indicates the predecessor 
of state si in the path corresponding to t(i). 
Note that 
1(i) = vi  aik1 for i = 1; : : :; n 
that 
t(j) = [max 
i 
t1(i)  pij]  ajkt for t = 2; : : :; T ; j = 1; : : :; n 
 t(j) = arg max 
1in
(t1(i)  pij) for t = 2; : : :; T ; j = 1; : : :; n 
that 
P = max 
i 
T (i) 
s
kT = arg max 
1in
T (i) 
and that 
s
kt =  t+1(s
kt+1 ) for t = 1; : : :; T  1 
The recurrence equation can be established as follows: 
t(j) = max 
St1 
P(St1; t = sj ;Ot) = 
= max 
i 
max 
St2 
P(St2; t1 = si; t = sj ;Ot1; t = kt) = 
= max 
i 
max 
St2 
P(t = sj ; t = kt j St2; t1 = si;Ot1)  
 P(St2; t1 = si;Ot1) =

48 CHAPTER 2. APPLIED STATISTICS 
= max 
i 
max 
St2 
P(t = sj j t1 = si)  P(t = kt j t = sj)  
 P(St2; t1 = si;Ot1) = 
= [max 
i 
P(t = sj j t1 = si)  max 
St2 
P(St2; t1 = si;Ot1)]  
 P(t = kt j t = sj) = 
= [max 
i 
pij  t1(i)]  ajkt 
The rst three equalities are exact, while the fourth one uses the Markov 
assumptions. 
The Viterbi algorithm can be implemented eciently using a trellis, and 
the main dierence to the forward algorithm for calculating P(O) is that 
here the accumulators represent maximums, rather than sums. In fact, the 
number of required calculations is the same, namely O(n2T). 
The Viterbi algorithm is discussed in more detail in [Forney 1973], while 
general dynamic programming is well-described in [Bellman 1957]. 
2.1.7 Parameter Estimation for HMMs 
We now turn to the problem of estimating the parameters v, P and A 
dening the HMM. If we have access to annotated training data, i.e., data 
where we know both the sequence of signals and the underlying sequence of 
states, we can calculate the parameters directly from the observed relative 
frequencies. It is however in this case necessary to deal appropriately with 
the problem of sparse data as discussed in Section 2.4. 
If we only have training data consisting of signal sequences where the 
underlying state sequences are unknown, we have to resort to other methods. 
Here we will dene a set of recurrence equations that can be used 
to iteratively improve our estimates of the parameters. By doing this right, 
we can get some guarantees that each iteration will yield us a better set of 
estimates. 
To this end, we rst dene the set of probabilities 
t(i; j) = P(t = si; t+1 = sj j O) = 
P(O; t = si; t+1 = sj) 
O 
This is the joint probability of being in state si at time t and of being in 
state sj at time t+ 1 conditional on the entire observed signal sequence O 
from time 1 to time T. 
Note that 
t(i; j) = 
t(i)  pij  ajkt+1  t+1(j) 
P(O) 
= 
t(i)  pij  ajkt+1  t+1(j) 
Pi;j t(i)  pij  ajkt+1  t+1(j) 
since 
P(O; t = si; t+1 = sj) = P(Ot+1;O>t+1; t = si; t+1 = sj) = 
= P(Ot+1; t = si; t+1 = sj)  P(O>t+1 j Ot+1; t = si; t+1 = sj) = 
= P(Ot; t = si)  P(t+1 = kt+1; t+1 = sj j Ot; t = si)  
 P(O>t+1 j t+1 = sj) = 
= P(Ot; t = si)  P(t+1 = sj j t = si)  P(t+1 = kt+1 j t+1 = sj)  
 P(O>t+1 j t+1 = sj) = 
= t(i)  pij  ajkt+1  t+1(j)

2.1. MARKOV MODELS 49 
For this reason, the probabilities t(i; j) are also referred to as forwardbackward 
variables. 
We immediately see that 
t(i) = P(t = si j O) = 
n X
j=1 
P(t = si; t+1 = sj j O) = 
n X
j=1 
t(i; j) 
We now observe that 
1(i) = probability of starting in state si 
T1 
Xt=1 
t(i; j) = expected number of transitions from state si to state sj 
T1 
Xt=1 
t(i) = expected number of transitions from state si 
T X
t=1:kt=j 
t(i) = expected number of times signal j is emitted in state si 
We can now establish the recurrence equations: 
vi = 1(i) 
pij = PT1 
t=1 t(i; j) 
PT1 
t=1 t(i) 
aij = PT
t=1:kt=j 
t(i) 
PT
t=1 t(i) 
This is known as the Baum-Welch or forward-backward reestimation algorithm, 
but can also be interpreted as an incarnation of the EM (expectationmodi
cation) algorithm or as an application of Lagrange multipliers to enforce 
the constraints 
n X
i=1 
vi = 1 
n X
j=1 
pij = 1 for i = 1; : : :; n 
m X
j=1 
aij = 1 for i = 1; : : :; n 
when maximizing P(O) w.r.t.  = (v;P;A). The resulting equations are 
vi = 
vi 
@P 
@vi 
Pn
k=1 vk 
@P 
@vk 
for i = 1; : : :; n 
pij = 
pij 
@P 
@pij 
Pn
k=1 pik 
@P 
@pik 
for i = 1; : : :; n ; j = 1; : : :; n 
aij = 
aij 
@P 
@aij 
Pm
k=1 aik 
@P 
@aik 
for i = 1; : : :; n ; j = 1; : : :;m 
which turn out to be exactly the reestimation equations given above after 
some symbolic manipulation, see [Rabiner 1989].

50 CHAPTER 2. APPLIED STATISTICS 
Let P(O j ) denote the probability of the observation sequence O 
calculated using the old parameters v, P and A, and let P(O j 
) denote 
this probability calculated using the new parameters v, P
and A. Some 
nice people have shown that either 
=  or P(O j 
) > P(O j ), see 
[Baum 1972]. This means that we either get a better set of parameters, or 
the same one, in the next iteration. 
2.1.8 Further reading 
A comprehensive tutorial on hidden Markov models for speech recognition 
is given in [Rabiner 1989]. 
2.2 Elementary Information Theory 
2.2.1 Entropy 
Let  be a discrete stochastic variable  with a nite range 
 = fx1; : : :; xMg and let pi = p(xi) be the corresponding probabilities. How much information 
is there in knowing the outcome of ? Or equivalently: How much 
uncertainty is introduced by not knowing the outcome of ? This is the 
information needed to specify which of the xi has occurred. We could do 
this simply by writing \xi", but let us assume further that we only have a 
small set of symbols A = fak : k = 1; : : :;Dg | a coding alphabet | at our 
disposal to do this. Thus each xi will be represented by a string over A. 
To get this further grounded, assume that  is in fact uniformly distributed, 
i.e., that pi = 1 
M for i = 1; : : :M, and that the coding alphabet is 
f0; 1g. Thus, each xi will be represented by a binary number. We will then 
need N : 2N1 < M  2N digits to specify which xi actually occurred. 
Thus we need log2M digits1. 
So what if the distribution is nonuniform, i.e., if the pis are not all 
equal? How much uncertainty does a possible outcome with probability 
pi introduce? The basic assumption is that it will introduce equally much 
uncertainty regardless of the rest of the probabilities pj : j 6= i. We can 
thus reduce the problem to the case where all outcomes have probability pi. 
In such a case, there are 1 
pi 
= Mpi possible outcomes. The fact that this 
is not in general an integer can be handled with Mathematical Yoga. We 
thus need log2Mpi = log2 
1 
pi 
= log2 pi binary digits to specify that the 
outcome was xi. Thus, the uncertainty introduced by pi is in the general 
case log2 pi. The uncertainty introduced by the random variable  will 
be taken to be the expectation value of the number of digits required to 
specify the outcome. This is the expectation value of log2 P(), i.e., 
E[log2 P()] = Pipi log2 pi. 
Now, we let D be an arbitrary base D > 0;D 6= 1, and not necessarily 
2. Since log2X = log2 D  logD X, this does not change anything material, 
only introduces a scaling factor log2D, so the uncertainty could equally 
well be taken to be Pi pi logD pi, for any base D >0;D 6= 1. We will use 
the particularly good base e  2:72, which is not an integer, nor a rational 
number, and where \loge" is usually denoted \ln". 2 Having already dealt 
with sets of noninteger size, to have a coding alphabet with a noninteger 
number of code symbols should be only slightly unsettling. This allows us 
1Or really, the smallest integer N  log2M. 
2See Appendix C.3 for a discussion of the number e and the function ln x.

2.2. ELEMENTARY INFORMATION THEORY 51 
to dene the quantity H[], called the entropy of : 
H[] =  X xi2
 
p(xi) lnp(xi) =  
M X
i=1 
pi ln pi 
If qi is another set of probabilities on 
, then 
 
M X
i=1 
pi ln pi   
M X
i=1 
pi ln qi (2.1) 
Equality is equivalent to 8i pi = qi. 
Proof: The logarithm is a convex function on the real line, i.e., 
f(x)  f(x0)+f0(x0) (xx0) for all x 2 R. Since d 
dx ln x = 1x 
, 
choosing x0 = 1 and thus f(x0) = 0 and f0(x0) = 1, we have 
lnx  x  1 for all x 2 R. In fact, if x 6= 1, then lnx < x  1. 
In particular 
ln 
qi 
pi  
qi 
pi  1 
By multiplying by pi and summing over i we obtain 
M X
i=1 
pi ln 
qi 
pi  
M X
i=1 
pi( 
qi 
pi  1) = 
M X
i=1 
qi  
M X
i=1 
pi = 1  1 = 0 
Thus 
M X
i=1 
pi ln 
qi 
pi 
= 
M X
i=1 
pi ln qi  
M X
i=1 
pi ln pi  0 
and the proposition follows. We note that if for any i, 0 6= pi 6= 
qi, we have 
pi ln 
qi 
pi 
< qi  pi 
and we obtain strict inequality through a similar summation. 2 
The joint entropy of  and  is dened as: 
H[; ] =  
M X
i=1 
L Xj=1 
p(xi; yj) lnp(xi; yj) 
The conditional entropy of  given  is dened as: 
H[ j ] = 
=  
L X
j=1 
p(yj) 
M X
i=1 
p(xi j yj) lnp(xi j yj) =  
L X
j=1 
M X
i=1 
p(xi; yj) lnp(xi j yj) 
The conditional and joint entropies are related just like the conditional 
and joint probabilities: 
H[; ] = H[] + H[ j ]

52 CHAPTER 2. APPLIED STATISTICS 
This can easily be established from the above denitions by utilizing the 
fact that p(xi; yj) = p(yj)p(xi j yj) and rearranging the summation orders. 
The following two inequalities hold 
H[; ]  H[] +H[] 
H[ j ]  H[] 
where in both cases equality is equivalent to  and  being independent. 
The rst one is an immediate consequence of the second one, which in turn 
can be established much in the same way as the previous equation using 
Eq. 2.1. 
The information conveyed by , denoted I[ j ], is the reduction in 
entropy of  by nding out the outcome of . This is dened by 
I[ j ] = H[] H[ j ] 
Note that: 
I[ j ] = H[] H[ j ] = H[]  (H[; ] H[]) = 
= H[] + H[] H[; ] = H[] + H[]  H[; ] = I[ j ] 
This means that the information about  conveyed by  is necessarily equal 
to the information about  conveyed by . 
Finally, we note that: 
H[] = E[ln P()] 
H[ j ] = E[ln P( j )] 
I[ j ] = H[]  H[ j ] = E[lnP()]  E[ln P( j )] = 
= E[ln P() + lnP( j )] = E[ln 
P() 
P( j ) 
] 
All this this can readily be generalized to several variables by instead 
letting  and  denote sets of variables, and letting i and j be multiindices, 
i.e., ordered tuples of indices. 
2.2.2 Related Information Measures 
Mutual Information 
A very popular quantity in the Computational Linguistics literature around 
1990 was Mutual Information Statistics, denoted MI. If  and  are two 
discrete random variables, we can dene the mutual information as: 
MI[; ] = E[ln 
P(; ) 
P()  P() 
] 
Note that this is symmectic in  and , i.e., that MI[; ] = MI[; ]. 
Actually, the quantity presented as the mutual information in most of these 
publications was the corresponding expression for the outcomes xi, and yj: 
MI[xi; yj ] = ln 
P(xi; yj) 
P(xi)  P(yj) 
If MI[xi; yj ]  0, there is a strong correlation between xi and yj; if 
MI[xi; yj]  0, there is a strong negative correlation. We will however 
let mutual information refer to the quantity with the expectation value.

2.2. ELEMENTARY INFORMATION THEORY 53 
Now 
MI[; ] = E[ln 
P(; ) 
P()  P() 
] = E[ln 
P( j ) 
P() 
] = E[ln 
P() 
P( j ) 
] 
Comparing this with the expression for the conveyed information given at 
the end of Section 2.2.1, we nd 
I[ j ] = E[ln 
P() 
P( j ) 
] = MI[; ] 
Thus, the mutual information is identical to the conveyed information. This 
incidentally allows us to see the symmetry of the conveyed information: 
I[ j ] = MI[; ] = MI[; ] = I[ j ] 
Perplexity 
The perplexity of a random variable  is the exponential of its entropy, i.e. 
Perp[] = eH[] 
To take a linguistic example, assume that we are trying to predict the next 
word t in a word string from the previous t  1 ones 1; :::; t1. The 
entropy is then a measure of how hard this prediction problem is. Let 
P(wi
t j w1; :::;wt1) be an abbreviation of P(t = wi j 1 = wk1 ; :::; t1 = 
wkt1 ). 
H[t j w1; :::;wt1] = 
N X
i=1 
 P(wi
t j w1; :::;wt1)  ln P(wi
t j w1; :::;wt1) 
If all words have equal probability, i.e., P(wit
j w1; :::;wt1) = 1 
N , the 
entropy is the logarithm of the branching factor N at this point in the 
input string:
H[t j w1; :::;wt1] = 
N X
i=1 
 
1
N  ln 
1 
N 
= lnN 
This means that the perplexity eln N is the branching factor N itself. If the 
probabilities dier, the perplexity can be viewed as a generalized branching 
factor that takes this into account. 
In [Jelinek 1990] we nd the following denition of perplexity: 
ln PerpE = lim 
t!1
1
t 
lnP(w1; :::;wt) 
where we let PerpE denote the empirical perplexity. Here P(w1; :::;wt) 
denotes the probability of the word string w1; :::;wt, and should be read as 
an abbreviation of P(1 = wk1; :::; t = wkt). 
Since we cannot experimentally measure innite limits, we terminate 
after a nite test string w1; :::;wT, arriving at the measured perplexity 
PerpM: 
ln PerpM =  
1
T 
lnP(w1; :::;wT)

54 CHAPTER 2. APPLIED STATISTICS 
Rewriting P(w1; :::;wt) as P(wt j w1; :::;wt1)  P(w1; :::;wt1) gives us 
ln PerpM = 
1
T 
T X
t=1 
lnP(wt j w1; :::;wt1) 
What is the expectation value of ln PerpM? Let's nd out! 
E[ln PerpM] = E[ 
1
T 
T X
t=1 
ln P(wt j w1; :::;wt1)] = 
= 
1
T 
T X
t=1 
E[lnP(wt j w1; :::;wt1)] = 
= 
1
T 
T X
t=1 
N X
i=1 
 P(wit
j w1; :::;wt1)  lnP(wit
j w1; :::;wt1) = 
= 
1
T 
T X
t=1 
H[t j w1; :::;wt1] 
This is the average entropy at each string position of any string of length 
T, corresponding to the geometric mean of the string perplexity of any 
string of length T. In view of the Law of Large Numbers, we realize that 
the empirical preplexity as dened above, will approach this quantity as t 
approches 1, motivating the denition. 
Cross Entropy, Relative Entropy and Metrics 
Let us return to the Equation 2.1 of Section 2.2.1: If we have two distributions 
(collections of probabilities) p(x) and q(x) on 
, then 
Xx 
p(x) lnp(x)  Xx 
p(x) lnq(x) (2.2) 
Equality is equivalent to 8x p(x) = q(x), which means that the distributions 
are identical. The latter quantity, 
Hp[q] = Xx 
p(x) lnq(x) 
is often referred to as the cross entropy of q w.r.t. p. The above inequality 
means that the cross entropy of a distribution q w.r.t. another distribution 
p is minimal when q is identical to p. 
We can utilize this to create a distance measure for distributions by 
measuring the distance between the probability collections p(x) and q(x). 
First we note using Equation 2.1 that: 
0  Xx 
p(x) lnp(x)  p(x) lnq(x) = Xx 
p(x) ln 
p(x) 
q(x) 
with equality only when 8x p(x) = q(x). We now dene 
D[p jj q] = Xx 
p(x) ln 
p(x) 
q(x) 
= Hp[q]  H[p] 
This is known as the relative entropy or Kullback-Leibler (KL) distance. 
A metric is a real-valued function that measures distances. Any metric 
m(X; Y) must have the following three properties:

2.2. ELEMENTARY INFORMATION THEORY 55 
1. m(X; Y )  0 and m(X; Y ) = 0 , X = Y . 
2. m(Y;X) = m(X; Y ). 
3. m(X; Y )  m(X;Z) + m(Z; Y ). 
The rst condition states that the metric is a positive function unless the 
two elements are identical, in which case it takes the value zero. The second 
requirement is that the metric is symmetric, i.e., that the distance from Y 
to X is the same as the distance from X to Y . The third one is called 
the triangle inequality, which means that the distance from X to Y is not 
greater than the distance from X to some third point Z plus the distance 
from Z to Y . 
Although we have established that the relative entropy has the rst of 
these properties, it lacks the second two. If for example p(1) = 1/4, p(2) 
= 3/4; r(1) = r(2) = 1/2; and q(1) = 3/4, q(2) = 1/4, then 
D[p jj r] 6= D[r jj p] and 
D[p jj q] > D[p jj r] +D[r jj q] 
A true metric for probability distributions is the Euclidian distance, 
which is the natural measure of distance used in the real world: 
d(p; q) = sXx 
(p(x)  q(x))2 
Ever since Pythagoras, we sum the squares of the Cartesian coordinates 
and take the square root to get the distance between two points in space. 
This doesn't have anything to do with entropy, though. 
The relative entropy, and the cross entropy, can also be used for evaluation. 
Assume that p is the true probability distribution, and that we have 
various estimates qk of p. The best estimate is of course p itself which will, 
accoring to Equation 2.2, minimize the cross entropy Hp[q] and thereby also 
the relative entropy D[p jj q] = Hp
[q]  H[p]. We can thus claim that the 
smaller the cross entropy w.r.t. p, the better the estimate of p. 
A problem with this approach is that we may not know the real distribution 
p. However, if we take a random sample X from the distribution 
p, and calculate 
1 
jXj Xx2X 
ln qk(x), we have an estimate of Hp[q]. So we 
can argue that if qk yields a lower value of this than qk0 , then the former 
is a better estimate of p than the latter. This technique is known as 
cross-entropy evaluation. 
2.2.3 Noiseless Coding 
Let  continue to be a discrete stochastic variable  with a nite range 
 = 
fx1; : : :; xMg and corresponding probabilities pi = p(xi); i = 1; : : :; pM. Let 
us make a sequence of independent observations of  and call the generated 
string over fx1; : : :; xMg a message. 3 Further, let A = fak : k = 1; : : :;Dg be a code alphabet. We will associate with each element xi a string over 
A which we will call the code word for xi. We will call the set of code 
words a code. Let ni denote the length of the code word for xi, and let 
n = PM
i=1 pini be the expectation value of the code-word length. 
3This is an example of a discrete memoryless stochastic process, see Section 2.1.1.

56 CHAPTER 2. APPLIED STATISTICS 
The Noiseless Coding Problem: Given  and A dened as 
above, construct a code that minimizes n. 
A code where any string in A corresponds to at most one message 
is said to be uniquely decipherable.4 This means that it is impossible to 
come up with a sequence of code words that can be interpreted as another 
sequence of code words. This is obviously a highly diserable property. 
Still, we can do even better than this by requiring that it should be 
possible to determine the code word without looking further into the coded 
message. Such a code is said to be instantaneous. One way of guaranteeing 
this is to require that no code word is a prex of another code word. A little 
afterthought yields that this is actually a necessity for an instantaneous 
code. Note that an instantaneous code is also uniquely decipherable. 
Let us temporarily assume that in addition to  and A, the code word 
lengths ni : i = 1; : : :;M are also prescribed. How can we determine 
whether or not it is possible to construct a uniquely decipherable, or instantaneous 
code? The answer is simply that the existence of both an 
uniquely decipherable code and an instantaneous code is equivalent to the 
requirement PM
i=1Dni  1. 
Proof: <To be written> 2 
Amongst other things, this means that if we can construct an uniquely 
decipherable code, we can also construct an instantaneous code. 
Returning to the noiseless coding problem, where we again are in command 
of the code-word lengths, we can establish the following lower bound 
on the mean code-word length n: 
n = Xpini  
H[] 
lnD 
Equality in the above  is equivalent to 8i2f1;:::;Mgpi = Dni . This is 
known as the Noiseless Coding Theorem. Here, the size D of the code 
alphabet is often referred to as the base of the code. 
Proof: <To be written> 2 
In general, it is impossible to nd integers ni : pi = Dni. To construct 
an optimal code, we will do the second best thing and choose ni such that 
ni  1 < ln pi 
lnD  ni 
We can construct an instantaneous code with these code-word lengths. 
Firstly, note that the second inequality implies that pi  Dni . Thus, 
there exists an instantaneous base D code with these code-word lengths, 
since 
M X
i=1 
Dni  
M X
i=1 
pi = 1 
So what is the mean code-word length of this code? From the rst inequality 
we have 
n = 
M X
i=1 
pini  
M X
i=1 
pi(ln pi 
lnD 
+ 1) = PM
i=1 pi ln pi 
lnD 
+ 
M X
i=1 
pi = 
H[] 
lnD 
+ 1 
4A denotes all possible strings over the Alphabet A including the empty string.

2.2. ELEMENTARY INFORMATION THEORY 57 
and similarly from the second inequality we have 
n = 
M X
i=1 
pini  
M X
i=1 
piln pi 
lnD 
= PM
i=1 pi ln pi 
lnD 
= 
H[] 
lnD 
Thus we have proved that 
Given a discrete random variable  with nite 
 and with 
entropy H[], there exists an base D instantaneous code whose 
mean code-word length n satises 
H[] 
lnD  n  
H[] 
lnD 
+ 1 
Now assume that we instead of inventing code words for individual 
symbols xi, invent code words for blocks of length s, i.e., strings over 

 = fx1; : : :; xMg of length s. This is known as block coding. This will 
correspond to the outcome of s independent observations i : i = 1; : : :; s 
of . We can then cut down the interval to 
H[] 
lnD  ns  
H[] 
lnD 
+ 
1
s 
Here ns is the mean code word length per symbol in 
. 
To actually construct an optimal code, we will turn to the special case 
of D = 2. The procedure is much the same for D > 2. We can without 
further loss of generality assume that the coding alphabet A is f0; 1g. We 
will describe the Humann Procedure, which works by recursion over the 
cardinality M of 
 . 
If M = 2, then we assign x1 the code word 0, and x2 the code word 1. 
If M >2, we proceed as follows: Firstly, we may without loss of generality 
assume that i < j ) pi  pj , and that within each group of symbols 
with equal probability, the symbols are ordered after code word length. 
Thus xM1 and xM have the smallest probabilities. For any such optimal 
instantaneous code of size M we have the following: 
1. i < j ) ni  nj . 
2. nM1 = nM. 
3. At least two code words of length nM have a common prex of length 
nM  1. 
Proofs: 
1. If this were not the case we could simply swap code words 
for xi and xj , reducing n and contradicting the optimality. 
2 
2. If this were not the case we could simply strip the last digit 
from WM, reducing n and contradicting the optimality. 2 
3. If no two code words of length nM had a common prex 
of length nM  1, we could strip the last from digit all 
code words of length nM, reducing n and contradicting 
the optimality. 2

58 CHAPTER 2. APPLIED STATISTICS 
The reason that we can strip the last digit from a code word 
without the result being another code word is the fact that the 
code is assumed to be an instantaneous code, were no code word 
is the prex of another. 
We construct a problem that is one step smaller by dropping xM, letting 
p0
i = pi for i = 1; : : :;M  2, and setting p0
M1 = pM1 + pM. We solve 
the problem of size M  1 by recursion (most likely after having reordered 
the symbols, since p0
M1 is most likely no longer the smallest probability), 
yielding an optimal instantaneous code C0 = fW0 
i : i = 1; : : :;M  1g. We 
now construct from this a code C = fWi : i = 1; : : :;Mg for the problem of 
size M. This is done by setting Wi = W0 
i for i = 1; : : :;M  2 and setting 
WM1 = W0M
10 and WM = W0M
11. 
The Human procedure results in an optimal instantaneous code C. 
Proof: We prove this by induction over the size M. For the 
base case M = 2, we have the code x1 ! 0 and x2 ! 1, which 
is clearly both optimal and instantaneous. 
If C0 is instantaneous, then C will also be instantaneous, since 
due to the way the code C0 is extended, no code word will be 
a prex of another. We now need to prove that given that the 
procedure yields an optimal code for size M  1, it will also 
yield an optimal code for size M. Assume the opposite5, i.e., 
that there is a better code C00 = fW00 
i : i = 1; : : :;Mg, and thus 
that PM
i=1 pin00 
i < PM
i=1 pini. We will use this code to create 
a new code C000 = fW000 
i : i = 1; : : :;M  1g for M  1 that 
is better than C0, contradicting the induction assumption and 
thus establishing the induction step. 
Again we remove W00 
M and set W000 
i = W00 
i for i = 1; : : :;M  2, 
strip the last digit from W00 
M1, and assign p000 
M1 = pM1+pM. 
Before we deliver the nal blow, we will summerize what we 
have got this far: 
n0
i = ni for i = 1; : : :;M  2 
n000 
i = n00 
i for i = 1; : : :;M  2 
nM1 = nM 
n0
M1 = nM1  1 
n00 
M1 = n00 
M 
n000 
M1 = n00 
M1  1 
p000 
i = p00 
i = p0
i = pi for i = 1; : : :;M  2 
p00 
M1 = pM1 
p000 
M1 = p0
M1 = pM1 + pM 
p00 
M = pM 
M X
i=1 
pin00 
i < 
M X
i=1 
pini 
Now we go for the kill: 
M1 
Xi=1 
p000 
i n000 
i  
M1 
Xi=1 
p0
in0
i = 
5This is a regular trick in Mathematics: To prove P under the assumptions , assume 
:P and prove that :P ^ )?. Of course,  is required to be consistent.

2.3. MULTIVARIATE ANALYSIS 59 
= 
M2 
Xi=1 
p000 
i n000 
i + p000 
M1n000 
M1  
M2 
Xi=1 
p0
in0
i  p0
M1n0
M1 = 
= 
M2 
Xi=1 
pin00 
i + (pM1 + pM)(n00 
M1  1)  
 
M2 
Xi=1 
pini  (pM1 + pM1)(nM1  1) = 
= 
M2 
Xi=1 
pin00 
i + pM1n00 
M1 + pMn00 
M1  pM1  pM  
 
M2 
Xi=1 
pini  pM1nM1 + pMnM1 + pM1 + pM = 
= 
M X
i=1 
pin00 
i  
M X
i=1 
pini < 0 
Thus C000 is a better code than C0, and we have established a 
contradiction. The induction step must therefore be valid, and 
thus the entire claim. 2 
2.2.4 More on Information Theory 
The plan is to extend the section on Information Theory considerably to 
include among other things the Noisy-Channel Model. In the mean while, 
we refer the reader to [Ash 1965]. 
2.3 Multivariate Analysis 
The plan is to include a rather long section on Multivariate Analysis. In 
the mean while, we refer the reader to [Tatsuoka 1988]. 
2.4 Sparse Data 
The plan is to include a section on Sparse Data covering at least the following: 
 Overtraining. 
 Back-o Smoothing: Use the distributions in more general contexts 
to improve on the MLE of the probs (ie, rel freqs). 
 Good-Turing Reestimation: General consideration on how population 
distributions behave. r = (r + 1)  Nr+1 
Nr 
and ~ Pr = r 
N . Controversial 
intuition: Population size for each rank invariant, Nr  r = C or 
Nr = C
r . Cf. Zipf's law, [Zipf 1935]. 
 Deleted Interpolation: Maximize the probability of held out data 
w.r.t. the backo weights. 
 Successive Abstraction. 
In the mean while, we refer the reader to [Jelinek & Mercer 1980] for Deleted 
Interpolation, to [Good 1953] and [Katz 1987] for Good-Turing reestimation, 
and to [Samuelsson 1996] for Successive Abstraction.

60 CHAPTER 2. APPLIED STATISTICS

Chapter 3 
Basic Corpus Linguistics 
3.1 Empirical Evaluation 
Chinchor et al. write in Computational Linguistics 19(3): "One of the common 
problems with evaluations is that the statistical signicance of the 
results is unknown", [Chinchor et al 1993], p. 409. 
Empirical evaluation of natural language processing systems in general 
is very young. The attempt to put evaluation of the eciency of speech 
recognition, and natural language processing systems on solid grounds started 
at the end 80ies in the US within the (D)ARPA1 speech and natural 
language workshops as well as the (D)ARPA message understanding conferences. 
First evaluation settings were so called blackbox evaluations, 
i.e. the systems under evaluation are tested as a whole, no information 
about the performance of single system components can be made. In order 
to cope with this drawback a dierent type of evaluation setting, called 
glassbox evaluation, has been created. The task is to provide guidelines 
and measures for detailed evaluation of single system components. Crucial 
preconditions for eciency evaluation are: (1) thoroughly designed test material, 
corpora annotated according to the tasks tested, and (2) suiteable 
evaluation models and measures. 
The theoretical background for measuring system eciency stems from 
information retrieval research. In the following major characteristics of a 
basic eciency evaluation model, and its adaptation to natural language 
processing tasks will be described. 
3.1.1 Contingency Tables 
Contingency tables are matrices to represent classications of observations. 
The use of contingency tables in natural language system evaluation was 
inuenced by work in information retrieval research. In a simple setting 
contingency tables summarize results of binary decisions (yes/no decisions) 
on categories which are dened to classify a text. Considering a single category 
n binary decisions have to be made to characterize n units of text, 
see gure 3.1. The elements of a in gure 3.1 are called true positives 
or hits more colloquially speaking, i.e. elements that are correctly detected 
as positives by the system. The elements of b are named false positi- 
ves or false alarms, i.e. elements that are wrongly considered as positives. 
1(Defence) Advanced Research Projects Agency 
61

62 CHAPTER 3. BASIC CORPUS LINGUISTICS 
Element classes c and d are called false positives (misses) or true ne- 
gatives (correct rejections) respectively. False negatives are positives that 
are rejected by the system, true negatives on the contrary are correctly detected 
negatives. In terms of probabilities the elds of a contingency table 
are lled with conditional probabilities, see gure 3.2, with A representing 
the number of positive decisions made by the system, N representing the 
number of rejections made by the system, a and n representing the total 
number of positives and negatives in the test set. Note that the probabilities 
of true positives and false negatives, and false positives and true 
negatives respectively sum up to one. 
Yes is Correct No is Correct 
Decides Yes a b a+b 
Decides No c d c+d 
a+c b+d a+b+c+d=n 
Figure 3.1: contingency table for binary decisions, cf. Chinchor et al. 93 
Yes is Correct No is Correct 
Decides Yes P(Aja) P(Ajn) 
Decides No P(Nja) P(Njn) 
1.0 1.0 
Figure 3.2: contingency table with probabilities 
3.1.2 Important Measures on Contingency Tables 
The most important measures dened on contingency tables are the completeness 
measures recall, also known as true positive rate, fallout, the false 
positive rate, and precision, a measure for accuracy. Recall and fallout 
were rst used in signal detection theory [Swets 64] to measure a system's 
ability to detect signals within noise. In information retrieval fallout is often 
replaced by precision. Another measure interdependent with precision 
is error rate. 
In terms of distinguishing signals from noise, recall characterizes the 
number of correctly detected signals wrt. all signals presented. Fallout 
determines the number of correctly detected non-signals relative to the total 
number of non-signals, and precision determines the number of correctly 
detected signals relativ to the total number of items considered as signals 
by the system. 
According to gure 3.1 recall, fallout and precision can be dened as 
following: 
recall = 
a 
a + c 
fallout = 
b 
b + d 
precision = 
a 
a + b 
= 
true positives 
true positives + false positives 
= accuracy 
error rate = 100% precision

3.1. EMPIRICAL EVALUATION 63 
3.1.3 Extended Contingency Table Model 
In evaluation of message understanding systems the contingency table model 
is adapted according to the needs of natural language systems' evaluation 
for more ne-grained and distinctive scoring. The binary decision between 
correct and incorrect system output as characteristic for contingency 
tables is replaced by a dichotomy of answers between correct and incorrect. 
This is achieved by modelling the system as making binary decisions between 
generation of correct and incorrect llers for database templates. For 
each message in the test set appropriate lls for the database templates are 
predened. These template lls are called answer keys.2 Thus the number 
of YES decisions made by the system is regarded as the number of llers 
generated by the system, while the number of positive decisions to be made 
at all is the number of llers in the answer key. Figure 3.3 gives an overview 
of scoring criteria developed within MUC-3 ([MUC-3 1991]). Partial 
matches (PAR) between system answers and answer keys are scored as half 
correct. 
Category Criterion 
Correct COR system response = key 
Partial PAR response = key 
Incorrect INC response 6= key 
Spurious SPU blank key vs non-blank response 
Missing blank response vs non-blank key 
Noncommittal NON blank response and blank key 
Figure 3.3: scoring criteria, cf. Chinchor et al. 93 
3.1.4 Measures, Extended 
According to the above established criteria the already familiar evaluation 
metrics of recall, precision and fallout can be computed. Recall refers to 
the completeness of lls attempted by the system. Precison refers to the 
accuracy of attemted lls, and fallout refers to the degree of incorrect lls 
produced by the system relative to the number of possible incorrect lls 
(POSINC). A new measure, called overgeneration, is introduced. Overgeneration 
refers to the degree of spurious generation, i.e. the number 
of false decisions (not correct items are judged as correct) wrt. the total 
number of decisions made by the system. 
In order to provide non-trivial information on system eectiveness at 
least two measures have to be considered. Perfect system performance for 
instance could be characterized in terms of 100% precision and 100% recall 
or 100% recall and 0% overgeneration. 
recall = 
COR+ 0:5  PAR 
POS 
precision = COR + 
0:5  PAR 
ACT 
fallout = 
INC + SPU 
POSINC 
2For a detailed description of generation of answer keys see [Chinchor et al 1993].

64 CHAPTER 3. BASIC CORPUS LINGUISTICS 
overgeneration = 
SPU 
ACT 
#POS = #COR+#PAR+#INC +#MIS 
#ACT = #COR+#PAR+#INC +#SPU 
Another type of measure is the so called \+1-2{scoring", which originates 
from evaluation of speech recognition systems. The measure allows 
for the option not to answer without penalty. Correct answers are scored 
+1, incorrect ones are scored -2, and no system responses are scored -1. 
3.1.5 Loose Attempts on Measuring System Eciency 
Apart from systematic attemts to evaluate the performance of systems, a 
number of other, more or less loose, criteria are mentioned in the literature: 
time and space complexity (O-notation), measures for the correctness of the 
output of part-of-speech taggers, robustness, and ease of adaptability to different 
domains or languages. The eectiveness of part-of-speech taggers is 
measured in terms of accuracy, i.e. the percentage of correctly assigned 
parts-of-speech in running text. The notion of robustness addresses a system's 
ability to cope with corrupted or ungrammatical input. This feature 
is extremely relevant for real world applications, where ungrammatical3 and 
corrupted input is likely to appear, e.g. speech signals corrupted by noise, 
repairs in speech, headlines and various kinds of insertions in newspaper 
text, etc. Even though robustness is a major factor for a system's quality, 
there are no objective criteria for measuring robustness. The same is true 
for adaptability. Adaptability addresses the question how easily a system 
can be adapted to new domains and languages. 
In the following we examine criteria for empirical evaluation of part-ofspeech 
taggers. 
3.1.6 Empirical Evaluation of Part-of-Speech Taggers: 
A Case Study 
The most prominent feature in the evaluation of part-of-speech taggers is 
accuracy. There might be at least two accuracy measures, one measuring 
the percentage of correct output on word level, the other measuring the 
percentage of correct output on sentence level, cf. [Merialdo 1994]. It is 
obvious that accuracy on sentence level will always be below accuracy on 
word level. The performance of state of the art taggers is exclusively measured 
by word level accuracy. Word level accuracy of state of the art taggers 
ranges from 95% [Church 1988], 96% [DeRose 1988], [Cutting et al 1992] 
to the extreme of 99% [Church 1988]. For a discussion of comparability 
problems of accuracy data cf. also [Abney 1997]. Accuracy is coupled with 
dierent conditions the systems operate on. Thus comparability of the quality 
of the output of systems depends on comparability of conditions such 
as:
3Ungrammaticality as opposed to a strong notion of grammaticality such as Chom- 
sky's notion of competence grammar.

3.2. CORPORA 65 
 Size of the tagset used: It is more likely to achieve modest tagging 
results with a small tagset. 
 Size of training corpus and size of tagset: The size of the training set 
wrt. the size of the tagset used aects accuracy. Taggers working on 
small tagsets achieve relatively good results when trained on small 
corpora (some 10 to 100 000 words) while increasing the size of the 
tagset reduces accuracy in case the size of the training set is not 
increased accordingly. 
 Type of training and test corpus: The type of corpora used for training 
and testing aects the quality of a tagger's output in several 
ways. Accuracy diers whether training and testing material are 
derived from a similar type of corpus or not, and whether the systems 
compared train and test on comparable corpora. As an example 
for dierences in training and test corpus see experiments with 
VOLSUNGA [DeRose 1988]. The experiments revieled accuracy differences 
due to the type of test corpus. 
 Complete or incomplete vocabulary: Accuracy is higher in case all 
words in the test corpus are known (complete vocabulary) than in 
case the test text comprises unknown words (incomplete vocabulary). 
 Type of vocabulary: Dierent systems make use of dierent types 
of vocabulary, such as vocabulary derived from test and/or training 
corpus including frequency information relativ to the corpus in question, 
external lexica with or without a morphology component, extra 
lists of collocations, etc. An example for the eect of the change in 
accuracy due to change in vocabulary is given in [DeRose 1988]. The 
addition of an idiom list to the vocabulary improved the accuracy of 
the tagger described from 94% to at least 96%. 
Accuracy also diers wrt. the trade-o between training method, size 
of training corpus, and number of iterations in training. For a detailed 
analysis see [Merialdo 1994]. 
Summing up, questions typically kept quiet about are: 
How successful is a tagging method reaching 96% accuracy when 90% 
?
On the basis of which data is accuracy computed? Is it either a 
measure of precision or simply the percentage of positives found? 
Are the gures reported for the accuracy of dierent taggers comparable 
at all? 
3.2 Corpora 
The number of corpora (text and speech), and lexical databases available is 
constantly increasing; see for example gure 3.2. Various initiatives for data 
collection and specication of annotation guidelines exist, cf. gure 3.1. 
Especially, LDC has become a major site for the distribution of corpora, 
lexical databases, and corpus processing tools.

66 CHAPTER 3. BASIC CORPUS LINGUISTICS 
Initiative Acronym Location 
Data Collection Initiative ACL/DCI University of 
of the Association of Pennsylvania, USA 
Computational Linguisitcs 
European Corpus Initiative ECI University of Edinburgh, 
Scotland 
(Defense) Advanced Research (D)ARPA/LDC 
and Projects Agency 
Linguistic Data Consortium LDC 
Text Incoding Initiative TEI University of 
Pennsylvania, USA 
International Computer Archive ICAME Bergen, Norway 
of Modern English 
Centre for Lexical Information CELEX Nijmegen, NL 
Table 3.1: data collection and coding initiatives 
3.2.1 Types of Corpora 
Corpora are used for training and testing of statistic models of natural 
language and speech, and for evaluation of components of natural 
language systems; cf. sections 3.1 and 3.2.2. We distinguish raw and 
annotated corpora. Annotations are formal representations of partial linguistic 
descriptions. Due to generalization properties (nd local maxima 
from parameters set) of statistical models, learning from annotated data 
(supervised learning) leads to better results than learning from raw data 
(unsupervised learning). Corpora dier according to the selection of material 
collected. We distinguish balanced, pyramidal and opportunistic 
corpora. 
The most prominent types of corpora are part-of-speech tagged corpora, 
and corpora annotated with syntactic structure. They are used 
for various kinds of natural language statistics such as part-of-speech tagging, 
stochastic grammar induction, stochastic parsing, disambiguation of 
structural or semantic ambiguities, machine translation, lexical knowledge 
acquisition, etc. In the case of machine translation, parallel corpora 
are required, i.e. corpora that consist of texts and their translations. For 
training and testing of speech recognition systems, time aligned, phon- 
logically transcribed speech corpora are required. For the time being, 
corpora anotated with discourse structure and semantically annota- 
ted corpora are rare. (Cf. table 3.2.) 
Corpora According to Text Type 
Balanced corpora per denition consist of dierent text genres of size 
proportional to the distribution (relevance) of a certain text type within 
the language in question. Which of course is tricky to operationalize. 
See the Brown Corpus as an attempt to construct a balanced 
corpus. 
Pyramidal corpora range from very large samples of a few representative 
genres to small samples of a wide variety of genres.

3.2. CORPORA 67 
Opportunistic corpora: Most corpora available can be characterized as 
opportunistic which means \take what you can get!" 
Corpora According to Annotation Type 
Raw: Text is tokenized and cleaned, e.g. control characters are eliminated. 
Text type, headlines, and paragraphs are possibly marked. Cf. the 
ECI MCI. 
PoS-tagged: Raw text is annotated with syntactic category at word level 
(part-of-speech PoS). 
Treebanks: PoS-tagged text is annotaded with sceletal syntactic structure. 
Typically, a parse grammar is dened. Corpora are automatically 
parsed. Parse trees are selected and if necessary corrected by 
human annotators. Word strings for which no parse tree is found by 
the grammar are either omitted or manually annotated. Cf. the Penn 
Treebank I and II, [Marcus et al. 1994]. 
Linguistically interpreted corpora: In contrast to treebanks, where only 
syntactic category and phrase structure is annotated, linguistically 
interpreted corpora aim at deliberate annotation of various kinds of 
linguistic information; cf. [Black et al. 1996], [Skut et al. 1997]. 
Corpora According to Use 
Training: Statistical models for NL and speech processing are trained on, 
we may also say learn from, large, typically annotated corpora. 
Testing: Test corpora are used for evaluation of statistical models after 
training. 
Evaluation: For system component evaluation, particularly annotated corpora 
(cf. [MUC-3 1991]) and test suites (cf. [Lehmann et al. 1996]) 
are used. 
3.2.2 Test Suites versus Corpora 
Test suites are deliberately constructed linguistic data, specied according 
to particular features to be tested. Thus, the annotation schemes are designed 
such that the characteristics of particular linguistic phenomena are 
optimally captured. Test suites are: 
 Articial: Selected linguistic phenomena are considered in isolation. 
Example data are constructed such that particular phenomena are 
exhaustively described. The lexicon is kept as small as possible. 
 Restricted with respect to the number of phenomena treated: Only 
phenomena central to the particular evaluation task are described. 
 Descriptive: Linguistic properties of isolated phenomena are described. 
Theory specic assumptions are avoided. 
 Competence driven: The examples are constructed by linguists according 
to the above specied criteria. 
 Collections of positive and negative examples: The linguistic phenomenon 
of interest is described by systematically varied, annotated 
grammatically correct and false examples.

68 CHAPTER 3. BASIC CORPUS LINGUISTICS 
In contrast to test suites, corpora consist of real world data, i.e. language 
data as occurring in running text and speech. Thus, major characteristics 
of corpus data are: 
 Linguistic phenomena occur in context. 
 Combination of performance and competence aspects of language: 
Corpus data reect language usage. Thus, all data, even data that are 
false in terms of competence grammar must be considered as positive 
data. 
 Frequency information on a variety of phenomena instead of full coverage 
of selected phenomena. 
 All data appearing must be captured by the annotation scheme. 
3.2.3 Tokenization 
Tokenization of text, i.e. splitting up a text into tokens (words, numbers, 
abbreviations, acronymes, dates, etc.) is not as simple as it might seem at 
rst glance. To get a avour of what is meant, see the following examples. 
Full Stop versus Period: Sentence boundaries are not obvious. Periods 
are tokens on their own (full stop), or they are part of tokens, e.g. in 
the case of abbreviations, dates, ordinals, enumerations. 
Dashes: might be token internal (needle-like, corpus-based), but they are 
also used for punctuation (..., will contain 16 processors - twice as 
many as ... ). The distinction seems to be easy, but what if blanks 
are missing or wrongly inserted? Words are usually delimited by 
blanks, or a blank and a punctuation mark. Most English compound 
nouns in contrast to German ones are graphically not distinct from 
non-compounds (fruit cake, sandwich maker). There are even worse 
cases: how to deal for instance with New York-based? Dashes also 
appear in clitics such as in French verb subject inversions, e.g. a-t-il 
(has-t-he). Here the question arises: How many tokens are there? 
One { a-t-il, two { the verb a (has) and the pronoun il (he), three 
{ verb, pronoun and clitic t, or even ve { verb, hyphen, clitic t, 
hyphen, pronoun? A standard solution is to delete the clitic t and 
the hyphens during preprocessing, thus information on the clitic is 
lost. A solution to preserve information given by the clitic is either 
to consider the whole string as one cliticised token, or as complex 
expression consisting of three tokes, namely the verb a, the clitic t, 
and the pronoun il. 
Acronyms: appear in various shapes: 
 capital letters only e.g. IBM (International Business Machines), 
BBC (British Broadcasting Company); 
 uppercase and lowercase letters mixed with or without periods, 
e.g. Ges.m.b.H. (Gesellschaft mit beschrankter Haftung, private 
limited liability company); 
Thus, acronymes must be distinguished on the one hand from ordinary 
words in capital letters such as headlines, and on the other hand 
from abbreviations. A clear distinction between abbreviations and 
acronyms is problematic as acronyms are abbreviations that function 
as names or have become names.

3.2. CORPORA 69 
Apostrophes: There is a number of words occurring with a single quote, 
such as engl.: can't, he'll, he's, father's, or 'z in the French string les 
'z oreilles (the 'z ears). Can't is really tricky as it can be rewritten 
as cannot or can not. While the rst looks pretty like a single word 
(one token), the second consists obviously of two tokens, a verb and 
an adverb. He'll can be rewritten as he will, thus a two token analysis 
seems to be justied. As a consequence 'll must either be considered 
as a word, or reinterpreted as will. From a linguistic point of view the 
second solution would be the preferred one. In the case of he's and 
father's, we have a similar surface form wordstring+apostrophe+s4, 
but dierent interpretations of s, with the former a shorthand for 
the verb is, and the later a morphological marker for case genitive. 
Thus, a two token analysis of the former, but a one token anaysis for 
the later is justied. In the case of les 'z oreilles, 'z represents the 
phonological liaison between the phonemes [s] and [o]. Thus, z neither 
belongs to les nor to oreilles and therefore should be treated as token 
on its own. 
3.2.4 Training and Testing 
There are dierent ways to train and test stochastic models for natural 
language and speech processing: 
Training and testing on one corpus type: A corpus is divided into two 
parts comprising similar text genres. One part (the larger one) is used 
for training and thus is called training corpus, the other part is used 
for testing, obviously the test corpus. 
Training and testing on dierent corpus types: If a statistical model 
is trained on one corpus type and tested on another, the result is 
likely to be worse than the result obtained from training and testing 
on the same corpus type. In order to get a clearer picture of a model's 
performance it is trained and tested on various subcorpora. 
Testing on the training set: This is what you better don't! Testing on 
the training set means that the same corpus is used for training and 
testing, which is a major sin in corpus linguistics. Due to the fact 
that the model is already optimized on the test corpus, the outcome of 
testing is much better than it would be for any another test set. Thus, 
no valid statement about the performance of the statistical model is 
possible. 
3.2.5 Tagsets 
Tagsets vary in size, contents, complexity, and level of annotation. 
The size of tagsets varies from approx. 40 to 200 dierent tags, e.g. 36 
PoS-tags and 20 tags for non-terminals in the Penn Treebank, or 197 
PoS- tags in the London-Lund Corpus. 
Tagsets dier with respect to linguistic classication , see for instance 
German participles. They can be classied as verbal and adjectival 
or verbal forms. In the later case, participles used as adjectives and 
adjectives proper fall into one class. Thus, depending on the decisions 
4+ is here used as concatenation operator.

70 CHAPTER 3. BASIC CORPUS LINGUISTICS 
made in the annotation scheme, tagsets are not necessarily compatible. 
Tagsets vary in granularity of the information conveyed. The LOB and 
Brown tagsets for instance distinguish conjunctions and prepositions, 
whereas this information is conated into a single tag in the Penn 
Treebank. In contrast to tagsets with dierent classications, tagsets 
with dierent granularity but the same partition of linguistic information 
can be easily transformed into each other. 
Tagsets dier with respect to the complexity of tags, i.e. a tag could 
convey simplex or complex information. Part-of-speech tags for instance 
represent syntactic category only or syntactic category and 
inection . The former is sucient for languages with little inection 
such as English. The later is perferable for highly inecting languages 
like German and French. 
Summing up, current tagsets represent mainly syntactic information, 
i.e. syntactic category (such as noun, verb, adverb, adjective, pronoun, 
article, etc.), and phrase structure (non-terminals, such as NP, VP, S, : : : ). 
Even in the case of highly inecting languages, morphological information 
is largely omitted in the annotion. 
In the majority of tagsets dierent classes of information are mingled 
into a single tag; e.g. syntactic category and inection, word level and 
phrase level information. A typical example for the former are verbs as 
they usually are annotated according to their syntactic category (main, 
auxiliary, modal) and their inectional properties, such as nite, innite, 
past participle, present participle. 
Examples for the later are adjectives and pronouns. Adjectives are often 
categorized as attributive or predicative. Thus, phrase level information 
is annotated at word level. Some German pronouns for instance can be 
annotated according to their syntactic context as attributive (e.g. viele 
Kinder kamen, many children came), substituting (e.g. viele kamen, many 
came), or even as accompanied by a determiner (die vielen Kinder kamen, 
the many children came). Cf. the tagset presented in [Thielen and Schiller 
1995]. 
Apart from syntactic category and inection, tagsets also comprise tags 
for foreign words, unknown words, multi-word lexemes, symbols, etc. Tagsets 
for semantic or pragmatic annotation are still rare. 
For examples of various tagsets see appendix D.

3.2. CORPORA 71 
Corpus Name Appearance Availability 
Brown Corpus text freely available 
PoS-tagged contact: 
Henry Kucera@brown.edu 
retagged and parsed not freely available 
by Penn Treebank project contact: LDC 
ldc@linc.cis.upenn.edu 
http://www.ldc.upenn.edu 
LOB (Lancester-Oslo/Bergen Corpus text available through ICAME 
and London/Lund Corpus) PoS-tagged contact: icame@hd.uib.no 
British National Corpus text and speech freely available 
PoS-tagged within Europe 
contact: natcorp@oucs.ox.ac.uk 
United Nations parallel text available through LDC 
Parallel Text Corpus English, French, Spanish 
Language Bank of Modern Swedish text non-commercial use only 
partially with concordances contact: Goteborg University 
Dept. of Cl 
Wall Street Journal text available on ACL/DCI CD-ROM I 
DSO Corpus sense-tagged English available through LDC 
nouns, verbs 
Collins English Dictionary '79 edition lexicon available on ACL/DCI CD-ROM I 
CELEX-2 lexicon available through LDC 
English, German, Dutch 
graphemic, phonemic transcription 
morpheme and syllable structure 
word stress 
Word Net conceptual word hierarchy 
Penn Treebank II text available through LDC 
PoS-tagged contact: ldc@linc.cis.upenn.edu 
phrase structure 
dependency structure 
ECI MCI raw text available through ELSNET 
European Corpus Initiative's contact: elsnet@let.ruu.nl 
Multilingual Corpus I 
MARSEC speech available through 
Machine-Readable orthographic transcription Uni Lancester 
Spoken English Corpus time-aligned Dept. of Ling. 
Groningen Speech Corpus speech available through SPEX 
orthographic transcription contact: spex@spex.nl 
TIMIT speech available through LDC 
time-aligned 
phonetic and orthographic 
transcription, speech waveforms 
ATIS speech available through LDC 
graphemic transcription 
classication of 
utterances 
speech waveforms 
VERBMOBIL speech available 
dialogues 
orthographic transcription 
dialogue structure 
Table 3.2: corpora and lexical databases

72 CHAPTER 3. BASIC CORPUS LINGUISTICS

Chapter 4 
Stochastic Grammars 
The basic idea behind stochastic grammars is to augment a grammatical 
theory with probabilities, thereby assigning probabilities to strings, parse 
trees and derivations. Or as Rens Bod puts it ([Bod 1995b], p. 14): 
\What does a statistical extension of a linguistic theory, often 
referred to as a \stochastic grammar", look like? In the literature, 
we can observe the following recurrent theme: (1) take 
your favorite linguistic theory (a competence model), (2) attach 
application probabilities to the productive units of this theory." 
There are several reason for wanting to do this. One reason is to be 
able to rank various word strings according to probability to select the most 
probable one from a set of string hypotheses, for example generated by a 
speech-recognition system. In this case, the stochastic grammar is used as a 
language model. Another reason is to be able to select a preferred analysis, 
amongst those assigned to a given word string by the linguistic theory, by 
selecting the most probable one. In this case, the stochastic grammar is used 
for disambiguation. A third reason, although related to the second one, is to 
speed up parsing and/or limit the search space by pruning away suboptimal 
or improbable search barnches, so-called preference-based parsing. [Kimball 
1973] and [Marcus 1980] constitute highly interesting reading on rule-based 
versions of this, and their psycholinguistic motivations. 
We will in the following consider various stochastic extenstions of contextfree 
grammars, and we will start by recapitulating some formal language 
theory. 
4.1 Some Formal Language Theory 
A formal grammar is a system for rewriting strings over some alphabet V . 
4.1.1 Context-free Grammars 
A context-free grammar G is a quadruple hVN; VT; S;Ri where: 
VN is a nite set of nonterminal symbols. 
VT is a nite set of terminal symbols. Let V denote VN [ VT . 
S 2 VN is a distinguished start symbol, or axiom. 
R is a nite set of productions X !  where X 2 VN and  2 V . 
The string X 2 V  can be rewritten in one step as  2 V  i 
X !  is in R. This means that one occurrence of the nonterminal symbol 
73

74 CHAPTER 4. STOCHASTIC GRAMMARS 
X in a string is replaced by the string . This is denoted X ) . 
In general, if a string  2 V  can be rewritten as the string   in a nite 
number of steps, this is denoted  )  . Thus ) is the transitive closure 
of the relation ). The number of rewrite steps can be zero, thus allowing 
 ) , i.e., ) is reexive. If S ) , i.e., if  can be derived from the 
start symbol S, then  is said to be a sentential form. 
L(G) denotes the set of strings over VT generated by G and is dened 
as f! 2 V  
T : S ) !g. 
A context-free grammar is said to be in Chomsky Normal Form (CNF), 
if the productions are either of the form Xi ! XjXk, a binary production 
rewriting a nonterminal as two nonterminals, or of the form Xi ! w where 
w 2 VT , a unary production rewriting a nonterminal as a single terminal 
symbol. Also allowing productions of the form Xi ! Xj , i.e., unary productions 
rewriting a nonterminal as another single nonterminal, results in 
grammars in Extended Chomsky Normal Form (ECNF). 
4.1.2 Derivations 
A derivation of a string of terminal symbols ! is a sequence of string rewrites 
S = 0 ) : : :) M = ! 
over V where the rst sentential form, 0, consists of the axiom S only and 
the last one, M, is !. This is the leftmost derivation i in each step, the 
leftmost nonterminal symbol of the sentential form is rewritten. A leftmost 
derivation step is denoted )l and the transitive closure is denoted )l 
. 
This means that for a leftmost derivation 
m = X 
 2 V  
T 
X 2 VN 
 2 V  
X !  2 R 
m+1 =  
Note that m is uniquely decomposed into an initial (possibly empty) string 
of terminals, followed by a nonterminal, followed by the rest of the string. 
Since this species in each rewrite step which nonterminal of the string 
will be rewritten, we can represent a leftmost derivation as the sequence 
r1; : : :; rM of rules applied in each step. 
We will now proceed to establish the correspondences between derivations 
and parse trees. 
4.1.3 Trees 
A tree  is a connected directed acyclic graph. Let the set of nodes be 
denoted N. There is a function ` that maps the nodes of the tree into some 
set of symbols, the so-called labelling function. The arcs in the tree depict 
the immediate-dominance relation ID. If n immediately dominates n0, then 
n is called the mother of n0 and n0 is called a daughter of n. The immediatedominance 
relation is intransitive and irreexive. Its transitive closure is 
called the dominance relation, denoted D. This relation is transitive and 
antisymmetric, and thus reexive.

4.1. SOME FORMAL LANGUAGE THEORY 75 
There is also a partial order indicated by the horizontal position of 
each node | the precedence relation P. This relation is transitive and 
asymmetric (and thus irreexive). If one node dominates another, neither 
one of them precedes the other. Thus a pair of nodes hn; n0i can be a 
member of P only if neither hn; n0i nor hn0; ni is a member of D. 
Arcs in the tree are not allowed to cross. This means that if some 
node precedes another node, all nodes that the former dominates precede 
the latter, and that the former node precedes all nodes the latter node 
dominates. This can be stated formally as 
8n; n0; n00 P(n; n0) ^ D(n; n00) ) P(n00; n0) 
and 
8n; n0; n00 P(n; n0) ^ D(n0; n00) ) P(n; n00) 
In addition to this, we will require that for any two distinct nodes n and n0, 
they are related either by D or P, i.e., exactly one of the following holds: 
D(n; n0); D(n0; n); P(n; n0) or P(n0; n) 
A tree has a single root, i.e., there is a unique minimal element w.r.t. D. 
If  is a tree, then R( ) denotes the root of  . The maximal elements w.r.t. 
D are called the leaves of the tree. These must be ordered by P, and the 
ordered sequence of the labels of the leaves is called the yield of the tree. 
Y( ) denotes the yield of the tree  . The non-maximal elements w.r.t. D are called internal nodes. 
4.1.4 Parse Trees 
A parse tree (or derivation tree) of any string of terminal symbols ! generated 
by a context-free grammar G as dened above must obey the following: 
 The root of the tree is labelled with the axiom S. 
 All leaves of the tree are labelled with elements in VT . More specicly, 
the yield of the tree must be !. 
 All internal nodes are labelled with elements in VN. 
 If there is a node labelled X in the parse tree that immediately dominates 
nodes n1; : : :; nK labelled X1; : : :;XK (where ni precedes nj 
for i < j, i.e., P(ni; nj)), then there is a production in G of the form 
X ! X1; : : :;XK. 
T(G) denotes the set of parse trees generated by the grammar G and is 
dened as f : Y( ) 2 L(G)g. 
A grammar G is said to be nitely ambiguous i there is only a nite 
number of parse trees for any nite string in L(G), i.e., if 
8! 2 L(G) j!j < 1 ! jf : Y() = !gj < 1 
This is equivalent to requiring that no (nonterminal) symbol can be rewritten 
as itself in one or more rewrite step, i.e., that X )+ X is impossible. 
We will in the following assume that the grammars are nitely ambiguous. 
Even so, the number of parse trees in general grows exponetially in the 
string length.

76 CHAPTER 4. STOCHASTIC GRAMMARS 
A partial parse tree is a parse tree where we have dropped the requirements 
that the root must be labelled with the axiom S and the leaves must 
be labelled with terminals. Partial parse trees can be combined through 
tree substitution. We will here dene leftmost tree substitution. 
Leftmost tree substitution: 
Let  and  0 be partial parse trees. Let   0 denote  extended 
by identifying the leftmost (i.e., minimal w.r.t. P) leaf of  
labelled with a nonterminal symbol with the root of  0. The 
selected leaf node of  is called the substitution site. The label 
of the substitution site much match the label of the root of  0. 
The dominance and precedence relations D and P are extended 
accordingly. Although the tree-substitution operation is not 
associative, let    0   00 denote ((   0)   00). 
hPacked parse forests will be dened in a later release.i 
4.2 Stochastic Context-free Grammars 
The simplest example of how a grammar can be augmented with a probabilistic 
theory is a stochastic context-free grammar (SCFG); we simply 
add a probability distribution P on the set of productions R. A stochastic 
context-free grammar is thus a quintuple hVN; VT; S;R; Pi where: 
VN is a nite set of nonterminal symbols. 
VT is a nite set of terminal symbols. Again V denotes VN [ VT . 
S 2 VN is a distinguished start symbol, or axiom. 
R is a nite set of productions X !  where X 2 VN and  2 V . 
P is a function from R to [0; 1] such that: 
8X 2 VN P2V  P(X ! ) = 1 
Note the conditioning on the LHS symbol of the production ri. The following 
is a simple example of a SCFG: 
S ! NP VP (1.0) 
VP ! V (0.5) 
VP ! VP PP (0.5) 
NP ! John (1.0) 
V ! sleeps (1.0) 
PP ! outside (1.0) 
We will dene string probabilities in terms of tree probabilities, which 
in turn will be dened in terms of derivations probabilities. One could 
alternatively dene string probabilities dirctly in terms of derivation probabilities. 
 The probability of a string is dened as the sum of the probabilities 
of the parse trees that have this string as a yield. 
 The probability of a parse tree is dened as the probability of the 
corresponding leftmost derivation. 
Let the leftmost derivation be represented by the sequence of productions 
used in each rewrite step, and let the random variable m be the 
production used in rewrite step m. We can thus view a leftmost derivation 
as a stochastic process 1; : : :; M, see Section 2.1.1, where the set of states

4.2. STOCHASTIC CONTEXT-FREE GRAMMARS 77 
are the set of productions of the grammar. This allows us to dene the 
probability of a leftmost derivation recursively: 
P(1 = ri1; : : :; M1 = riM1; M = riM) = 
= P(M = riM j 1 = ri1; : : :; M1 = riM1 )  
 P(1 = ri1; : : :; M1 = riM1 ) = 
= 
M Ym=1 
P(m = rim j 1 = ri1; : : :; m1 = rim1 ) 
To fully characterize this stochastic process, we need to specify the probabilities 
P(m = rim j 1 = ri1; : : :; m1 = rim1) for all possible values 
of m = 1; : : :;M and i1; : : :; iM. The independence assumption P(m = 
rim j 1 = ri1; : : :; m1 = rim1) = P(rim) is characteristic for stochastic 
context-free grammars. Here P(ri) is the probability of production ri given 
by the stochastic grammar. This means that the probability of rewriting 
a nonterminal X with a production in R is independent of the previous 
sequence of rewrites. Thus we have 
P(1 = ri1; : : :; M = riM) = 
M Ym=1 
P(rim) 
However mathematically sound, this approach obscures the rather intricate 
relationship between the sequence of derivation steps and the current 
leftmost nonterminal node in the partial parse trees resulting from each 
derivation step. Each production X ! X1; : : :;XK in R corresponds to a 
partial parse tree where the root, labelled X, immediately dominates a sequence 
of nodes labelled X1; : : :;XK, and where there are no other nodes. 
Now, let m be the partial parse tree corresponding to rm in a leftmost 
derivation r1; : : :; rM. Then 1  : : :  M is the parse tree corresponding to 
this leftmost derivation. 
Consider the sequence of partial parse trees tm = 1  : : :  m resulting 
from the m rst rewrite steps for m = 1; : : :;M. Note that the nal parse 
tree is simply the last element tM of this sequence. It is more natural to 
discuss the probability of the resulting parse tree in terms of this sequence 
of partial parse trees, rather than in terms of the sequence of rewrite steps, 
although they are isomorphic. This yields the following formulation of the 
probability of a parse tree  : 
P() = P(tM) = 
M Ym=1 
P(m j tm1) (4.1) 
tm = 1  : : :  m 1  m  M 
t0 = 2664 
N = fn0g 
D = fhn0; n0ig 
P = ; `(n0) = S 
3775
P(m j 1  : : :  m1) = P(rm j r1; : : :; rm1) 
We will use some extractor function g(tm) to extract the relevant information 
from the partial parse tree tm when estimating the probabilities 
P(m+1 j tm). This means that we will only look at the portions of the tree 
that we think are relevant for estimating this probability: 
P(k+1 j tm)  P(k+1 j g(tm)) (4.2)

78 CHAPTER 4. STOCHASTIC GRAMMARS 
This means that the set of possible partial parse trees is partitioned into 
a set of equivalence classes, each of which is associated with a probability 
distribution over the set of elementary trees that constitute the set of 
productions. 
For stochastic context-free grammars, the interesting information is the 
label of the leftmost nonterminal of the yield of the tree, which is required 
to match the LHS of the production used in the next rewrite step. Thus, 
the function g() is L(Y( )), where Y( ) is the yield of  and L() returns 
the leftmost nonterminal symbol of the string . 
The probabilities assigned to analyses are preserved under transformation 
to and from CNF: 
 Productions of the form A ! BCDE (p) with A;B; C; D;E 2 VN 
are replaced with a set of productions of the form A ! BC0 (p), C0 ! CD0 (1:0) and D0 ! DE (1:0). This introduces new nonterminals 
C0;D0;E0 which do not gure elsewhere in the transformed grammar. 
 Productions of the form A ! B (p1) are removed. (Alternatively 
Extended Chomsky Normal Form is employed.) For each production 
of the form X ! A (p2) with ;  2 V , a new production 
X ! B (p1  p2) is introduced. 
 Productions of the form A !  (p1), where  is the empty string, are 
removed. For each production of the form X ! A (p2), a new 
production X !  (p1  p2) is introduced. 
 Productions of the form X ! a (p) with a 2 VT are replaced with 
the production pair X ! A0 (p) and A0 ! a (1:0). 
4.3 A Parser for SCFGs 
We will next adapt the Viterbi algorithm discussed in Section 2.1.6 to parsing 
with stochastic context-free grammars. This presentation is inspired by 
[Wu 1995] and we will use a variant of the Cocke-Younger-Kasami (CYK) 
parsing algorithm, see [Aho & Ullman 1972], pp. 314{320. The parser described 
in this section nds the most probable parse (MPP) of a given input 
string ! in O(N3T3) time and O(NT2) space, where N = jVNj is the size 
of the nonterminal alphabet and T is the string length. 
Assume that VN = fX1; : : :;XNg, where S = X1, and that ! = 
w1; : : :; wT. Assume further that the grammar is in Chomsky Normal Form, 
see Section 4.1.1. We will maintain an accumulator n(Xi) for each nonterminal 
symbol Xi in the grammar and each node n in the tree. 
In any parse tree, a node n uniquely determines a substring, which in 
turn species a pair of string positions (s; t). Let wst denote ws+1; : : :; wt, 
and let wst be the yield of the subtree rooted in n. This function is not 
necessarily a surjunction, i.e., there may be string positions that do not 
correspond to any node in the parse tree. Nor is it necessarily an injunction, 
i.e., several nodes may be mapped to the same pair of string postitions. We 
can however dene a partial inverse function by choosing the node closest 
to the root. Thus  is the partial parse tree whose yield is wst and whose 
root node n is minimal w.r.t. D. More formally, Y() = wst ^ R() = 
n ^ (Y( 0) = wst ^ R( 0) = n0 ! D(n; n0)). The nodes that do 
not correspond to any pair of string positions will thus be the daughters of 
unary productions. For a grammar in CNF, these nodes are the leaves of 
the tree, which will not gure directly in the algorithm below.

4.3. A PARSER FOR SCFGS 79 
We thus have the set of accumulators 
s;t(Xi) 1  i  N; 1  s < t  T 
These are dened as the maximum probability of any partial parse tree  
spanning the substring wst = ws+1; : : :; wt given that the root of the parse 
tree is labelled Xi: 
s;t(Xi) = max 
:Y()=wst 
P( j `(R( )) = Xi) 
The probability of the most probable parse tree with ! = w0T as a yield 
is thus 
0;T (S) = max 
:Y()=! 
P( j `(R( )) = S) 
and the parse tree itself is 
argmax 
 : Y() = ! 
P( j `(R( )) = S) 
Let pi!jk denote P(Xi ! XjXk j Xi) and pi!w denote P(Xi ! w j Xi). 
We can construct this parse tree bottom-up: 
1. Initialization 
8i; t : 1  i  N; 1  t  T 
t1;t(Xi) = pi!wt 
2. Recursion 
8i; r; t : 1  i  N; 1  r < t  T 
r;t(Xi) = max 
1  j  N 
1  k  N 
r < s < t 
pi!jk r;s(Xj) s;t(Xk) 
24 
r;t(Xi) 
r;t(Xi) 
r;t(Xi) 35 
= 
argmax 
1  j  N 
1  k  N 
r < s < t 
pi!jk r;s(Xj) s;t(Xk) 
3. Reconstruction 
n = (s; t) 
Left(n) =  Nil if t  s  2 
(s; s;t(`(n))) otherwise 
Right(n) =  Nil if t  s  2 
(s;t(`(n)); t) otherwise 
`(Left(n)) = s;t(`(n)) 
`(Right(n)) = s;t(`(n)) 
In the initialization step, the probabilities of the lexical productions 
are assigned to the nonterminals at the nodes immediately dominating the 
input string. In the recursion step, we are interested in calculating 
r;t(Xi) = max 
:Y()=wrt 
P( j `(R( )) = Xi)

80 CHAPTER 4. STOCHASTIC GRAMMARS 
from 
r;s(Xj) = max 
0 :Y(0)=wrs 
P( 0 j `(R( 0)) = Xj) 
and 
s;t(Xk) = max 
00 :Y(00)=wst 
P( 00 j `(R( 00)) = Xk) 
for all possible intermediate points s in the string wrt and all possible 
productions Xi ! XjXk. We note that 
max 
:Y()=wrt 
P( j `(R( )) = Xi) = 
= max 
j;k;s
[P(Xi ! XjXk j Xi)  max 
0 :Y(0)=wrs 
P( 0 j `(R( 0)) = Xj)  
 max 
00 :Y(00)=wst 
P( 00 j `(R( 00)) = Xk)] 
which gives the recursion formula. 
The complexity of this probabilistic parsing scheme is O(N3T3) in time 
and O(NT2) in space. It can easily be generalized to the case of a general 
stochastic context-free grammar, not necessarily in Chomsky Normal Form. 
hRef?i 
hThe relationship between the HMM trellis, the -variables 
and a packed parse forest will be revealed in a later release.i 
This section described a probabilistic version of the CYK parsing algorithm. 
For a very nice presentation of a probabilistic Earley parsing scheme, 
we are happy to be able to refer to [Stolcke 1995]. 
4.4 Parameter Estimation for SCFGs 
We now turn to the problem of estimating the production probabilities 
pi!jk = P(Xi ! XjXk j Xi) and pi!w = P(Xi ! w j Xi) of a SCFG. If 
we have access to an annotated corpus of parse trees, we can calculate these 
probabilities directly from the observed relative frequencies. It is however 
in this case necessary to deal appropriately with the problem of sparse data, 
as discussed in Section 2.4. 
If we only have a corpus of unannotated text, we have to resort to other 
methods. Here we will dene a set of recurrence equations that can be used 
to iteratively improve our estimates of these probabilities. By doing this 
right, we can get some guarantees that each iteration will yield us a better 
set of estimates. 
One method is to rst nd all vaild parse trees for each sentence in 
the corpus. In the rst iteration step, we assign a uniform distribution to 
the set of parse trees resulting from each sentence. We then perform a frequency 
count of the productions, conditioned on the LHS symbol, each time 
weighting with the probability of the parse tree. This will determine a new 
probability distribution for the productions. Using this, we can estimate 
a new probabilities for the parse trees conditional on the corresponding 
sentence. This in turn allows us to estimate the production probabilities 
from the frequency counts conditioned on the LHS symbol, again weighting 
with the previous estimate of the probability of the parse tree. This in turn 
determines a new estimate of the tree probabilities, and we can iterate this 
procedure until we tire.

4.4. PARAMETER ESTIMATION FOR SCFGS 81 
A problem with this method is that the number of possible parse trees 
of an input string is exponential in the string length. Another method for 
reestimating the production probabilities that avoids this problem is called 
the inside-outside algorithm. The basic idea of the inside-outside algorithm 
is to use the current estimates of the production probabilities to estimate 
from the training corpus the expected frequencies of certain other derived 
quantities that depend on the production probabilities, and then recompute 
new estimates of the production probability using these derived quantities. 
The goal is to nd a set of production probabilities that (locally) maximizes 
the likelihood of generating the training corpus. 
4.4.1 The Inside and Outside Variables 
We will use two sets of accumulators, namely the inside probabilities and 
the outside probabilities: 1 
 The inside probability I!
i (s; t) estimates the probability 
P(Xi ) wst j Xi) 
of a given nonterminal Xi deriving the substring wst = ws+1; : : :; wt. 
 The outside probability O!
i (s; t) estimates the probability 
P(S ) w0sXiwtT j S) 
of deriving the string w0sXiwtT = w1; : : :; wsXiwt+1; : : :; wT from 
the axiom S. We will in the following omit the conditioning on S to 
aid readability. 
Nice gure needed here! 
We have the following relationships between the inside, outside and 
production probabilities: 
1. Inside-variable initialization 
8i; t : 1  i  N; 1  t  T 
I!
i (t  1; t) = pi!wt 
2. Inside-variable recursion 
8i; r; t : 1  i  N; 1  r < t  T 
I!
i (r; t) = 
N Xj;k=1 X r<s<t 
pi!jk  I!
j (r; s)  I!
k (s; t) 
3. Outside-variable recursion 
8i; r; t : 1  i  N; 1  r < t  T 
O!
i (r; t) = 
N Xj;k=1 
r1 X
s=0 
pj!ki  O!
j (s; t)  I!
k (s; r) + 
+ 
T X
s=t+1 
pj!ik O!
j (r; s)  I!
k (t; s) 
1In an extension to this scheme, Larson introduces the so-called far-side probabilities.

82 CHAPTER 4. STOCHASTIC GRAMMARS 
Comparing this with the parsing algorithm of the previous section, we see 
that the only dierence between the r;t(i) and I!
i (r; t) accumulators, apart 
from the fact that the argument and subscripts have been swapped, is 
that the latter are summed in the recursion step, whereas the former are 
maximized, in both cases over the same set of quantities. 
The recursion formula for the inside variables follows from 
P(Xi ) wrt j Xi) = 
N Xj;k=1
P(Xi ) XjXk ) wrt j Xi) = 
N Xj;k=1 X r<s<t
P(Xi ) XjXk j Xi)  P(Xj ) wrs j Xj)  P(Xk ) wst j Xk) 
The recursion formula for the outside variables follows from 
P(S ) w0rXiwtT ) = 
N Xj;k=1 
r1 X
s=0 
P(S ) w0sXjwtT ) w0sXkXiwtT ) w0swsrXiwtT) + 
+ 
T X
s=t+1 
P(S ) w0rXjwsT ) w0rXiXkwsT ) w0rXiwtswsT ) = 
N Xj;k=1 
r1 X
s=0 
P(S ) w0sXjwtT )  P(Xj ) XkXi j Xj)  P(Xk ) wsr j Xk) + 
+ 
T X
s=t+1 
P(S ) w0rXjwsT )  P(Xj ) XiXk j Xj)  P(Xk ) wts j Xk) 
We see that the complexity is O(N3T3) for calculating both all inside 
variables and all outside variables. 
4.4.2 Deriving the Reestimation Equations 
The basic idea is to estimate pi!w from 
^ P(Xi ! w) 
^ P(Xi) 
and pi!jk from 
^ P(Xi ! XjXk) 
^ P(Xi) 
: 
pi!w = ^ P(Xi ! w j Xi) = 
^ P(Xi ! w) 
^ P(Xi) 
pi!jk = ^ P(Xi ! XjXk j Xi) = 
^ P(Xi ! XjXk) 
^ P(Xi) 
We will thus need the quantities ^ P(Xi), ^ P(Xi ! w) and ^ P(Xi ! XjXk). 
Deriving ^ P(Xi) 
The string probability P(!) is the probability of deriving ! from the axiom 
S = X1: 
P! = P(!) = P(S ) !) = I!
1 (0; T) 
The joint probability of deriving ! and the nonterminal Xi guring (at 
least once) in some derivation of ! is 
P! 
i = P(S ) 1Xi2 ) !) =

4.4. PARAMETER ESTIMATION FOR SCFGS 83 
= X 0r<sT 
P(S ) w0rXiwsT ) !) = 
= X 0r<sT 
P(S ) w0rXiwsT )  P(Xi ) wrs j Xi) = 
= X 0r<sT 
O!
i (r; s)  I!
i (r; s) 
So the probability P(Xi) of Xi guring in some derivation (of any string) 
can be estimated by normalizing with the string probability P! and averaging 
this quantity over the corpus W: 
^ P(Xi) = 
1 
jWj X!2W 
P! 
i 
P! 
where jWj is the corpus size, i.e., the number of sentences. The complexity 
of calculating this is X!2W 
O(N  T2 
!). 
Deriving ^ P(Xi ! w) 
Likewise, the joint probability of deriving ! and the (lexical) production 
Xi ! w guring in some derivation of ! is 
P(S ) 1Xi2 ) 1w2 ) !) = 
= X 1tT;wt=w 
P(S ) w0t1XiwtT ) !) = 
= X 1tT;wt=w 
P(S ) w0t1XiwtT )  P(Xi ) wt j Xi) = 
= X 1tT;wt=w 
O!
i (t  1; t)  pi!w 
The probability P(Xi ! w) of applying the production Xi ! w in some 
derivation can similarily be estimated by 
^ P(Xi ! w) = 
1 
jWj X!2W 
X 1tT;wt=w
O!
i (t  1; t)  pi!w 
P! 
The complexity of calculating this is X!2W 
O(N  T!) if the lexical entries 
can be accessed in constant time. 
Deriving ^ P(Xi ! XjXk) 
Finally, the joint probability of deriving ! and the production Xi ! XjXk 
guring in some derivation of ! is 
P(S ) 1Xi2 ) 1XjXk2 ) !) = 
= X 0r<tT 
P(S ) w0rXiwtT ) w0rXjXkwtT ) !) = 
= X 0r<s<tT 
P(S ) w0rXiwtT )  P(Xi ! XjXk j Xi)  
 P(Xj ) wrs j Xj)  P(Xk ) wst j Xk) = 
= X 0r<s<tT 
O!
i (r; t)  pi!jk  I!
j (r; s)  I!
k (s; t)

84 CHAPTER 4. STOCHASTIC GRAMMARS 
Analogously, the probability P(Xi ! XjXk) of applying the production 
Xi ! XjXk in some derivation can be estimated by 
^ P(Xi ! XjXk) = 
1 
jWj X!2W 
X 0r<s<tT 
O!
i (r; t)  pi!jk  I!
j (r; s)  I!
k (s; t) 
P! 
The complexity of calculating this is X!2W 
O(N3  T3 
!). 
The Reestimation Equations 
Assembling all this will yield us the nal set of recurrence equations for the 
probabilities pi!w and pi!jk just as in the case of estimating the model 
parameters of an HMM, cp. Section 2.1.7: 
pi!w = ^ P(Xi ! w j Xi) = 
^ P(Xi ! w) 
^ P(Xi) 
= 
= X!2W 
X 1tT;wt=w
O!
i (r; s)  pi!w 
P! 
X!2W 
P! 
i 
P! 
pi!jk = ^ P(Xi ! XjXk j Xi) = 
^ P(Xi ! XjXk) 
^ P(Xi) 
= 
= X!2W 
X 0r<s<tT 
O!
i (r; t)  pi!jk  I!
j (r; s)  I!
k (s; t) 
P! 
X!2W 
P! 
i 
P! 
Each iteration step requires X!2W 
O(N3T3 
!) calculations, which is clearly 
polynomial in corpus size. 
The method is extended to general context-free grammars, not necessarily 
in Chomsky Normal Form, in [Kupiec 1992] and to partially bracketed 
training text in [Pereira & Schabes 1992] 
As we have seen, the string probability P(S ) !) = I!
1 (0; T) can also 
be calculated in O(N3T3) time and O(NT2) space. Unfortunately, nding 
the most probable sentence (MPS) of a word graph, which is the normal 
output from a speech recognizer, is NP hard in string length, see [Sima'an 
1996]. 
4.5 Adding Probabilistic Context to SCFGs 
Consider the following SCFG

4.5. ADDING PROBABILISTIC CONTEXT TO SCFGS 85 
S ! NP VP (0.70) 
...
VP ! V NP NP (0.10) 
...
NP ! Pron (0.20) 
NP ! Det N
(0.20) 
NP ! NP PP (0.30) 
... 
It will assign 
\(Show) (me) (all igths to Boston from Atlanta leaving before 
ten AM that serve breakfast and stop over in Baltimore ...)" 
the same probability as 
\(Show) (all igths to Boston from Atlanta leaving before ten 
AM that serve breakfast and stop over in Baltimore ...) (me)" 
So instead of 
P(NP ! Pron j NP) 
it would be better to use 
P(NP ! Pron j VP ! V  NP NP) 
Likewise, the stochastic model as it stands is too blunt to be of any use 
when disambiguating for example noun-noun compounds. 
It is, theoretically, very simple to add more contextual information to 
the probability distributions by looking further up the tree at the substitution 
site. Assume for convenience that the grammar is in CNF. To take 
not only the LHS label into account, but the entire mother dotted item: 
 Replace each occurrence of a RHS nonterminal with a new unique 
nonterminal. 
 Multiply out the rules by creating a new rule for each matching old 
LHS. 
This creates a new SCFG G1 that eectively yields the probabilities of the 
old productions of grammar G0 conditional on the mother production and 
the expanded nonterminal of its RHS. This can be repeated ad innum to 
yield stochastic models Gp with increasingly more contextual information. 
Note again the similarity to HMMs, this time how Nth-order HMMs can 
be represented as rst-order HMMs by expanding out sequences of states 
into distinct new states. 
The number of nonterminals jG1j in the new grammar G1 equals the 
number of distinct occurrences of nonterminals in the RHSs of the productions 
of the original grammar G0, which is exactly 2jR0j, two times the 
number of original productions. This is maximally 2N3, where N = jG0j is the number of nonterminals in G0. Thus, the most probable parse can 
be found in O(N9T3) time and O(N3T2) space. The algorithm is thus still 
polynomial both in N and T.

86 CHAPTER 4. STOCHASTIC GRAMMARS 
In the general case, the parsing complexity is O(N6p+3T3) in time and 
O(N2p+1T2) in space for p generalization steps. The two key general reccurence 
equations are: 
jGp+1j = 2jRpj 
jRp+1j  2N2jRpj  (2N2)pjR0j 
The rst equality can be established as follows: Each RHS occurrence in 
the grammar Gp of the previous step yields a new symbol in the resulting 
grammar Gp+1. This is simply the number of rules in the previous step, 
jRpj, times two. 
For the following inequalities, we rst observe that the number of productions 
in Gp+1 that each production in Gp gives rise to equals the number 
of occurrences of the particular LHS symbol in the RHSs of the productions 
of Gp. 
Assume that no symbol occurs more than B times in the RHSs of Gp. 
Then this is true also for Gp+1. We establish this claim as follows: When we 
construct Gp+1 from Gp we replace each occurrence of a grammar symbol 
in the RHSs of the productions with a new symbol. This means that before 
LHS expansion, each RHS occurrence is a unique symbol. The number 
of occurences of any symbol will thus be the number of times the RHS 
containing it is duplicated by LHS expansion. We need exactly one version 
of each production for each occurence of the LHS symbol in a RHS of Gp. 
But this is bounded by B. 
So if there is an upper bound B to the number of occurences of any 
grammar symbol in G0, this upper bound is valid also for Gp : p = 1; 2 : : : 
by induction. So each production in Gp is expanded into maximally B 
productions in Gp+1. No symbol can occur more than 2N2 times in the 
RHSs of G0, so B = 2N2 yields the inequalities. 
Starting with a maximally dense grammar G0 we have jR0j = N3. We 
thus have jGpj = 2pN2p2N3 = 2pN2p+1. This means that the parsing 
complexity is O(jGpj3T3) in time and O(jGpjT2) in space, which establishes 
the claim. Note however that the complexity is exponential in the number 
of generalization steps. 
4.6 Theoretical Probability Losses 
A somewhat surprising fact is that the string probabilities assigned by an 
SCFG do not necessarily sum to one over the set of nite strings. The reason 
for this is that each string probability is dened as the sum of the derivation 
probabilities of the string. Now, the SCFG will assign probabilities also to 
nonterminating derivations. For example, the probability of a terminating 
derivation is zero in the following SCFG 
S ! aS (1) 
since there are none. 
This is in some sense a degenerate case, since the probability of the one 
nonterminating derivation S ) aS ) aaS : : : is nonzero. But even for 
grammars where the probability of each nonterminating derivation is zero, 
the sum of these probabilities need not be, as in the following case: 
Proposition 
Consider the SCFG

4.6. THEORETICAL PROBABILITY LOSSES 87 
S ! a (p) 
S ! SS (q) 
where p; q  0 and p + q = 1. 
Then 
1 X
n=1 
P(an) = min1; 
p
q 
Proof 
The total number of nonterminal nodes in any parse tree for an is 2n  1. 
There are n preterminal nodes, i.e., S rewritten using S ! a and thus 
n  1 Ss that are rewritten using S ! SS. This means that the string 
probability of an is pn  qn1 times the number of dierent parse trees for 
an. The number of dierent parse trees equals the number of dierent 
binary bracketings of a string with n terminals. The recurrence equation is 
f(n) = 
n1 
Xk=1 
f(k)  f(n  k) 
With initial value f(1) = 1, the solution is the sequence of Catalan numbers 
 2n  2 
n  1  
n 
Thus, the string probability of an is 
P(an) = f(n)  pn  qn1 
and the sum of the string probabilities is 
x = 
1 X
n=1
P(an) = 
1 X
n=1 
f(n)  pn  qn1 = p  
1 X
n=1 
f(n)  rn1 = p  g(r) 
where r = p  q = p  (1  p) = (1  q)  q. 
From the grammar we can establish that 
x = p + q  x2 
with solutions 
x = 8<: 
1
p
q 
and thus 
g(r) = 8>
><>
>: 
1
q
1
p 
Let y be dened by 
g(r) = 
1
y 
r = y  (1  y)  0

88 CHAPTER 4. STOCHASTIC GRAMMARS 
Then 
y = 
1
2 r1
4  r 
Since g(r) is continuous on [0; 
1
4
], we have 
g(0) = lim 
!0+ 
1 X
n=1 
f(n)  n1 = f(1) = 1 
and thus for all r : 0  r  14 
y = 
1
2 
+r1
4  r = 
1
2 
+r1
4  y + y2 = 
= 
1
2 
+r(
1
2  y)2 = 
1
2 
+  
1
2  y 
= max(1  y; y) = max(p; q) 
This means that 
g(r) = min1
p
; 
1
q 
and thus 
x = p  g(r) = min1; 
p
q 
In the general case, let C = ci be the vector of the probability of each 
nonterminal Xi not terminating. Let A be the matrix of the quantities 
aij = Xk 
P(Rk j Xi)  nj(Rk) 
where nj(R) is the number of times the symbol Xj occurs in the RHS of 
R. 
Then C  AC and for any ci to be non-zero, i.e., ci > 0, we see 
by diagonalizing with T that in order for C  AC to be true, we need 
TC  TAT1TC, which in turn means that for some eigenvalue k of  = 
TAT1, we must have k > 1. Some nice spectral theorem guarantees that, 
under suitable conditions, such a matrix T exists, and that the eigenvalues 
of A are the same as those of TAT1. Using a continuity argument on ci 
as functions of the model parameters P(Rk j Xj ), we nd that a non-zero 
ci requires an eigenvalue greater than one. 
In the example above, A = [2q], and the requirement is 2q > 1, or 
q > 
1
2
. 
Check out: Wetherell 1980, Booth and Thompson 1973, Fu's \Syntactic 
Pattern Recognition". 
4.7 Stochastic Tree-Substitution Grammars 
We will consider another way in which a context-free grammar can be 
augmented with a probabilistic theory. Reconsider the denition of the 
probability of a leftmost derivation, Eq. (4.1). Assume that we allow the 
set of elementary parse trees used here to consist of trees of depth greater

4.8. STOCHASTIC HISTORY-BASED GRAMMARS 89 
than one. We will thus replace the set of trees corresponding to the set of 
productions R with a more general set of elementary trees. Let us further 
assume that the extractor function g( ) of Eq. 4.2 is the same as for SCFGs, 
i.e., L(Y( )). This means that the probability of an elementary tree being 
substituted at the leftmost nonterminal leaf is conditional only on the label 
of this node. 
Since we are working with a general set of elementary trees, we must take 
into account that we no longer have a one-to-one correspondence between 
leftmost derivations and parse trees |Now there may exist several leftmost 
derivations of a parse tree. In order to get the probability of a parse tree, we 
will sum over the set of possible leftmost derivations of it. The probability 
of a string will still be the sum of the probabilities of the parse trees that 
have this string as a yield. 
A stochastic tree-substitution grammar (STSG) thus consists of a quintuple 
hVN; VT; S;R; Pi where 
VN is a nite set of nonterminal symbols. 
VT is a nite set of terminal symbols. As usual, V = VN [ VT . 
S 2 VN is a distinguished start symbol, or axiom. 
R is a nite set of partial parse trees, so-called elementary trees. 
P is a function from R to [0; 1] such that: 
8X 2 VN P:`(R())=X P() = 1 
For these grammars, nding the most probable parse (MPP) is NP hard, 
[Sima'an 1996]. One can however nd the most probable derivation (MPD) 
in O(N3jRj), time see [Sima'an 1995]. This is done by rst using a CYK 
parser to construct a packed parse forest from the underlying CFG, and 
then performing Viterbi search, with a few clever optimizations, on the resulting 
packed parse forest, now taking the probabilities of the STSG into 
account. Similar techniques allow using Monte-Carlo techniques to estimate 
the MPP by sampling from the set of derivations, see [Bod 1995b]. 
For a nice description of STSGs and a related paradigm referred to as 
data-oriented parsing (DOP), originally due to Remko Scha, see [Bod 1995a] 
and [Bod 1995b]. Tree-substitution grammars are a special case of treeadjoining 
grammars, see [Joshi et al 1975]. In addition to tree substitution, 
the latter also allow the adjoining operation. Stochastic lexicalized treeadjoining 
grammars are described in [Schabes 1992]. 
hTo be continued.i 
4.8 Stochastic History-Based Grammars 
Again, reconsider the denition of the probability of a leftmost derivation, 
Eq. (4.1). Assume that we in Eq. 4.2 instead use a more general extractor 
function g( ) than L(Y( )), the one used for SCFGs and STSG. The typical 
type of function g( ) will be a decision tree, which is used to arrive at a 
probability distribution by asking a sequence of questions about the parse 
tree  , or equivalently, the about sequence of derivation steps in the leftmost 
derivation, see [Black et al 1993]. The decision-tree techniques are described 
in [Bahl et al 1989], which is well worth reading. 
We realize that we can in general view any derivation of a parse tree 
as a sequence of disambiguation decisions, and that we can dene the derivation 
probability based on the decision history. If there is a ono-to-one 
correspondence between the derivation and the restulting parse tree, we can 
use this to dene the parse probability. We can again use a decision tree 
to \ask 20 (binary) questions" about the derivation history, i.e., about the

90 CHAPTER 4. STOCHASTIC GRAMMARS 
previous disambiguation decisions made, and thus determine a probability 
distribution on the set of disambiguation actions currently available. 
This is exactly what is done in the work reported in [Magerman 1995], 
one of the most important, and most readable, articles on probabilistic parsing 
to date. One of the nice characteristics of the scheme is that it allows 
lexical information to be used for structural disambiguation. Due to the 
complex conditionings of the probabilistic language model, it uses a disambiguation 
scheme distinct from Viterbi search: It rst nds a reasonably 
high-probability complete parse, and then switches to breadth-rst search, 
pruning search branches with an accumulated probability below that of the 
complete parse found. 
hTo be continued.i 
4.9 Lexicalization of Stochastic Grammars 
Lexicalization of stochastic grammars has become a major research topic. 
For the time being, approaches modeling a combination of phrase structure 
and lexical dependency structure show the best results, cf. [Eisner 1996] 
and [Collins 1997]. There are other approaches like stochastic lexicalized 
tree adjoining grammar ([Schabes 1992]), and history-based grammar using 
lexical information in the derivation history ([Black et al 1993], [Magerman 
1995]). 
An early arpproach on stochastic dependency grammar is probabilistic 
Link Grammar ([Laerty et al. 1992]), a grammar that models n-gram 
lexical dependencies. An examole for a more recent approach on n-gram 
lexical dependencies is presented in [Collins 1996]. 
4.9.1 Stochastic Dependency Grammar and Related 
Approaches 
In classical dependency grammar [Tesniere 1959], syntactic structure is 
determined on the basis of dependency (head-modier) relations between 
words. Sentences and phrases are thus represented by the set of dependency 
relations that hold between their lexical elements. 
Dependency structures can be derived from context-free grammars by 
mapping trees into dependency structures. This is an advantage as the statistics 
already well known from stochastic context-free grammar can also 
be applied to dependency-based approaches. In order to map trees into 
dependency structures a head-daughter is dened for each phrase and its 
lexical head is perculated up the tree. The other daughters of the phrase 
are considered as modiers. Accordingly a dependency relation for each 
head-modier pair can be dened, for example by a triple representing the 
modier non-terminal, the mother non-terminal and the head non-terminal. 
< NP; S; V P >, for instance, expresses a subject-verb dependency, cf. [Collins 
1996]. For illustration, we present three recent approaches in more 
detail. 
A Model for Bigram Lexical Dependencies 
The probability of a parse tree T given a particular word string S is de- 
ned by the probability of the set of head words of NPs B (called base 
NPs in [Collins 1996]) conditioned on S, and the probability of the set of 
dependencies D given S and B, cf. [Collins 1996]. It is

4.9. LEXICALIZATION OF STOCHASTIC GRAMMARS 91 
P(TjS) = P(BjS)  P(DjB; S) 
P(DjB; S) is dened by the conditional probabilities of all binary modiferhead 
relations given S and B. The set D of links from a modier to a head 
is dened by the product of the Maximum Likelihood estimates of the syntactic 
relations Rj that hold between the modifer word-tag pairs < wj; tj > 
and the head word-tag pairs < whj ; thj > in S. As an extra condition, a 
distance measure dj;hj between a modier and its head is added. The distance 
measure is based on information like the number of words and the 
syntactic category of phrases between the head and its modier. We have 
P(DjS;B) = 
m Y
j=1
F(Rjj < wj; tj >;< whj ; thj >; dj;hj) 
Two Generative Models 
Another possibility to represent the dependency structure of a sentence is 
to generate for each word i in a sentence the sequence of its right and left 
children, cf. [Eisner 1996], [Collins 1997]. 
In [Eisner 1996] for each word-tag pair tword(i) of a sentence the sequence 
of left and right children (left kids(i); right kids(i)) is generated. 
The probability that a child is generated is conditioned on the tag of the 
previously generated left child of i (kidc+1) or the previously generated 
right child of i (kidc1) and the word-tag pair tword(i) itself. The formula 
that generates the sequence of right children is given below. 
P(T;S) = 
n Y
i=1
0@ 
1+#right kids(i) 
Y c=(1+#left kids(i));c6=0
P(tword(kidc(i))jtag(kidc1(i)); tword(i)1A
A linguistically more rened model is presented in [Collins 1997]. In 
addition to the lexical head, information on the argument structure dened 
by the head is given. The probability of a parse tree T for a sentence S is 
dened by the product of the probabilities of its productions. Productions 
are dened by the conditional probability of the right-hand sides RHSi 
given the left-hand sides LHSi. Thus we have 
P(T;S) = 
n Yi=1 
P(RHSijLHSi) 
Right-hand sides are further decomposed into the probability PH that 
a head category H is generated conditioned on a parent category P and a 
head word h, and the probabilities of the left and right contexts of H(h) 
under P. The contexts are dened by the probabilities of the left and right 
constituents (Pl, Pr), and the probability of the left and right complements 
(PLC, PRC). Constituents are calculated from the probability of the head 
category (Li or Ri) and the probability of the head word (li or ri) conditioned 
on the parent category P, the parent's head category H and the 
head word h, as well as a distance measure d between the head and the 
edge of the context constituent, and a set representing the left or the right

92 CHAPTER 4. STOCHASTIC GRAMMARS 
arguments (LC, LR) of H(h). The probability of left and right arguments 
is conditioned by P, H and h. Thus we have the following formula: 
n Y 
i=1 
PH(HjP;h)Pl(Li; lijP; H;h; dr; LC)Pr(Ri; rijP; H;h; dr;RC)PLC(LCjP; H;h)PRC(RCjP; H;h) 
4.10 Probabilistic LR Parsing 
In Section 4.2, we dened the parse probability of any stochastic extension 
of a CFG as the probability of its leftmost derivation.2 Conceptually, the 
stochastic version of CFG was viewed as a top-down process, accentuated 
by the fact that the conditioning of the production probabilities is on their 
LHS. In contrast to this, the parser for SCFG described in Section 4.3 
worked in a bottom-up fashion. We could equally well instead dene the 
parse probability as that of the rightmost derivation, and then specify this 
sequence backwards. This is exactly what an LR parser does|it constructs 
the rightmost derivation in reverse. In fact, this is what the \R" in \LR" 
stands for. The \L" stands for left-to-right scanning of the input string. 
4.10.1 Basic LR Parsing 
An LR parser is a type of shift-reduce parser that was originally devised for 
programming languages, [Knuth 1965], and is well described in e.g. [Aho 
et al 1986]. Various aspects of it relevant to natural-language parsing are 
descussed in [Tomita (ed.) 1991]. The success of LR parsing lies in handling 
a number of production rules simultaneously by the use of prex merging, 
rather than attempting one rule at a time. 
An LR parser is basically a pushdown automaton, i.e., it has a pushdown 
stack in addition to a nite set of internal states and a reader head for 
scanning the input string from left to right one symbol at a time. The 
stack is used in a characteristic way: The items on the stack consist of 
alternating grammar symbols and states. The current state is simply the 
state on top of the stack. The most distinguishing feature of an LR parser 
is however the form of the transition relation | the action and goto tables. 
A non-deterministic LR parser can in each step perform one of four basic 
actions. In state S with lookahead symbol3 Sym it can: 
1. accept(S,Sym): Halt and signal success. 
2. error(S,Sym): Fail and backtrack. 
3. shift(S,Sym,S2): Consume the input symbol Sym, push it onto the 
stack, and transit to state S2 by pushing it onto the stack. 
4. reduce(S,Sym,R): Pop o two items from the stack for each phrase 
in the RHS of grammar rule R, inspect the stack for the old state S1 
now on top of the stack, push the LHS of rule R onto the stack, and 
transit to state S2 determined by goto(S1,LHS,S2) by pushing S2 
onto the stack. 
2If the leftmost derivation was not unique, as in Section 4.7, we dened it as the sum 
of the probabilities of the leftmost derivations. 
3The lookahead symbol is the next symbol in the input string i.e. the symbol under 
the reader head.

4.10. PROBABILISTIC LR PARSING 93 
S ! NP VP (1) 
VP ! V (2) 
VP ! V NP (3) 
VP ! V NP NP (4) 
VP ! VP PP (5) 
NP ! Det N (6) 
NP ! Pron (7) 
NP ! NP PP (8) 
PP ! Prep NP (9) 
Figure 4.1: Sample Grammar 
Prex merging is accomplished by each internal state corresponding to 
a set of partially processed grammar rules, so-called \dotted items" containing 
a dot () to mark the current position. For example, if the grammar 
contains the following three rules, 
VP ! V 
VP ! V NP 
VP ! V NP NP 
there will be a state containing the dotted items 
VP ! V  
VP ! V  NP 
VP ! V  NP NP 
This state corresponds to just having found a verb (V ). Which of the three 
rules to apply in the end will be determined by the rest of the input string; 
at this point no commitment has been made to any of them. 
4.10.2 LR-Parsed Example 
The example grammar of Figure 4.1 will generate the internal states 
of Figure 4.2. These in turn give rise to the parsing tables of Figure 4.3. 
The entry \s2" in the action table, for example, should be interpreted as 
\shift the lookahead symbol onto the stack and transit to State 2". The 
action entry \r7" should be interpreted as \reduce by Rule 7". The goto 
entries simply indicate what state to transit to once a phrase of that type 
has been constructed. Note the two possibilities in States 11, 12 and 13 for 
lookahead symbol preposition, \Prep", both of which must be tried. We 
can either shift it onto the stack or perform a reduction. This is called a 
shift-reduce conict and is the source to the ambiguity in the sentence John 
sees a man with a telescope. 
Using these tables we can parse the sentence John reads a book as follows:

94 CHAPTER 4. STOCHASTIC GRAMMARS 
State 0 
S0 
)  S 
S )  NP VP 
NP )  Det N 
NP )  Pron 
NP )  NP PP 
State 1 
S ) NP  VP 
NP ) NP  PP 
VP )  V 
VP )  V NP 
VP )  V NP NP 
VP )  VP PP 
PP )  Prep NP 
State 2 
NP ) Det  N 
State 3 
NP ) Pron  
State 4 
S0 
) S  
State 5 
S ) NP VP  
VP ) VP  PP 
PP )  Prep NP 
State 6 
VP ) V  
VP ) V  NP 
VP ) V  NP NP 
NP )  Det N 
NP )  Pron 
NP )  NP PP 
State 7 
NP ) NP PP  
State 8 
PP ) Prep  NP 
NP )  Det N 
NP )  Pron 
NP )  NP PP 
State 9 
VP ) VP PP  
State 10 
NP ) Det N  
State 11 
VP ) V NP  
VP ) V NP  NP 
NP ) NP  PP 
NP )  Det N 
NP )  Pron 
NP )  NP PP 
PP )  Prep NP 
State 12 
VP ) V NP NP  
NP ) NP  PP 
PP )  Prep NP 
State 13 
PP ) Prep NP  
NP ) NP  PP 
PP )  Prep NP 
Figure 4.2: The resulting internal states 
State Action Goto 
Det N NP Prep Pron V eos NP PP S VP 
0 s2 s1 s3 1 4 
1 s8 s6 7 5 
2 s10 
3 r7 r7 r7 r7 r7 r7 
4 acc 
5 s8 r1 9 
6 s2 s11 r2 s3 r2 11 
7 r8 r8 r8 r8 r8 r8 
8 s2 s13 s3 13 
9 r5 r5 
10 r6 r6 r6 r6 r6 r6 
11 s2 s12 s8/r3 s3 r3 12 7 
12 s8/r4 r4 7 
13 r9 r9 s8/r9 r9 r9 r9 7 
Figure 4.3: The corresponding LR parsing tables

4.10. PROBABILISTIC LR PARSING 95 
Action Stack String 
init [0] John reads a book 
s1 [1, NP, 0] reads a book 
s6 [6, V, 1, NP, 0] a book 
s2 [2, Det, 6, V, 1, NP, 0] book 
s10 [10, N, 2, Det, 6, V, 1, NP, 0]  
r6 [11, NP, 6, V, 1, NP, 0]  
r3 [5, VP, 1, NP, 0]  
r1 [4, S, 0]  
accept [4, S,0]  
Initially State 0 is pushed onto the empty stack. The noun phrase (NP) 
corresponding to the word \John" is shifted onto the stack and the parser 
transits to State 1 by pushing it onto the stack. Next, the verb (V ) 
corresponding to the word \reads" is shifted onto the stack and the parser 
transits to State 6 by pushing it onto the stack. Then the determiner (Det) 
corresponding to the word \a" is shifted onto the stack and the parser transits 
to State 2. The noun (N) corresponding to the word \book" is shifted 
onto the stack and the parser transits to State 10. At this point, the noun 
and the determiner on top of the stack are reduced to a noun phrase using 
Rule 6 (NP ! Det N) by popping State 10, the noun, State 2 and the 
determiner from the stack. The noun phrase is then pushed onto the stack, 
and the parser transits to State 11 by pushing it onto the stack. Next, the 
noun phrase and the verb on top of the stack are reduced to a verb phrase 
(VP) using Rule 3 (VP ! V NP), which is pushed onto the stack, and the 
parser transits to State 5. Then the verb phrase and the noun phrase on 
top of the stack are reduced to a sentence (S) using Rule 1 (S ! NP VP), 
which is pushed onto the stack, and the parser transits to State 4. Finally, 
the input string is accepted. 
4.10.3 LR-Table Compilation 
Compiling LR parsing tables consists of constructing the internal states 
(i.e. sets of dotted items) and from these deriving the shift, reduce, accept 
and goto entries of the transition relation. 
New states can be induced from previous ones; given a state S1, another 
state S2 reachable from it by goto(S1,Sym,S2) (or shift(S1,Sym,S2) if 
Sym is a terminal symbol) can be constructed as follows: 
1. Select all items in state S1 where a particular symbol Sym follows 
immediately after the dot and move the dot to after this symbol. 
This yields the kernel items of state S2. 
2. Construct the non-kernel closure by repeatedly adding a so-called 
non-kernel item (with the dot at the beginning of the RHS) for each 
grammar rule whose LHS matches a symbol following the dot of some 
item in S2. 
For example State 1 of Figure 4.2 can be constructed from State 0 by 
advancing the dot of the items S !  NP VP and NP !  NP PP to form 
the items S ! NP  VP and NP ! NP  PP which constitute the kernel 
of State 1. The remaining non-kernel items are generated by the grammar 
rules for VPs and PPs (the categories following the dots in the new kernel 
items), namely Rules 2, 3, 4, 5 and 9.

96 CHAPTER 4. STOCHASTIC GRAMMARS 
Using this method, the set of all parsing states can be induced from an 
initial state whose single kernel item has the top symbol of the grammar 
preceded by the dot as its RHS. In Figure 4.2 this is the item S0 !  S of 
State 0. 
The shift, goto and accept entries fall out automatically from this procedure. 
Any dotted item where the dot is at the end of the RHS gives 
rise to a reduction by the corresponding grammar rule. Thus it remains to 
determine the lookahead symbols of the reduce entries. 
In Simple LR (SLR) the lookahead is any terminal symbol that can 
follow immediately after a symbol of the same type as the LHS of the rule. 
In LookAhead LR (LALR) it is any terminal symbol that can immediately 
follow the LHS given that it was constructed using this rule in this state. 
In general, LALR gives considerably fewer reduce entries than SLR, and 
thus results in faster parsing. 
4.10.4 Generalized LR Parsing 
Generalized LR (GLR) parsing extends basic LR parsing with two concepts; 
a graph-structured stack (GSS) and a packed parse forest ([Tomita 
(ed.) 1991]). It also diers in being a breadth rst, accumulative search, 
synchronizing on shift actions, rather than a depth-rst backtracking algorithm. 
Conceptually, a GLR parser works as follows: 
1. Shift the next input symbol onto the stack(s). Don't keep the old 
stack(s), only the new one(s). 
2. Perform all possible reduce actions on the stack(s). This will give rise 
to a new stack for each reduction. Keep the old stacks (before the 
reductions). Repeatedly perform all possible reduce actions on the 
new set of stacks, accumulating new stacks, until no further reduce 
actions are possible. 
3. Goto 1. 
A graph-structured stack is used instead of a set stacks. This means that 
the new portions of the GSS constructed by shifting in Step 1 are merged 
with the stack portions resulting from subsequent (repeated) reductions in 
Step 2. 
After a shift action, two stack portions are merged if the new top states 
are equal. If we perform a reduction and the corresponding goto entry would 
imply creating a stack continuation that already exists, we simply use the 
existing one. The two partial parse trees corresponding to the two merged 
LHS symbols will then dominate the same substring, but be structually 
dierent: Since all surviving stack portions will have just shifted the word 
prior to the current string position (and possibly have been subjected to 
subsequent reductions), the dominted string will end at the the current 
string position. Since the same node was reached by the reduction prior 
to pushing the LHS symbol onto the GSS, the dominated string will begin 
at the same string position. Since the action seqeunces producing the two 
derivations are dierent, the partial parse trees will also be dierent. 
The partial parse trees associated with any nonterminal symbol will 
however not be recoverable from the GSS. For this reason, we will store the 
structure of each partial parse tree associated with each node in the GSS 
in a parse forest. By doing this in a clever way, we can avoid multiplying 
out the potentially exponentially many dierent parse trees that can be

4.10. PROBABILISTIC LR PARSING 97 
associated with the parsed word string | This is where the packed parse 
forest enters into the story. 
The LHS symbol of any reduction is constructed from a sequence of RHS 
symbols, each associated with a (set of) partial parse tree(s). The latter 
will be recorded in the packed parse forest, and the only information we 
need to specify for the node of the packed parse forest associated with the 
LHS of a reduciton is the set of nodes in the packed parse forest associated 
with the RHS symbols. This means that the internal structure of each RHS 
symbol is encapsulated in its node in the packed parse forest. So, regardless 
of in how many dierent ways these RHS symbols can in turn have been 
constructed, this ambiguity is contained locally at the corresponding nodes 
of the packed parse forest, and not multiplied out at the node corresponding 
to the current LHS of the production. This is known as local ambiguity 
packing. 
In the case of a normal CFG parsing algorithm, we need one node for 
each nonterminal symbol and each string position, see 4.1.4. Here we need 
one for each node that ever gured in the graph-structured stack. hJohn 
Carroll writes one for each internal state.i 
4.10.5 Adding Probabilities 
As pointed out above, we can calculate the parse probability P( ) from the 
probability of the sequence of productions used in the rightmost derivation 
of the parse viewed in reverse order. This gives us the very same recurrence 
equation as when using the sequence of productions used in the leftmost 
derivation, but the interpretation is now somewhat dierent: 
P( ) = P(1 = ri1; : : :; M1 = riM1; M = riM) = 
= 
M Ym=1 
P(m = rim j 1 = ri1; : : :; m1 = rim1 ) 
Since there is a one-to-one correspondence between this sequence and the 
sequence of actions of the LR parser, we can equally well use the action 
sequence to specify the rightmost derivation in reverse, and thus the parse: 
P( ) = P(1 = ai1; : : :; L1 = aiL1; L = aiL) = 
= 
L Y
l=1 
P(l = ail j 1 = ai1; : : :; l1 = ail1 ) 
Here al is the lth action performed by the LR parser. 
We will approximate the action probabilities conditional on the previous 
action sequence, P(l = ail j 1 = ai1; : : :; l1 = ail1 ), with the action 
probabilities conditional on the parsing state Skl after l  1 actions, just 
prior to the lth action, and further approximate these probabilities by using 
an extractor function g(S) to select relevant portions of the parsing states: 
P(l = ail j 1 = ai1; : : :; l1 = ail1)  P(l = ail j g(Skl )) 
For example, this function might select the current internal state and the 
current lookahead symbol, and discard the rest of the stack content, and 
the remaining input string. The main point is that during parsing, these 
approximations can be determined locally, and thus multiplied together: 
P( )  
L Y
l=1 
P(l = ail j g(Skl ))

98 CHAPTER 4. STOCHASTIC GRAMMARS 
This eectively constructs equivalence classes for the various histories of 
action sequences. 
Let us return to the parse tables of 4.3. We will here assign a probability 
distribution to each pair of state and lookahead symbol over the set of 
possible actions. As we see, the only situations where we have nondeterminism 
in the parsing tables are in States 11,12 and 13 with a preposition 
as a lookahead. In State 11, for example, we have the shift-reduce conict 
s8/r3, which corresponds to a potential PP-attachment ambiguity in the 
input string. We will here assign some probability p11 to s8 and 1  p11 
to r3 in State 11 with lookahead symbol preposition. Similarly, we assign 
p12 to s8 and 1  p12 to r4 in State 12, and p13 to s8 and 1  p13 to r9 in 
State 13, in both cases with lookahead symbol preposition. The rest of the 
actions are uniquely determined by the state and lookahead symbol and 
thus have probability 1. 
4.10.6 Probabilistic GLR Parsing 
One way of doing probabilitisc LR parsing is to rst construct a packed 
parse forest by normal GLR parsing, but in the process attribute a probability 
to each dierent local decision made at each node. These probabilities 
are calculated as the product of the action sequences required to construct 
the mother symbol from its daughter symbols. However, the probabilities 
of the latter have not yet been propagated to the former to determine the 
probability of each possible analysis; the representeation is thus packed also 
probabilistically. The probabilities can be made conditional on arbitrary 
portions of the state of the LR parser, i.e., on the stack content and the 
remaining input string. 
The parse probability is dened as the product of each action probability 
conditional on the previous sequence of actions. Since these action 
probabilites are approximated by the probabilites conditional on the state 
of the LR parser, the probability of a parse is simply the product of the 
probability of each decision in the packed parse forest. So in the disambiguation 
step, a Viterbi-like search of the packed parse forest can be used 
to nd the most probable parse. This means that the time complexity 
of the disambiguation step is linear in the size of the packed parse forest, 
as is calculating the string probability by summing instead of maximizing. 
hCheck!i 
In this way, probabilistic LR parsing is broken down into two distinct 
steps: Normal GLR parsing, with an extra element of attributing each decision 
in the packed parse forest with a probability. Each such probability 
is simply the product of a sequence of action probabilities, and calculating 
them introduces only a constant overhead, and does not add to the complexity 
of GLR parsing. The worst-case time complexity of the original GLR 
parsing algorithm is exponential, both in string length and grammar size. 
This is since there can theoretically be an exponential number of dierent 
internal states in the size of the grammar, and for certain grammars there 
may be inputs that force a parser to visit all states [Johnson 1991]. It can 
however be brough down to polynomial in string length by an optimization 
due to Kipps, see [Kipps 1991]. It essentially involves avoiding to search 
the graph-structured stack for the return state when reducing by instead 
employing a dynamically built table. Using large portions of the stack 
for probabilistic conditioning counteracts this eect, and increases parsing 
complexity. 
The actual probabilistic disambiguation is done on the output of the

4.11. SCORING 99 
GLR parsing step, on the packed parse forest. This can be done using 
essentially Viterbi search in time linear in the size of the packed parse 
forest. Although the size of the parse forest is polynomial in string length, 
it is worst-case exponential on the size of the grammar for the same reasons 
that parsing complexity is exponential in grammar size. 
So although probabilistic LR parsing is theoretically exponential in 
grammar size, in practice, it doesn't seem to get near these bounds [Carroll 
1994]. For a discussion on how to extend probabilistic LR parsing to uni- 
cation grammars, and for interesting reading on probabilistic LR parsiong 
in general, we can recommend [Briscoe & Carroll 1993]. 
4.11 Scoring 
A practical problem is that due to various probability losses, i.e., the fact 
that the estimated probabilities tend to be smaller than they ought to 
be, parses involving longer derivations tend to be penalized. This is because 
they involve a greater number of derivation steps, involving a greater 
number of multiplications with too small quantities. One popular remedy, 
[Magerman & Marcus 1991], [Carroll 1994], is to instead use the geometric 
mean of the probabilities instead of their product. This means that we 
leave the realm of probability theory, and enter into the heuristic scoring 
business. For example, this complicates using the Viterbi algorithm for 
nding the most probable parse. 
Due to the conditionings of the probabilites, SCFGs are rather insensitive 
to lexical information, and in particular to lexical co-occurrences. 
One method for trying to compenstate for this is to include amongst other 
things lexical information in more or less ad hoc ways by devising a score 
accepting contributions from various information sources. This has been 
explored in a number of dierent contexts: For Earley parsing with Scoring, 
see [Magerman & Marcus 1991], for CYK parsing with scoring, see [Magerman 
& Weir 1992], for LR parsing with scoring, see [Su et al 1991], and 
for parsing with unication grammars with scoring, see [Alshawi & Carter 
1994]. 
hTo be extented.i

100 CHAPTER 4. STOCHASTIC GRAMMARS

Chapter 5 
Selected Topics in 
Statistical NLP 
In the following, we will distinguish three types of literatur: Main articles, 
additional articles which are of major interest in a basic course on statistical 
methods in NLP (see \Also" item), and further readings. 
5.1 Statistical Part-of-Speech Tagging 
5.1.1 In short 
max 
Tags
P(Tags j Word String) 
5.1.2 Linguistic Background 
Part-of-speech (PoS) tagging consists in assigning to each word of an input 
text a (set of) tag(s) from a nite set of possible tags, a tag palette or a tag 
set. The reason that this is a research issue is that a word can in general be 
assigned dierent tags depending on context. This assignment can be done 
in a number of dierent ways. One of these is statistical tagging, which is 
advocated in [Church 1988], [Cutting et al 1992] and many other articles. 
Here, the relevant information is extracted from large sets of often handtagged 
training data and tted into a statistical language model, which is 
then used to assign the most likely tag to each word in the input text. 
5.1.3 Basic Statistical PoS tagging 
We will describe a generic, but somewhat vanilla-avoured statistical PoS 
tagger. Statistical PoS taggers generally distinguish between lexical probabilities, 
i.e., the probability of a particular tag conditional on the particular 
word, and contextual probabilities, which describe the probability of a particular 
tag conditional on the surrounding tags. The latter conditioning is 
usually on the tags of the neighbouring words, and very often on the n1 
previous tags. 
Thus we in general have the following two information sources: 
 Lexical probabilities: 
The probability of each tag Ti conditional on the word W that is to 
101

102 CHAPTER 5. SELECTED TOPICS IN STATISTICAL NLP 
be tagged, P(Ti j W). Often the converse probability P(W j Ti) is 
given instead. 
 Tag N-grams: 
The probability of tag Ti at position k in the input string, denoted Tik 
, 
given that tags Tkn+1 : : :Tk1 have been assigned to the previous 
n  1 words. Often n is set to two or three, and thus bigrams or 
trigrams are employed. When using trigram statistics, this quantity 
is P(Tik
j Tk2; Tk1). 
These probabilities can be estimated either from a pretagged training 
corpus or from untagged text, a lexicon and an initial bias, see Section 2.1.7. 
The training data is often divided into a training set, used to estimate the 
statistical parameters, and a set of held back data used to cope with sparse 
data by way of backo smoothing. For example, tag trigram probabilities 
can be estimated as follows: 
P(Tik
j Tk2; Tk1)  3f(Tik
j Tk2; Tk1) + 2f(Tik
j Tk1) + 1f(Tik
) 
Here f is the relative frequence in the training set. The weights j = 
j(Tk2; Tk1) may depend on the particular contextual tags, but are required 
to be nonnegative and to sum to one over j. Appropriate values 
for these weights can be estimated using the held-out portion of the training 
corpus by employing any of a number of techniques; two ones much 
used today are deleted interpolation, [Jelinek & Mercer 1980], and modied 
Good-Turing estimation, [Church & Gale 1991]. Another possibility is to 
use all data for training and employ successive abstraction to perform the 
backo smoothing, see [Brants & Samuelsson 1995]. 
In general, the information sources S1; : : :; Sn are combined by multiplying 
the scaled probabilities: 
P(T j S1; : : :; Sn) 
P(T)  
n Y
i=1 
P(T j Si) 
P(T) 
This formula can be established by Bayesian inversion, then performing 
the independence assumptions, and renewed Bayesian inversion: Assume 
that we have information sources S1; : : :; Sn and we wish to estimate P(T j S1; : : :; Sn), the probability of tag T given this information. 
P(T j S1; : : :; Sn) = 
= 
P(T)  P(S1; : : :; Sn j T) 
P(S1; : : :; Sn)  P(T)  
n Y
i=1 
P(Si j T) 
P(Si) 
= 
= P(T)  
n Y
i=1 
P(T)  P(Si j T) 
P(T)  P(Si) 
= P(T)  
n Y
i=1 
P(T j Si) 
P(T) 
In particular, using lexical statistics and trigram probabilities, we get 
P(Tk j T1; : : :; Tk1;W1; : : :;Wn)  
 
P(Tk j Tk2; Tk1)  P(Tk j Wk) 
P(Tk) 
= 
P(Tk j Tk2; Tk1)  P(Wk j Tk) 
P(Wk) 
The tagger works as follows: First, each word is assigned the set of all 
possible tags according to the lexicon. This will create a lattice. A dynamic

5.2. STATISTICAL MACHINE TRANSLATION 103 
programming technique is then used to nd the sequence of tags T1; : : :; Tn 
that maximizes 
P(T1; : : :; Tn j W1; : : :;Wn) = 
= 
n Y
k=1 
P(Tk j T1; : : :; Tk1;W1; : : :;Wn)  
 
n Y
k=1 
P(Tk j Tk2; Tk1;Wk)  
 
n Y
k=1 
P(Tk j Tk2; Tk1)  P(Tk j Wk) 
P(Tk) 
= 
= 
n Y
k=1 
P(Tk j Tk2; Tk1)  P(Wk j Tk) 
P(Wk) 
Since the maximum does not depend on the factors P(Wk), these can be 
omitted, yielding the standard statistical PoS tagging task: 
max 
T1;:::;Tn 
n Y
k=1 
P(Tk j Tk2; Tk1)  P(Wk j Tk) 
This is well-described in for example [DeRose 1988]. 
5.1.4 Suggested Reading 
 Main: [DeRose 1988], [Cutting et al 1992] 
 Also: [Church 1988], [Weischedel et al 1993] 
 Further: [Black et al 1992], [Schuetze 1994], [Merialdo 1994] 
The rst chapter of [Karlsson et al 1995] contains a nice overview of 
part-of-speech tagging in general. [Church 1988] is a classical reference on 
basic statistical part-of-speech taggging. Dynamic programming, in particular 
the Viterbi algorithm, as applied to part-of-speech tagging is very 
well described in [DeRose 1988]. Other sections, in particular that on the 
CLAWS system, are less perspicuous. The decisive reference on dynamic 
programming in general is [Bellman 1957]. [Cutting et al 1992] describes 
the use of an HMM-based tagger were the parameters are estimated from 
unannotated text using the Baum-Welch algorithm. 
5.2 Statistical Machine Translation 
Currently under construction. Due mid 1996. 
5.2.1 In short 
max 
Target Text
P(Target Text j Source Text)

104 CHAPTER 5. SELECTED TOPICS IN STATISTICAL NLP 
5.2.2 Suggested Reading 
 Main: [Brown et al 1993], 
 Also: [Church 1993], [Wu 1995] 
 Further: [Brown et al 1990], [Kay & Roscheisen 1993], [Dagan et al 
1993], [Gale & Church 1993], [Kupiec 1993] 
5.3 Statistical Language Learning 
Currently under construction. Due mid 1996. 
 Main: [Brill 1993], [Magerman & Marcus 1990] 
 Also: [Osborne & Bridge 1994], [Daelemans 1994]. 
5.4 Structural Ambiguity and Semantic Clas- 
ses 
5.4.1 Linguistic Background 
In natural language analysis we have to deal with a number of structural 
ambiguous constructions, i.e. syntactic structures that are equally well licensed 
by dierent derivations. Thus we get more than one correct parse 
tree for such a construction. Depending on the context, one of the syntactically 
possible structures is the preferred one. In linguistics the most famous 
class of structural ambiguities are prepositional phrase attachments. For 
those who have never heard about such a thing, go on reading. Those who 
are familiar with the term could skip this subsection. 
The syntactic structure of a the famous sentence I saw the man with the 
telescope can be considered as either [S I [VP saw [NP the man [PP with the 
telescope]]]] or [S I [VP saw [NP the man] [PP with the telescope]]]. That 
means the with phrase can either be attached to the object noun phrase 
or to the verb phrase. In the rst case, the interpretation is that the man 
had the telescope, in the second the telescope modies the seeing event. 
Without any extra knowledge ( semantic or pragmatic context) there is no 
way to decide which of these two readings will be more appropriate. If we 
consider the sentence I saw the deer with the telescope our knowledge about 
the world tells us that attachment of the PP to the object NP leads to an 
odd interpretation, as in our world a deer rarely comes with a telescope. 
Nevertheless the structure is absolutely correct. 
To deal with this kind of ambiguities a number of dierent approches 
have been proposed, such as: 
 The Discourse Model Approach 
Altman and Steedman 1988 ([Altmann & Steedmann 1988]) claim 
that pp-attachment can only be resolved by considering discourse 
information. In terms of computational linguistics this means modelling 
of discourse information which is fairly complex and not yet well 
understood.

5.4. STRUCTURAL AMBIGUITY AND SEMANTIC CLASSES 105 
 The Structure-Based Approach 
A number of psycholinguists tried to explain attachment preferences 
by means of general principles that are supposed to govern human language 
processing. Two famous but controversal priciples are right association 
([Kimball 1973]), and minimal attachment ([Frazier 1978]). 
Right association states that a constituent tends to attach to another 
constituent immediately to its right. Whereas minimal attachment 
says that a constituent tends to attach so that a minimal number of 
nodes in a derivation tree is required. But these two principles are 
not accepted without argument. Interested readers may nd further 
discussion in [Konieczny et al. 1997]. 
 The Lexical Association Approach 
This approach basically assumes that attachment ambiguities can be 
resolved by lexical information, such as information related to the 
dominating verb, the object head noun the preposition, and the head 
noun of the NP dominated by the preposition. We distinguish two 
major branches, one inuenced from psycholinguistics1, the other based 
on statistics. The later we will elaborate on in the following. 
5.4.2 Association Models 
The main assumption is that resolution of structural ambiguity is often possible 
with limited lexical information, and this information can be learned 
from either a corpus ([Hindle & Rooth 1993]), or a corpus and a semantic 
hierarchy ([Resnik 1993]). While Resnik advocates a class-based model 
which makes use of conceptual relationships such as those represented in 
WordNet2, Hindle and Rooth adopt lexical associations discovered from 
text for structural disambiguation. Hindle and Rooth integrate lexical information 
on the preceding verb, the object head noun, and the following 
preposition into their mathematical model. Resnik incorporates information 
on the head verb, the preposition, the semantic class of the head noun 
of the NP governed by the preposition, and the semantic class of the object 
head noun. 
Preparation of the Corpus 
In order to access the relevant head nouns, the verbs, and the prepositions 
the corpus must be PoS-tagged, and rudimentary parsed. 
elaborate on standard taggers and synt. bracketing elsewhere 
Estimation of Attachment Preferences by Lexical Information 
In the training phase noun-preposition and verb-preposition bigrams as well 
as noun and verb unigrams are derived from the corpus. The preposition 
is either assigned to the noun or the verb. Assignment decisions are made 
according to: 
1See for example [Wittemore et al 1992], [Ford et al. 1982], [Taraban & McClelland 
1988], [Marcus 1980]. 
2WordNet is a conceptual taxonomy for English words. Conceptual relati- 
ons are represented as is-a hierarchies. You nd more about WordNet from 
ftp clarity.princeton.edu [128.112.144.1] (in the US), and ftp.ims.uni-stuttgart.de 
[141.58.127.61] (in Europe).

106 CHAPTER 5. SELECTED TOPICS IN STATISTICAL NLP 
 purely linguistic criteria, such as PPs do not attach to pronouns, PPs 
that follow a subject NP or a NP in preverbal position attach to the 
NP, at least in English, 
 thresholds derived from t-scores or 
 defaults 
If no clear attachment decision can be made, neither by linguistics, 
nor by t-scores, the prepositions are equally attachted to the head 
nouns of the object NPs, or to the verbs. 
In order to guess attachment preferences, one is interested in the contrast 
or dierence between the conditional probalility of seeing a certain 
preposition given a particular noun, and the conditional probalility of seeing 
a certain preposition given a particular verb, i.e. 
P(prepjnoun) P(prepjverb) 
These conditional probabilies correspond to the preposition-noun or 
preposition-verb bigram frequences respectively which have been derived 
from the training corpus. The bigram frequencies are normalized by the 
global verb or noun frequency respectively. Thus the conditional probabilities 
are dened as follows: 
P(prepjnoun)  
f(noun preposition bigram) 
f(noun) 
, and 
P(prepjverb)  
f(verb preposition bigram) 
f(verb) 
For a statistics-based determination of pp-attachment preferences for a 
specic sentence a t-score is derived from the conditional probabilities; 
t  
P(prepjnoun)  P(prepjverb) 
2p2(P(prepjnoun)) + 2(P(prepjverb)) 
To make sure that the signicance of the result is at a certain (at least 
a 95%) level a threshold is dened accordingly.3 In all cases of attachment 
ambiguity where the results are below the dened threshold, pp-attachment 
is resolved according to the default case. 
Sparse Data 
Sparseness of training data is a common problem in statistics-based approaches. 
In case of estimation of lexical associations on the basis of word 
forms a fairly large ammount of training data is required, because all in- 
ected forms of a word will be considered as dierent words. This can be a 
major problem in highly inecting languages like German. A better result 
by the same amount of data can be achieved by looking at lemmata (inectional 
dierences are ignored) or even at semantic classes. In the later case 
dierent words/lemmata are grouped together according to their semantic 
or conceptual characteristics. 
3For denition of signicance levels see chapter ??.

5.4. STRUCTURAL AMBIGUITY AND SEMANTIC CLASSES 107 
Estimation of Class Frequencies 
In a training corpus the head nouns of the object NPs, and the head nouns 
of the NPs subcategorized from a preposition are related to conceptual 
classes inWordNet. Class frequencies are estimated from lexical frequencies 
such that the frequency of a specic class C is determined by the lexical 
frequencies of the members n of that class and its subclasses c, i.e. 
f(C) = X n2cC 
f(n) 
Estimation of Conceptual Relationships 
Conceptual relationships are discribed by means of selectional preference, 
selectional association, and semantic similarity. 
The term selectional preference originates from linguistics. It is used 
to capture semantic dierences such as those given in Mary drank some 
wine / gasoline / pencils / sadness. While drinking and wine semantically 
correspond well, drinking of gasoline is quite odd, pencils are impossible 
to drink, and the semantic interpretation of drinking sadness is subject 
to metaphorisation. A widely used technique in statistics-based NLP to 
represent conceptual (dis)similarity is entropy (cf. ??). Thus the selectional 
preference of a word w for C, a semantic class and its subclasses, is dened 
as the relative entropy (also known as Kullback-Leibler distance) between 
two sets of probabilities, the prior probability P(C) and the conditional 
probability P(Cjw), which can be written as 
H[P(Cjw); P(C)]= X cjsubconcept of C 
P(cjw)log
P(cjw) 
P(c) 
The selectional association between a word w and a subconcept c, 
A(w; c), is the contribution c makes to the selectional preference of w for 
C, i.e. the relative entropy between a wird w, and a concept c normalized 
by the relative entropy between the word w and the larger concept C. 
A(w; c) = 
P(cjw)log P(cjw) 
P(c) 
H[P(Cjw); P(C) 
The selectional association between two words w1 and w2, A(w1; w2) is 
the maximum of A(w1; c) over all classes c to which w2 belongs. 
A(w1; w2) = max X cjw22c
A(w1; c) 
The semantic similarity of two nouns n1, n2, sim(n1; n2), is dened 
by the most specic class c both n1 and n2 belong to, i.e. the class c which 
is most informative for n1 and n2. 
sim(n1; n2) = max[logP(c)] 
Class-Based Evaluation of Attachment Sites 
The following trigrams are derived from the corpus and the noun taxonomy: 
 verb preposition classj

108 CHAPTER 5. SELECTED TOPICS IN STATISTICAL NLP 
 classi preposition classj 
Where classi is the conceptual class of the object NPs head noun and 
classj is the conceptual class of the head noun of the NP dominated by the 
preposition. 
Verb- and noun-scores are calculated as the relative entropy of the 
verb preposition classj trigram frequencies, and the classi preposition classj 
trigram frequencies, such that 
vscore = f(verb prep classj )log
P(verb prep classj ) 
P((verb) 
, and 
nscore = f(classi prep classj)log
P(classi prep classj ) 
P((classi) 
. 
To reduce the load of computation, rst all classes of the preposition's 
nominal object are considered, and then the class of the verb's nominal 
object is maximized. A paired t-test is calculated on the v- and nscores. In 
case of a positive result, the PP is attached to the object NP. In case of a 
negative result, the PP is attached to the verb. 
Estimators 
Hindle and Rooth work with ELE, Resnik suggested MLE or Good-Turing. 
For a discussion of estimation techniques see ??. 
5.4.3 Suggested Reading 
 Main: [Hindle & Rooth 1993] 
 Also: [Resnik 1993] 
 Further [Alshawi & Carter 1994] 
5.5 Word Sense Disambiguation 
Word sense disambiguation is a rather widespread task in statistical NLP. 
The approaches vary wrt. the parameters they estimate disambiguatuation 
information from. The techniques applied are rather similar. Parameters 
are either estimated from monolingual ([], []) or bilingual4 corpora ([Gale 
et al 1992], [Brown et al 1990]). In case of monolingual training material, 
words are either clustered according to their distribution in structural context 
([Pereira et al 1993], [Dagan et al 1994], []) or their distribution wrt. 
semantic category ([Gale et al 1992], [Yarowsky 1992]). 
5.5.1 Phenomena 
5.5.2 Parameter Estimation 
5.5.3 Disambiguation Model 
Bayesian discrimination 
4The standard bilingual is the Hansards corpus which comprises the french-english 
translations of Canadian Parliament speeches.

5.6. LEXICAL KNOWLEDGE ACQUISITION 109 
relative entropy 
mutual information 
?singular value decomposition 
?linear regression 
??? 
5.5.4 Suggested Reading 
 Main: [Gale et al 1992], [Yarowsky 1992] 
 Also: [Yarowsky 1992], [Gale et al 1992] 
 Further: [Pereira et al 1993], [Schuetze 1992], [Dagan et al 1993], 
[Dagan et al 1994] 
5.6 Lexical Knowledge Acquisition 
max 
Lexical Entry 
P(Lexical Entry j Text) 
 Main:[Manning 1993], [Utsuro et al 1992], [Smadja 1993]

110 CHAPTER 5. SELECTED TOPICS IN STATISTICAL NLP

Appendix A 
Desiderata 
The follwing appendices are planned: 
 More on Calculus 
 Some Numerical Analysis 
 Some Statistical Tables (Distributions, etc.) 
 Corpus Linguisitc Tools 
111

112 APPENDIX A. DESIDERATA

Appendix B 
Tools 
B.1 Simple Unix Commands 
Before we can do all the fancy statistics on texts, we need to access the raw 
data. Unix provides a number of built-in commands that are very useful 
to do so. In the following a list of elementary commands valuable for text 
preprocessing is given. The online man pages on your Unix machine will 
give you more information on the specic commands. 
Command Usage 
grep search a le for a pattern 
egrep search a le for disjunctive patterns 
sort sort and/or merge les 
uniq report repeated lines in a le 
tr translate characters 
wc display a count of lines, 
words and characters in a le 
cut cut out selected elds of each line of a le 
paste merge same lines of several les or 
subsequent lines of one le 
join merges those lines of a le that have the same key 
comm select or reject lines common to two sorted les 
cat concatenate les 
tail writes the last part of a le to standard output 
split split a line by indicated split characters 
and particularly useful: 
split -n split a le into pieces of n lines 
sed stream editor 
Unix also comes with a number of programming languages of which the 
following are popular for text handling: 
Awk is a pattern scanning and processing language that is preferably 
used for database construction. 
Lex or its successor ex is a language for lexical analysis. Typically 
tokenizer are written in lex or ex. Lex programmes can be interleaved 
with C code. They are compiled into C, and thus are rather 
ecient. 
Perl is a language for easy manipulation of text, les, and processes. 
113

114 APPENDIX B. TOOLS 
It borrows capabilities from shells and C, and subsumes awk and sed. 
Perl is compared to C easy to learn, but not very ecient. 
From the following examples1 you might get an idea about what you 
could do with the above commands. A good guide for novices is Ken 
Church's Unix for poets [Church 1994]. 
For those who are not used to Unix, the following is to understand the 
syntax of the command lines occurring in the examples below: 
< read from input file 
> write to output file 
| pipe 
| more pipe to more; 
more displays standard output screenwise 
The following is a list with the most essential characters to build search 
patterns with: 
+ one or more instances of a specified pattern 
* zero or more instances of a specified pattern 
? at most one instance of a specified pattern 
. any single character 
[a-z] any one character from the small alphabet 
[A-Z] any one character from the ?big alphabet 
[0-9] any one number between 0 and 9 
^ begin of line 
$ end of line 
| disjunction of patterns 
\ indicates that the following character is interpreted 
as such 
B.2 Split up a text using tr 
 Split up a text by blanks, and write each word (i.e. string of characters 
surrounded by blanks) in a single line to a le. The input le is inle, 
the output le is outle. n012 is the ASCII code for new line. 
tr ' ' '\012' < infile > outfile 
 Get rid of empty lines. 
tr -sc 'A-Za-z' '\012' < infile > outfile 
Now you have a le, lets call it wl, containing your input text as word 
list. With the word list you can do other fancy things. 
B.3 Sort word list: sort, uniq 
 Sort word list by dichtionary order. 
sort -d wl 
1The material is taken from an introductory course on corpus linguistics held by the 
authors at Uppsala University.

B.4. MERGE COUNTS FOR UPPER AND LOWER CASE: TR, SORT, UNIQ115 
 Sort word list by dichtionary order, and get rid of duplicates. Write 
output to outle. 
sort -d wl | uniq > outfile 
or 
sort -du wl > outfile 
 Sort word list by dictionary order, get rid of duplicates, and give a 
count of how often each word appeared in the word list. 
sort -d wl | uniq -c > outfile 
 Sort word list according to word frequency (numeric order). 
uniq -c wl | sort -d > outfile 
B.4 Merge counts for upper and lower case: 
tr, sort, uniq 
 Sometimes you just want to know how often a specic word occurs 
in a text. And you do not want to make a distinction between upper 
and lower case letters. 
tr 'A-Z' 'a-z' wl | uniq -c | sort -d > outfile 
B.5 Count lines, words, characters: wc 
wc infile counts lines, words, characters 
wc -l infile counts lines 
wc -w infile counts words 
wc -m infile counts characters 
B.6 Display the rst n lines of a le: sed 
sed 5q infile displays the rst 5 lines of the le inle 
sed 50q infile displays the rst 50 lines of the le inle

116 APPENDIX B. TOOLS 
B.7 Find lines: grep, egrep 
grep gh find lines containing 'gh' 
grep '^gh' find lines beginning with 'gh' 
grep 'hg$' find lines ending with 'hg' 
grep '^gh.*hg$' find lines beginning with 'gh' 
and ending with 'hg' 
grep -v gh delete lines containing 'gh' 
grep -v '^gh' delete lines beginning with 'gh' 
grep -v 'gh$' delete lines ending with 'gh' 
grep -v '^gh.*hg$' delete lines beginning with 'gh' 
and ending with 'hg' 
Note with egrep you can specify disjunctive search patterns; for instance: 
egrep 'ful$|ly$|ant$' find lines ending with 
'ful', 'ly', or 'ant' 
B.8 n-grams: tail, paste 
Suppose we have got a text le with each word on a single line. Now we 
can easily create all sorts of n-grams simply by using tail and paste. wl 
contains the original word list, nextwords contains the original word list 
reduced by the rst element, nextnextwords contains the original word list 
reduced by the rst two elements. The les bigrams, and trigrams contain 
all bigrams or trigrams respectively occurring in the text. 
 Create bi-grams: tail, paste 
tail +2 wl > nextwords 
paste wl nextwords > bigrams 
 Create tri-grams: 
tail +3 wl > nextnextwords 
paste wl nextwords nextnextwords > trigrams 
 : : : 
B.9 Manipulation of lines and elds: awk 
Despite awk is a general purpose programming language it is intended for 
shorter programs, especially for easy manipulation of lines and elds. Note, 
awk is fully integrated into Perl. Warning: awk also comes as nawk, and 
gawk. 
The following is useful to know:

B.9. MANIPULATION OF LINES AND FIELDS: AWK 117 
 Fields are addressed by $ eldnumber 
$1 rst eld 
$NF last eld 
$(NF-1) penultimate eld 
 Some operators 
>, <, >=, <=, ==, =!, etc. 
For more information see the man pages. 
 With -F you specify the eld seperator, e.g. -F: species : as current 
eld seperator. Blank is the default eld seperator. 
 Select elds by position 
The following command line species : as the current eld seperator, 
and prints the 4th eld of each line (record) to standard output. The 
input le is inle. 
awk -F: '{print $4}' infile 
 Filter elds by numerical comparison 
The following command line matches elds where the numerical value 
of the rst eld is larger than 2, and prints the contents of the 3rd 
eld of the according record to standard output. The le seperator is 
set to default (i.e. blank). The input le is inle. 
awk '$1 > 2 {print $3}' infile 
 Filter elds by string comparison 
The following command line matches lines where the rst eld is 
identical to the last one, and prints the contents of the 3rd eld to 
standard output. The le seperator is set to default (i.e. blank). The 
input le is again inle. 
awk '$1 == $NF {print $3}' infile

118 APPENDIX B. TOOLS

Appendix C 
Some Calculus 
C.1 Numbers 
C.1.1 Natural Numbers 
N = f0; 1; 2; : : :g 
The natural numbers are the natural ones when counting, i.e., for answering 
questions of the type \How many articles did Brigitte write this 
year?". 
The most important formal property of these numbers is that every one 
of them has a unique successor, i.e., there are countably innitely many of 
them, and there is a linear order dened on them. 
Also, one can add and multiply natural numbers, e.g., 2 + 3 = 5 and 
2  3 = 6. Thus, addition (+) and multiplication () are dened for these 
numbers. 
C.1.2 Integers 
Z = f: : : ;2;1; 0; 1; 2; : : :g 
Z+ = f1; 2; : : :g 
If we can add natural numbers, we can also formulate equations over 
them, e.g., 2+x = 5. However, to guarantee that we always have a solution 
to these additive equations, we are forced to extend the set of natural 
numbers with the set of negative integers to form the set of integers, e.g., 
when we want to solve the equation 5 + x = 2. 
This means that for each n 2 N we introduce the additive inverse n 
with the property that n + (n) = 0. n + (m) is usually written n  m 
if m 2 Z+. Note that 0 is its own inverse, since 0 + 0 = 0. In fact, 
x + 0 = 0+ x = x for all numbers x, and 0 is formally referred to as the 
neutral element, or identity, of addition. 
C.1.3 Rational Numbers 
The set of rational numbers Q is the set of all numbers of the form 
m
n 
, with 
m; n 2 Z; n 6= 0. 
119

120 APPENDIX C. SOME CALCULUS 
Q = fx : x = 
m
n 
;m; n 2 Z; n 6= 0g 
We can also formulatemultiplicative equations over the natural numbers 
and the integers, e.g., 2x = 6. If we want to guarantee that we always have 
a solution to these equations, we are forced to extend the set of integers 
to the set of rational numbers, e.g., when we want to solve the equation 
2  x = 5. 
Analogous to extending the natural numbers with their additive inverses, 
we will extend the integers with their multiplicative inverses. i.e., for 
each integer n we add n1 with the property that n  n1 = 1. m  n1 is 
usually written m=n or 
m
n 
. The only exception is the number 0, which has 
no inverse, since 0  x = 0 6= 1 for all numbers x. Similarly, 1 is the neutral 
element, or identity, of multiplication, since x1 = 1x = x for all numbers 
x, and 1 is its own multiplicative inverse. 
Since we don't want to be able to multiply ourselves out of Q, we need to 
add a lot of other numbers, apart from n1 for all integers n, namelymn1 
for all pairs of m; n 2 Z. Note that some pairs correspond to the same 
rational number, e.g. 
2
4 
= 
1
2
. This is called constructing the multiplicative 
closure of f0; n; n1 : 0 6= n 2 Zg. We should really construct the additive 
and multiplicative closure, which is known as the algebraic closure of this 
set, but here, the multiplicative closure alone suces. 
Since each rational has an additive inverse and each rational, except 0 
has a multiplicative inverse, and since addition and multiplication are both 
commutative operations (m + n = n + m and m  n = n  m), the rational 
numbers constitute what is in algebra known as a eld. 
There is also a linear order dened on the rationals Q. Note that, as 
opposed to the case with the intergers, there is always another rational 
number between any two distinct rational numbers, i.e., if we have p < q, 
then there is an r 2 Q such that p < r < q. 
C.1.4 Real Numbers 
We can formulate nonlinear equations over Q, e.g., x x = 2. The fact that 
there is no rational number that solves this equation came as a shock for 
Pythagoras and his followers, a shock that they never fully recovered from. 
The proof of this is rather simple, and runs roughly as follows: The basic 
idea is that if x = 
m
n 
solves this equation, where m and n are integers with 
no common factors, then both m and n are divisible by 2, and thus do have 
a common factor. Now, any rational number can be written on the form 
x = 
m
n 
, where the integers m and n have no common factors, so we must 
conclude that there is no rational solution to this equation. 
There is however another way to escape from the set of rationals using 
the linear order, rather than addition and multiplication, namely by the 
use of limits, see Section C.2.6. Assume that an 2 Q for n = 1; 2; : : : This 
does not imply that lim 
n!1
an 2 Q. We will informally dene the set of 
real numbers as is the set of limits of sequences (an) over Q, rather than 
mucking around with Dedekind cuts, although this denition is circular, 
since the real numbers are used to dene limits. 
Like the rational numbers, the real numbers have the property that 
each of them has an additive inverse and each one of them, except 0, has a

C.1. NUMBERS 121 
multiplicative inverse, and also here, addition and multiplication are both 
commutative operations. Thus, the real numbers also constitute a eld. 
There is also a linear order dened on the real numbers, like on the 
rationals Q. Unlike the rationals, though, if an 2 R for n = 1; 2; : : :, then 
the limit lim 
n!1
an 2 R. Thus the set of real numbers is closed under limits. 
This property is referred to as constituting a complete metric space. 
Since we dened R as the set of limits of sequences in Q, there is a 
rational number arbitrarily close to any real number. This means that Q 
is dense in R. 
C.1.5 Complex Numbers 
Let x2 denote x  x, see Section C.2.1, and assume that we want to solve 
the equation x2 = 1. There is no real number that solves this equation, 
since for real numbers, x2  0. If we extend the real eld with a solution to 
this equation, denoted i, and take the algebraic closure, we get the complex 
numbers C, which also constitute a eld. 
The complex numbers are usually written on the form z = x+iy, where 
x; y 2 R. x is called the real part of z (as you might have guessed), and y 
is called the imaginary part of z. 
This extension actually allows us to solve any equation of the form 
cnzn + cn1zn1 +   + c1z + c0 = 0 with ci 2 C 
in the sense that 
8z 2 C cnzn + cn1zn1 +   + c1z + c0 = (z  z1)  : : :  (z  zn) 
Note that the roots (i.e., solutions) zi need not be distinct. 
This means that there are no natural algebraic extensions of C, unlike 
the case with N;Q and R. So from this point of view, we've reached the 
end of the line. 
However, we do not have any linear order dened on the complex numbers. 
We can nevertheless introduce a distance between any two complex 
numbers z1 = x1 + iy1 and z2 = x2 + iy2, namely the Euclidian distance 
d(z1; z2) = p(x1  x2)2 + (y1  y2)2. 
Just like for the real numbers, it is the case that any sequence z1; z2; : : : 
in C that converges (using the Euclidian distance), converges to an element 
in C, so also from this point of view C is complete (i.e., a complete metric 
space), and no extension to the complex numbers suggests itself. 
C.1.6 Algebraic and Transcendental Numbers 
Assume that all the cocients ci of the equation above are rational numbers 
(or, equivalently, integers), i.e: 
anzn + an1zn1 +   + a1z + a0 = 0 with ai 2 Q 
The solutions to these equations are known as the algebraic numbers 
over Q. Note that the rationals and thus the integers and natural numbers 
are all algebraic over Q. Other examples are p3; 12 p2 and 
1 + p5i 
2 
. 
However, not all real numbers are algebraic over Q. It is actually the 
case that the overwhelming majority of the complex and real numbers are 
not algebraic over Q. These numbers are called transcendental numbers

122 APPENDIX C. SOME CALCULUS 
(over Q). The transcendental numbers include such celebrities as , the ratio 
of the circumference and the diameter of a circle, and e, the base of the 
natural logarithm. This was rst observed by the German mathematician 
Cantor, who proved the existence of (a very large number of) transcendental 
numbers without actually constructing a single one. He did this by 
comparing the sizes of the set of algebraic numbers, which is countable, 
and the set of real numbers, which is uncountable. Thus, there must be an 
awful lot of non-algebraic, i.e., transcendental, numbers. 
For further details on the number systems, consult any introductory 
book on the topic. 
C.2 The Very Basics 
C.2.1 Exponentials 
If not indicated dierently, the following holds for a; b 6= 0, n;m 2 Z, 
r; s; p; q 2 Z+ 
an = an1  a 
a0 = 1 
an = 
1 
an 
0r = 0 
00 not dened 
0n not dened 
an  am = an+m 
a
p
q  a
r
s = a
p
q+r
s obviously q; s 6= 0 
ar 
as = ars for r; s;2 Z 
a
p
q 
a
r
s 
= a
p
q r
s obviously q; s 6= 0 
(ar)s = (as)r = ars but note that (as)r 6= a(sr) for r; s;2 Z 
(a  b)m = am  bm 
(
a
b 
)r = (
ar 
br ) for r; s;2 Z

C.2. THE VERY BASICS 123 
ar 
as = 8<: 
ars r > s 
1 r = s 
1 
asr r < s 
for r; s;2 Z 
a > 1 : r < s i ar < as 
0 < a < 1 : r < s i ar > as 
(a + b)2 = a2 + 2ab + b2 
(a + b)3 = a3 + 3a2b2 + b3 
or generally: 
(a + b)n = 
n X
k=0 n
k akbnk 
a2  b2 = (a  b)(a + b) 
a3  b3 = (a  b)(a2 + ab + b2) 
or generally: 
an  bn = (a  b)(an1 + ban2 +   + bn2 + bn1) 
C.2.2 Roots 
The following holds for a; b 2 R+0 
; n 2 Z: 
npa = b i bn = a 
1 pa = a 
np0 = 0 
np1 = 1 
( npa)n = npan = a 
a 
1n
= npa 
a
p
q = q pap for p; q 2 Z+

124 APPENDIX C. SOME CALCULUS 
(a 
1n
)n = a 
(a
p
q )q = ap for p; q 2 Z+ 
npa  npb = npab 
npa 
npb 
= nra
b 
( npa)r = npar 
( npa)r = npar 
sqqpa = qqs pa = sq pa 
q pap = qkpapk 
If you did not know it already, you might have realized from the above 
that exponentials and roots are inverses over R+0 
. 
C.2.3 The Exponential Function 
The exponential function exp(x) also known as ex is dened as follows: 
exp(x) := 
1 X
k=0 
xk 
k! 
= ex ; x 2 R 
exp(1) = 
1 X
k=0 
1k 
k! 
= e 
For the exponential function ex it holds that: 
exp(x)  exp(y) = exp(x + y) ,for all x; y 2 R 
This is called the addition theorem for exponential functions. Now 
we can derive 
exp(0) = 1 
Which followes from 
exp(x) = exp(x + 0) = exp(x)  exp(0) 
exp(x) = exp(x)  exp(0) 
exp(0) = 
exp(x) 
exp(x) 
= 1

C.2. THE VERY BASICS 125 
From this it followes that
exp(x) = 
1 
exp(x) 
We can see this from 
exp(x)  exp(x) = exp(x  x) = exp(0) = 1 
For all x 2 R, the exponential function exp(x) is positive: 
exp(x) > 0 
The exponential function exp(x) is continuous, and strictly monotonically 
increasing. 
lim 
x!1
exp(x) = 0 
lim 
x!1
exp(x) = 1 
For the exponential function exp(x) the following equations hold: 
e = exp(1) = exp(n  
1
n
) = exp( 
1
n 
+ 
1
n 
+   + 
1
n
) = (exp 
1
n
)n 
npe = nr(exp 
1
n
)n = exp( 
1
n
) = e 
1n 
e
m
n = exp(
m
n 
) = exp(m 
1
n
) = exp( 
1
n
)m = npem = 
With the knowlege on exponential functions at hand, the formulas in 
C.2.1 and C.2.2 should have become much clearer now. 
C.2.4 Logarithms for Beginners 
We dierentiate between the natural logarithm ln with base e, and the 
general logarithm loga with base a, a > 0. The natural logarithm ln is the 
inverse of the exponential function exp(x) in the interval (0;1). Thus, the 
following holds: 
ln(exp(x)) = x , for all x 2 R 
exp(ln(x)) = x , for all x 2 R+ 
y = ln(x) $ x = ey , for all x 2 R+ 
ln is continuous and strictly monotonically increasing. 
ln 1 = 0 
lim 
x!0 
ln(x) = 1

126 APPENDIX C. SOME CALCULUS 
lim 
x!1
ln(x) = 1 
The multiplication theorem states: 
ln(x  y) = ln(x) + ln(y) ; x; y > 0 
From the multiplication theorem we can derive: 
ln(xn) = n  ln(x) 
ln( npx) = 
1
n  ln(x) 
From ex and ln(x), the general exponential function ab with a > 0, 
a 6= 1, and b 2 R is dended as 
ab := exp(b  ln(a)) = ebln(a) 
Similar to ex, the inverse of the general exponential function exists, 
which is called the general logarithm loga with base a: 
loga(ax) = x , for all x 2 R 
From this it is obvious that:
loga(a) = 1 
loga(1) = 0 
We further have:
aloga(x) = x , for all a > 0 
and obviously
loga aloga(x) = loga x , for all a > 0 
y = loga(x) $ ay = x , for x > 0 
From the multiplication theorem we can derive 
loga(xy) = y  loga(x) 
loga(x1) = loga(x) 
The general logarithm loga(x) is continuous, and strictly monotonically 
increasing for a > 1, and strictly monotonically decreasing for 0 < a < 1. 
loga(x  y) = loga(x) + loga(y) 
Of course, if a = e we get the natural logarithm ln.

C.2. THE VERY BASICS 127 
For the relation between logarithms with dierent bases see the following 
equation: 
loga(y) = 
ln(b) 
ln(a)  logb(y) , for y > 0 
From the following three equations we easily understand how the minus 
comes into the formula for entropy (cf. chapter Rfentropy, p. ??). 
log 1 = 0 
log(
a
b 
) = log(a)  log(b) obviously we have 
(log(x))  (log(y)) = log(
x
y 
) 
Remember, we had 
1 
pi 
= Mpi 
possible outcomes, which we rewrote as 
log2 
1 
pi 
= log2Mpi 
which in turn equals 
log2Mpi = log2 
1 
pi 
= log21  log2pi = 0 log2pi = log2pi 
In the same chapter we also had 
pi ln pi ! 0 
This becomes clearer by considering that 
lim 
x!0 
x lnx = 0 
which followes from the known limit, that x goes faster to zero than lnx 
goes to 1. 
C.2.5 Factorial 
n! = n  (n  1)! = 1  2  3      n ; n  1 
0! = 1

128 APPENDIX C. SOME CALCULUS 
C.2.6 Sequences 
A sequence denes an ordering on the elements of a set. Consider for instance 
a sequence of real numbers, which is an enumeration of real numbers 
ordered by natural numbers. Each real number in a set of reals is related 
to a natural number. Thus, the reals are ordered according to the natural 
numbers. We have the function f : N ! R, n 2 N, with f(n) usually 
written as an. 
a1; a2; a3;   ; an;    
The element an is called the n-th element of a sequence. The set of all 
elements ai, i=1,2,3,...,n is called the range of the sequence (an). 
For sequences, we say: 
A sequence is called strictly monotonically increasing if for all n 
an < an+1 
A sequence is called monotonically increasing if for all n 
an  an+1 
A sequence is called strictly monotonically decreasing if for all n 
an > an+1 
A sequence is called monotonically decreasing if for all n 
an  an+1 
Upper and lower bounds 
An upper bound of a sequence is an element k such that 
an  k , for all n 
A lower bound of a sequence is an element k such that 
an  k , for all n 
A sequence that has an upper bound is called upwards bounded. 
A sequence that has a lower bound is called downwards bounded. 
A sequence that has an upper bound and a lower bound is called upwards 
and downwards bounded. 
The least upper bound is called the supremum. If some element in 
the sequence takes the value of the supremum, the supremum is called the 
maximum. 
Equivalently, the greatest lower bound is called the inmum. If some 
element in the sequence takes the value of the inmum, the inmum is 
called the minimum. 
Convergence 
A sequence (an) is called convergent with a limit a, a 2 R if of all " > 0 
there is an index n" such that for all indices n  n" it holds that jaanj < ".

C.2. THE VERY BASICS 129 
In other words, all but a few elements of a convergent sequence (an) lie in 
the open interval (a  "; a + "), which is called a neighbourhood of a. For 
the limit a we write: 
a = limn!1an 
Characteristics of convergent sequences: 
(i) A convergent sequence (an) has an unambiguously dened limit. 
(ii) Convergent sequences are upwards and downwards bounded. 
(iii) Given two convergent sequences (an); (bn) with limn!1an = a and 
limn!1bn = b then the sum, dierence and product are convergent, and 
the following holds: 
limn!1(an + bn) = limn!1(an) + limn!1(bn) 
limn!1(an  bn) = limn!1(an)  limn!1(bn) 
limn!1(an  bn) = limn!1(an)  limn!1(bn) 
limn!1c  an = c  limn!1(an) , for all c 2 R 
The quotient an 
bn 
with bn 6= 0 is convergent, and the following holds: 
limn!1
an 
bn 
= 
limn!1(an) 
limn!1(bn) 
In case the limit a is unknown, convergence can be decided by the 
following criteria: 
 the Cauchy criterion, and more practically relevant by 
 the following theorem: 
A monotonically increasing, upwards bounded sequence is convergent. 
A monotonically decreasing, downwards bounded sequence is convergent. 
The Cauchy criterion for sequences states: 
A sequence (an) is convergent i for all " > 0 there is a n" such that for 
all p; q  n" it holds that jap  aq j < ". 
C.2.7 Series 
From any sequence (an) we can dene a series (sn) by summing up the 
elements ai of the the sequence (an). Thus, we have 
a1 + a2 + a3 +   + an +    = 
1 X
k=1 
ak 
sn = a1 + a2 + a3 +   + an = 
n X
k=1 
ak

130 APPENDIX C. SOME CALCULUS 
We say that the series (sn) is the n-th partial sum of the sequence (an). 
Convergence 
If the series (sn) converges to s, the series is said to converge. We write 
1 X
k=1 
ak = s 
The number s is called the sum of the series or point of convergence, 
which in fact means that s is the limit of the sequence of partial sums of 
the series. 
s = limn!1sn = limn!1 
n X
k=1 
ak 
The Cauchy criterion for convergence restated for series says: 
P1
k=1 ak converges i for all " > 0 there is a n" such that for all n  n" 
and for all p 2 N; p  1 it holds that 
j 
n+p 
X k=n+1 
akj < " 
In the following, characteristics of convergent series are given: 
For two convergent series P1
k=1 ak with sum s, and P1
k=1 bk with sum 
t, the series P1
k=1 ak + bk and P1
k=1 ak  bk are also convergent, and the 
following holds: 
1 X
k=1 
ak + bk = s + t 
1 X
k=1 
ak  bk = s  t 
Given a convergent series P1
k=1 ak with sum s: 
1 X
k=1 
c  ak = c  s , for c 2 R is convergent 
For the elements ai, i = 1; 2; 3;  ; n of a convergent series (sn) the 
following holds: 
lim 
n!1
an = 0 
In other words, the elements of a convergent series (sn) form a sequence 
(an) that rapidly goes to zero. 
The series P1
k=1 ak is said to converge absolutely if the series P1
k=1 jakj converges. From this it follows that for the absolutely convergent series 
P1
k=1 ak with sum s, and P1
k=1 bk with sum t, any series of the products 
ak  bl, k; l  1 is absolutely convergent, and 
lim 
n!1 
n X
k=1 
ak 
n X
l=1 
bl = s  t

C.2. THE VERY BASICS 131 
By denition, a non-negative convergent series P1
k=1 ak, where thus 
ak > 0, is absolutely convergent. 
Theorem: If P1
k=1 ak converges absolutely, then P1
k=1 ak converges. 
In the following three sucient conditions for convergence of series are 
given: 
Comparison test 
Given two series Pn
k=1 bn and Pn
k=1 an, and 0  an  bn, 
(i) Pn
k=1 an is convergent if Pn
k=1 bn is convergent, 
(ii) P1
k=1 bk is divergent if P1
k=1 ak is divergent. 
If 
lim 
n!1 
an 
bn 
= s 6= 0 
this can be restated as 
Pn
k=1 an is convergent i Pn
k=1 bn is convergent. 
Root test 
If kpjakj  q < 1, for all k  1, then P1
k=1 ak is absolutely convergent. 
If kpjakj  q > 1, for all k  1, then P1
k=1 ak is divergent. 
If kpjakj  q = 1, for all k  1, no information on convergence can be 
derived. 
Ratio test 
If jak+1j 
jakj  q < 1, for k  k0, then P1
k=1 ak, with ak 6= 0 is absolutely 
convergent. 
If jak+1j 
jakj  q  1, for k  k0, then P1
k=1 ak, with ak 6= 0 is divergent. 
Theorem: Leibniz 
Suppose, a series P1
k=1 ak has the following characteristics, 
(i) the sequence of its elements is positive and monotonically decreasing 
ja1j  ja2j  ja3j     
(ii) negative and positive elements alternate 
a2m1  0; ;a2m  0 ;m = 1; 2; 3;   
(iii) the sequence converges to 0 
lim 
n!1
an = 0 
Then, the series P1
k=1 ak converges.

132 APPENDIX C. SOME CALCULUS 
C.3 On the Number e, the Exponential Func- 
tion and the Natural Logarithm 
The number e has received a lot of attention in Mathematical Analysis for 
several reasons. One often sees the following as the denition of e  2:72: 
e = 
1 X
n=0 
1 
n! 
e is not an integer, nor a rational number, nor, in fact, an algebraic one. 
Alternatively one sees the following limit as the denition of e: 
e = lim 
n!1
(1 + 
1
n
)n 
The exponential function is denoted ex or exp(x). It is dened on the 
set of real numbers R as 
ex = 
1 X
n=0 
xn 
n! 
In fact, it is dened in the same way also for complex numbers. The reason 
that this function has received so much attention is the fact that it is its 
own derivative, i.e: 
d 
dx
ex = ex 
Actually, Cex is the only function for which this is the case. Here C is an 
arbitrary multiplicative constant. This is true not only viewed as a realvalued 
function of a real variable, but also as a complex-valued function of 
a complex variable. 
The inverse function of ex, i.e., the logarithm with base e, is called 
the natural logarithm and \loge x" is often denoted \ln x". Extending this 
function to the complex plane is slightly more complicated, and ln x is only 
dened for positive x when viewed as a real function. 
The following integral keeps popping up when working with the Normal 
distribution: 
Z 1 
1 
et2 
dt = p 
This can most easily be seen by noting that the integral over the real plane 
R2 should be that same in both Cartesian and Polar coordinates: 
(Z 1 
1 
et2
dt)2 = Z 1 
1 Z 1 
1 
e(x2+y2)dx dy = 
Z 1 
0 Z 2 
0 
rer2 
dr d = 2[
1
2
er2 
]10
=  
For those who are a bit worried about the terms Cartesian and Polar coordinates: 
Cartesian coordinates are the usual (rectangular) coordinates of 
the real plane, often referred to as the x and y coordinates. Polar (circular) 
coordinates describe the real plane by giving the distance to the 
origin and the angel, in radians, with the x axis. For example, the point 
(1; 1) in Cartesian coordinates corresponds the the point (p2; =4) in Polar 
coordinates.

Appendix D 
Tagsets 
For the annotation of written text we distinguish two classes of tags: word 
level and phrase level tags. Word level tags represent information related to 
lexical descriptions, i.e. information related to single word forms or lemmas. 
Phrase level tags represent information related to phrasal descriptions, i.e. 
the role a lexical element plays within a phrase, and the relations that hold 
between phrases. 
D.1 Word Level Tagsets 
D.1.1 Representation of Word Level Tags 
The most wellknown species of tagsets are part-of-speech (PoS) tagsets. 
They have become popular because of the variety of recent approaches to 
PoS-tagging. PoS-tagsets ideally represent exhaustive information on syntactic 
category, morphological features, punctuation, and some other kind 
of word level information such as symbol, abbreviation, foreign material, 
brackets, quotes, etc. A tag represents a specic portion of information 
related to lexical descriptions. This may either be 
 a single dimension of linguistic information, see for instance the Constraint 
Grammar tags for syntactic category such as V,A, ADV, DET, 
N (verb, adjective, adverb, determiner, noun), p. 146, or 
 various combinations of dimensions of linguistic information, see for 
instance the Susanne wordtags for verbs where syntactic category, information 
on transitivity and inection are combined. [Tzoukermann 
et al 1995] propose tags that are composed from syntactic category 
and morphological features, e.g. VIP3S the tag representing the information: 
verb, indicative, present, third person, singular. 
Another way of combining linguistic information is by conjunction of 
tags. See for instance the Constraint Grammar tagset, where a small number 
of PoS-tags, representing syntactic category, is combined with sets of 
PoS-specic features, p. 146. The word level annotation 
$<$SVO$>$ V PRES -SG3 VFIN @+FMAINV for instance represents the following 
linguistic information: transitive (<SVO>), verb (V), present (PRES), 
non-third singular (-SG3), nite (VFIN), matrix verb (@+FMAINV). 
In some tagsets, information on word level is combined with phrase 
level information. In Constraint Grammar, for instance, phrase level information 
is annotated at word level, cf.@+FMAINV. Another example are 
133

134 APPENDIX D. TAGSETS 
the Stuttgart-Tubingen tags ADJA and ADJD representing information 
on the adjective's grammatical function, namely attributival (ADJA) vs. 
adverbial or predicative (ADJD), or the distinction between prepositions 
(APPR) and postpositions (APPO), p. 152. 
D.1.2 Mapping between Linguistic Descriptions and 
Tagsets 
Armstrong et al. ([Armstrong et al. 1995]) for instance propose mapping 
between tags and descriptions with manually dened mapping tables. 
Linguistic descriptions are envisaged to be either represented as attribute 
value pairs, e.g. cat = prep is mapped to the tag PREP, or as typed feature 
structures, such as N[num=pl] which is mapped to the tag NPL, or 
as shortcuts for feature combinations, such as BD3FP (personal pronoun, 
third person, femininum, plural) which is maped to the less informative tag 
bfp (personal pronoun, femininum, plural), or V.sg.1.pres (verb, singular, 
rst person, present) which is maped to the less informative tag VPR (verb, 
present tense). More consequently, Tzoukermann et al. ([Tzoukermann et 
al 1995]) regard tags as abbreviations of hierarchically organized features, 
with PoS as supertype. Thus, the information related to verbs is organized 
as followes: verb (PoS) is a supertype of mood, person, number, and tense. 
D.1.3 Tagsets and the Representation of Ambiguity 
A crucial feature of lexical description is ambiguity. A single word form may 
relate to various mutually exclusive portions of linguistic information. The 
English word form stress for instance is either a verb or a noun. In the verb 
reading, it relates to a nite or non-nite form, and the nite reading can be 
interpreted as either present indicative or imperative. In highly inecting 
languages, such as German, inection introduces a great amount of ambiguity. 
Therefore morhological information is largely excluded from tagsets, 
see for instance the small and the large version of Stuttgart-Tubingen tagset 
for German (STTS) [Thielen and Schiller 1995]. 
Ambiguity within tagsets is dealt with either by 
 omitting portions of highly ambiguous linguistic description, 
cf. the small Stuttgart-Tubingen tagset; 
 underspecication, 
see for instance the hierarchical organisation of the tag names, where 
the initial characters represent the most general information, typically 
the syntactic category, e.g. VB for verb in general in the Penn 
Threebank tagset, V in the Susanne tagset, and the STTS; 
 disjunctions over tags or tag combinations. 
D.1.4 Minimal Criteria for the Development of PoSTagsets 
 Tags shall be kept as theory neutral as possible, in order to guarantee 
a high degree of descriptive generality. 
 Tags should reect a clear-cut distinction between dierent dimensions 
of linguistic description, in order to allow for a clear specication

D.2. TAGSETS FOR ENGLISH TEXT CORPORA 135 
of the descriptive level, and for exible and controlled access to various 
kinds of linguistic information via the tags. 
 Tags must be dened and documented with examples in such a way 
that maximal intercoder consistency is guaranteed. Especilly problematic 
cases for annotation must be documented extensively. 
To achieve compatibility of PoS-tagsets guidelines for standardized lexical 
description have been proposed, and tools for compatibility checks 
between tagsets have been developed. For the former see for instance the 
EAGLES activities on standardization of lexical descriptions (e.g. [Monachini 
and Calzolari 1994], [Teufel 1995b]). For the later, see for instance 
[Teufel 1995a], [Armstrong et al. 1995]. 
In the following, a number of tagsets for annotation of text (written 
language) is given. From the examples, we will see that tagsets provide more 
than mere PoS, but the information actually used in statistics reduces to 
a subset of approx. 50 to 100 PoS-tags. This is because when using large 
tagsets huge training corpora are necessary, which unfortunately do not 
exist. 
D.2 Tagsets for English Text Corpora 
English is the language the largest amount of annotated text is available for. 
Thus, most of the statistics-based approaches to NLP have been developed 
on English text. 
D.2.1 The Susanne Tagset 
The Susanne tagset is a classic among tagsets. The Susanne coprus has been 
derived from the Brown corpus ([Francis & Kucera 1982]). Annotation at 
word level in the Suzanne corpus is comparable to a data record: Each record 
of a Susanne le comprises six elds: reference, status, wordtag, word, 
lemma, and parse eld. The Susanne annotation scheme distinguishes tags 
representing surface grammar (i.e. phrase structure), and tags representing 
logical grammar (i.e. argument structure). 
The reference eld contains information about which le the annotated 
word is part of, and where it occurrs in the Brown Corpus. 
The status eld indicates whether a word form is an abbreviation (A), 
a symbol (S), or a misprint (E). If none of those are applicable the status 
eld contains a hyphen. 
The wordtag eld comprises 353 distinct wordtags (PoS-tags), plus 
additional tags for idioms. It is based on the "Lancaster" tagset listed in 
[Johansson et al. 1986], and enriched by additional grammatical distinctions 
which are indicated by lower-case suxes. In the following we will 
only give a few examples on verbs, so that you get an idea of how Susanne 
wordtags look like. The gory details you'll nd in [Sampson1995].

136 APPENDIX D. TAGSETS 
Command Usage 
VB0 be 
VBDR were 
VBDZ was 
VBG being 
VBM am 
VBN been 
VBR are 
VBZ is 
VD0 do 
VDD did 
VDG doing 
VDN done 
VDZ does 
VH0 have 
VHD had (past tense nite) 
VHG having 
VHN had (past participle) 
VHZ has 
VMK ought 
VMd modal (past) 
VMo modal (present) 
VV0i intransitive verb base form 
VV0t transitive verb base form 
VV0v base form with transitive verb or intransitive verb 
VVDi intransitive verb, past tense 
VVDt transitive verb, past tense 
VVDv transitive or intransitive verb, past tense 
VVGi present participle of intransitive verb 
VVGt present participle of transitive verb 
VVGv present participle of verb having (in)transitive uses 
VVNi past participle of intransitive verb 
VVNt past participle of transitive verb 
VVNv past participle of verb having (in)transitive uses 
VVZi intransitive verb, third person singular 
VVZt transitive verb, third person singular 
VVZv transitive or intransitive verb,third person 
The word eld contains what ever is considered to be a word token. 
The lemma eld contains the base form(s) of a word token. Typographical 
variation is eliminated. 
The parse eld gives information on syntactic structure. Formtags, 
functiontags, and indices are destinguished. 
Formtags are comparable to labels assigned to the nodes in a parse 
tree. In the annotation, the parse tree is represented by labelled brackets. 
Wordlevel, phraselevel, clauselevel, and rootlevel (paragraph, heading, title 
etc.) formtags are distinguished. Wordlevel formtags (= wordtags) represent 
information related to terminal nodes(= lexical elements). Phrase level 
formtags give information on phrase types such as NP, VP, AP, AdjP etc. 
Clause level formtags give information on the clause type, such as main 
clause, adverbial clause, relative clause etc. 
While formtags address surface grammatical properties, functiontags 
represent properties of logical grammar. Functiontags indicate the gram

D.2. TAGSETS FOR ENGLISH TEXT CORPORA 137 
matical function of complements, such as subject, direct object, indirect 
object, prepositional object, and adjuncts such as place, direction, time, 
manner. 
Indices specify the relation between surface and deep structure elements. 
To give a avour of the information encoded, the full list of rootlevel, 
clauselevel, and phraselevel formtags, and functiontags is presented below. 
Comments of the author are surrounded by brackets. 
Rootlevel Formtags 
O paragraph 
Oh heading 
Ot title (e.g. of book) 
Q quotation 
I interpolation (material inserted into a grammatically 
and semantically complete structure) 
Iq tag question 
Iu scientific citation 
Clauselevel Formtags 
S main clause 
Ss quoting clause embedded within quotation 
Fa adverbial clause 
Fn nominal clause 
Fr relative clause 
Ff "fused" relative (relative clause lacking an explicit 
antecedent, and functioning as a nominal 
element) 
Fc comparative clause 
Tg present participle clause 
Ti infinitival clause 
Tn past participle clause 
Tf "for-to" clause 
Tb "bare" nonfinite clause 
Tq infinitival relative clause 
Z reduced ("whiz-deleted") relative clause (relative clause 
where relative pronoun and finite verb have been omitted) 
L other verbless clause 
A special "as" clause (clause beginning with the subordinating 
conjunction or comparative preposition 'as', and not 
being last part of an as-comparison, and 'as' is not 
interpretavble as 'because', 'while', or 'when'. 
W "with" clause 
Phraselevel Formtags 
N noun phrase 
V verb group 
J adjective phrase 
R adverb phrase 
P prepositional phrase

138 APPENDIX D. TAGSETS 
D determiner phrase 
M numeral phrase 
G genitive phrase 
Vo operator section of verb group 
(i.e. the first part of a split 
verb group, e.g. in the case of 
subject-auxiliary inversion) 
Vr remainder of verb group from which Vo has been separated (ie. 
the second part of a split verb group) 
Vm V beginning with "am" 
Va V beginning with "are" 
Vs V beginning with "was" 
Vz V beginning with other 3rd-singular verb 
Vw V beginning with "were" 
Vj V beginning with "be" 
Vd V beginning with past tense 
Vi infinitival V 
Vg V beginning with present participle 
Vn V beginning with past participle 
Vc V beginning with modal 
Vk V containing emphatic DO 
Ve negative V 
Vf perfective V 
Vu progressive V 
Vp passive V 
Vb V ending with BE 
Vx V lacking main verb 
Vt catenative V 
Nq "wh-" N 
Nv "wh...ever" N (e.g. 'whoever', 'whichever direction') 
Ne "I/me" head 
Ny "you" head 
Ni "it" head 
Nj adjective head 
Nn proper name 
Nu unit noun head 
Na marked as subject 
No marked as nonsubject 
Ns singular N 
Np plural N 
Jq "wh-" J (e.g. 'how good') 
Jv "wh...ever" J (e.g. 'however bad', 'no matter how high') 
Jx measured absolute J (e.g. '10 cm long') 
Jr measured comparative J (e.g. 'almost two years later than ...') 
Jh postmodified J (e.g. 'long enough', 'analoguous to ...') 
Rq "wh-" R 
Rv "wh...ever" R 
Rx measured absolute R 
Rr measured comparative R 
Rs adverb conducive to asyndeton (a phrasal 'as', 'then',

D.2. TAGSETS FOR ENGLISH TEXT CORPORA 139 
'otherwise yet') 
Rw quasi-nominal adverb (phrase headed by one of the adverbs 
'here', 'there', 'now', 'then', 'anywhere', 
'elsewhere' , 'somewhere', 'nowhere') 
Po "of" phrase 
Pb "by" phrase 
Pq "wh-" P (e.g. 'after how many generations', 'of which') 
Pv "wh...ever" P (e.g. 'in whichever direction') 
Dq "wh-" D (e.g. 'how many', 'each of which') 
Dv "wh...ever" D (e.g. 'however much') 
Ds singular D (e.g. 'however much' Dvs) 
Dp plural D (e.g. 'how many' Dqp) 
Ms M headed by a cardinal number (e.g. 'several thousand', 
'1 billion or more') 
? interrogative clause 
* imperative clause 
% subjunctive clause 
! exclamatory clause or item 
" vocative item 
+ subordinate conjunct introduced by conjunction 
- subordinate conjunct not introduced by conjunction 
@ appositional element 
& co-ordinate structure acting as first conjunct within a 
higher co-ordination (marked in certain cases only) 
WT& co-ordination of words 
WT+ conjunct within wordlevel co-ordination that is introduced 
by a conjunction 
WT- conjunct within wordlevel co-ordination not introduced by 
a conjunction 
Complement Functiontags 
s logical subject 
o logical direct object 
S surface (and not logical) subject 
O surface (and not logical) direct object 
i indirect object 
u prepositional object 
e predicate complement of subject (e.g. He is not interested in 
being named 'a full-time director'.) 
j predicate complement of object (e.g. he established himself 
'as one of the guiding spirits') 
a agent of passive (i.e. by-object) 
n particle of phrasal verb (e.g. They slled 'up' the dean.) 
z complement of catenative 
x relative clause having higher clause as antecedent (e.g. He 
left early, 'which surprised me'.)

140 APPENDIX D. TAGSETS 
G "guest" having no grammatical role within its tagma 
Adjunct Functiontags 
p place 
q direction 
t time 
h manner or degree 
m modality 
c contingency 
r respect (phrases beginning with 'in connection with', 
'with respect to', etc.) 
w comitative (with-phrases meaning ``together_with'') 
k benefactive (e.g. she gave me a scarf 'for her son') 
b absolute (verbless adverbial clauses) 
Susanne Annotation 
A01:0010a - YB $<$minbrk$>$ - [Oh.Oh] 
A01:0010b - AT The the [O[S[Nns:s. 
A01:0010c - NP1s Fulton Fulton [Nns. 
A01:0010d - NNL1cb County county .Nns] 
A01:0010e - JJ Grand grand . 
A01:0010f - NN1c Jury jury .Nns:s] 
A01:0010g - VVDv said say [Vd.Vd] 
A01:0010h - NPD1 Friday Friday [Nns:t.Nns:t] 
A01:0010i - AT1 an an [Fn:o[Ns:s. 
A01:0010j - NN1n investigation investigation . 
A01:0020a - IO of of [Po. 
A01:0020b - NP1t Atlanta Atlanta [Ns[G[Nns.Nns] 
A01:0020c - GG +$<$apos$>$s - .G] 
A01:0020d - JJ recent recent . 
A01:0020e - JJ primary primary . 
A01:0020f - NN1n election election .Ns]Po]Ns:s] 
A01:0020g - VVDv produced produce [Vd.Vd] 
A01:0020h - YIL $<$ldquo$>$ - . 
A01:0020i - ATn +no no [Ns:o. 
A01:0020j - NN1u evidence evidence . 
A01:0020k - YIR +$<$rdquo$>$ - . 
A01:0020m - CST that that [Fn. 
A01:0030a - DDy any any [Np:s. 
A01:0030b - NN2 irregularities irregularity .Np:s] 
A01:0030c - VVDv took take [Vd.Vd] 
A01:0030d - NNL1c place place [Ns:o.Ns:o]Fn]Ns:o]Fn:o]S] 
A01:0030e - YF +. - .O] 
Literature 
An exhaustive description of the Susanne tagset is presented in [Sampson1995]. 
The Susanne corpus is freely available.

D.2. TAGSETS FOR ENGLISH TEXT CORPORA 141 
D.2.2 The Penn Treebank 
The second version of the PennTreebank (Treebank II) comprises a million 
words of 1989 Wall Street Journal material. The corpus is annotated 
with PoS, and a labelled bracket structure that allows for the extraction 
of predicate-argument structure. The PoS labels , as presented in [Santorini 
1995], are listed in section D.2.2. The bracket labels and syntactic 
functions are presented in section D.2.2. Labeling examples you'll nd in 
section D.2.2. The list of labels, and the examples are taken from [Bies et 
al. 1995].

142 APPENDIX D. TAGSETS 
The Penn Treebank Parts-of-Speech 
Command Usage 
CC coordinating conjunction 
CD cardinal number 
DT determiner 
EX existential there 
FW foreign word 
IN preposition or subordinating conjunction 
JJ adjective 
JJR comparative adjective 
JJS superlative adjective 
LS list item marker 
MD modal 
NN noun, singular or mass 
NNS noun, plural 
NNP proper noun, singular 
NNPS proper noun, plural 
PDT predeterminer 
POS possessive ending 
PRP personal pronoun 
PP$ possessive pronoun 
RB adverb 
RBR comparative adverb 
RBS superlative adverb 
RP particle 
SYM symbol 
TO innitiv marker to 
UH interjection 
VB verb, base form 
VBD verb, past tense 
VBG verb, gerund or present participle 
VBN verb, past participle 
VBP verb, non-3rd person singular, present 
VBZ verb, 3rd person singular, present 
WDT wh-determiner 
WP wh-pronoun 
WP$ possessive wh-pronoun 
WRB wh-adverb 
# Pound sign 
$ Dollar sign 
. sentence nal punctuation 
, comma 
: colon, semi-colon 
( left bracket character 
) right bracket character 
" straight double quote 
' left open single quote

D.2. TAGSETS FOR ENGLISH TEXT CORPORA 143 
Command Usage 
\ left open double quote 
' right close single quote 
" right close double quote 
The Penn Treebank II Syntactic Structure 
Command Usage 
I Bracket Labels 
I.I Clause-Level Labels 
S simple declarative sentence 
SINV subject-auxiliary inversion (not used with questions) 
SBAR relative or subordinate clause, incl. indirect questions 
SBARQ wh-question 
SQ within SBARQ (SBARQ consists of wh-element and SQ), 
labels yes/no question, and tag question 
S-CLF it-cleft, e.g. it was Casey who threw the ball 
SQ-CLF interrogative it-cleft, e.g. was it Casey who threw the ball 
I.II Phrase-Level Labels 
RRC reduced relative clause, complementizer 
and nite verb are missing, 
e.g. an orangutan 'foaming at the mouth', 
titles 'not presently in the collection' 
FRAG clause fragment 
VP verb phrase 
NP noun phrase 
ADJP adjective phrase 
PP prepositional phrase 
ADVP adverbial phrase 
WHNP wh-noun phrase, e.g. who 
WHADVP wh-adverbial phrase, e.g. why 
WHADJP wh-adjectival phrase,e .g. how cold 
WHPP wh-prepositional phrase, e.g. on what 
QP quantier phrase 
PRT particle, i.e. separated verb prex 
UCP unlike coordinated phrase 
PRN parenthetical 
NX head of a complex noun phrase 
NAC not a constituent; 
to show scope of certain prenominal modiers 
in a noun phrase 
INTJ interjection, e.g. Mike, would you 'please' close the door 
CONJP conjunction phrase, only used with 
adjacent multi-element conjunctions, 
e.g. (CONJP not only) ... (CONJP but rather) 
X unknown, uncertain, unbracketable 
II Function Tags 
II.I Text Categories

144 APPENDIX D. TAGSETS 
Command Usage 
-HLN headlines, datelines 
-TTL titles 
-LST list markers, i.e. mark list items in a text 
II.II Grammatical Functions 
-CLF true clefts, see S-CLF, and SQ-CLF above 
-NOM non-NP functioning as NP, 
e.g. I do not mind about 'your' leaving early, 
'what' I really like is chocolat 
-ADV clausal, and nominal adverbials, e.g. a little bit, 
you can leave 'if you really want to go' 
-LGS logical subjects in passives 
-PRD non-VP predicates, i.e. after copula verbs or in small claus 
e.g. Mary considers 'Peter a fool' (small clause) 
-SBJ surface subject 
-TPC topicalized, fronted constituent 
-CLR closely related, i.e. constituents 
between complement and adjunct, 
such as constituents closely related to the verb, 
elements in xed phrases, 
elements in support verb constructions 
-DTV dative PP-object; only with verbs that undergo dative shift 
e.g. I baked a cake for John, I baked John a cake 
II.III Semantic Roles 
-VOC vocative, 'Mike', would you please close the door 
-DIR direction, trajectory, e.g. 'from' Tokyo 'to' New York 
-LOC location 
-MNR manner 
-PRP purpose, reason 
-TMP temporal phrases 
-BNF benefactive; only with verbs that undergo dative shift 
-PUT locative complement of the verb 'put' 
-EXT extent, spatial extent of an activity, e.g. she walked '5 mile 
III Null Elements 
*T* used for wh-movement, relative clauses, tough movement, 
parasitic gaps, and topicalization 
(NP *) used in passive constructions, control and raising, 
reduced relative clauses, 
participal clauses and gerunds, imperatives, and innitives 
0 the null complementizer, used in SBAR if wh-element or 
that is missing

D.2. TAGSETS FOR ENGLISH TEXT CORPORA 145 
Command Usage 
*U* unit, marks interpreted position of a unit symbol 
*?* placeholder for ellipsed material 
*NOT* anti-placeholder in template gapping, 
used when template and copy 
in a coordinated structure are not entirely parallel 
Pseudo-Attachment to show attachment between non-adjacent constituents 
*ICH* interprete constituent here, used for discontinuous dependency 
*PPA* permanent predictable ambiguity, 
e.g. for structurally unresolvable pp-attachment 
*RNR* right node raising, indicating shared constituents 
*EXP* expletive 
IV Coindexing 
identity index integer following a label tag, e.g. (SBAR-1) 
reference index integer following a null element, e.g. (NP *T*-3) 
the indices of the null element and the related constituent are identical; 
null element and related element must appear within the same sentence 
Bracketed Structures 
(S (NP-SBJ Casey) 
(VP will 
(VP throw 
(NP the ball)))) 
(S (NP-SBJ-1 the guide) 
(VP was 
(VP given 
(NP *-1) 
(PP-DTV to 
(NP Arthur) 
(PP by 
(NP_LGS Ford)))))) 
(S (NP-SBJ (NP the person) 
(SBAR (WHNP-1 who) 
(S (NP-SBJ *T*-1) 
(VP threw 
(NP the ball))))) 
(VP is 
(ADJP_PRD very athletic))) 
Literature 
For the description of the predicate-argument structure see [Marcus et 
al. 1994], for the example annotations and all other details see [Bies

146 APPENDIX D. TAGSETS 
et al. 1995]. More information on the Treebank II can be found via 
http:/www.ldc.upenn.edu 
D.2.3 The Constraint Grammar Tagset 
The Constraint Grammar tagset comprises PoS-tags , and structural tags. 
PoS-tags give information on syntactic category and on morphosyntactic 
features related to specic syntactic categories. Additionally there are some 
other tags such as $2-NL representing a sequence of two or more newlines, 
and '*' indicating an upper case in words. Structural tags represent the 
syntactic function of a word, such as subject, object etc. The crucial thing 
to know is that in Constraint Grammar all information, even structural 
one, is expressed at word level.

D.2. TAGSETS FOR ENGLISH TEXT CORPORA 147 
The Constraint Grammar PoS-Tagset 
Command Usage 
A adjective 
ABBR abbreviation 
ADV adverb 
CC coordinating conjunction 
CS subordinating conjunction 
DET determiner 
INFMARK> innitive marker such as 'to' 
INTERJ interjection 
N noun 
NEG-PART negative particle such as 'not' 
NUM numeral 
PCP1 participial with -ing form 
PCP2 participial with -ed/-en form 
PREP preposition 
PRON pronoun 
V verb 
Features for abbreviations 
<Title> title such as '*dr' for 'Doctor' 
GEN genitive 
NOM nominative 
PL plural 
SG singular 
SG/PL singular or plural 
Features for adjectives 
<Attr> attributive 
<DER:al> derived adjective in -al 
<DER:ble> derived adjective in -ble 
<DER:ic> derived adjective in -ic 
<DER:ive> derived adjective in -ive 
<DER:less> derived adjective in -less 
<DER:like> derived adjective in -like 
<DER:ward> derived adjective in -ward 
<DER:wise> derived adjective in -wise 
<Nominal> likely NP head 
<Pred> predicative 
ABS absolute form 
CMP comparative form 
SUP superlative form 
Features for adverbs 
<**CLB> clause boundary as it is introduced by the adverb why 
<DER:bly> derived adverb in -bly 
<DER:ed> derived adverb in -ed 
<DER:ing> derived adverb in -ing 
<DER:ly> derived adverb in -ly 
<DER:ward> derived adverb in -ward 
<DER:wards> derived adverb in -wards

148 APPENDIX D. TAGSETS 
Command Usage 
<DER:wise> derived adverb in -wise 
ABS absolute form 
CMP comparative form 
SUP superlative form 
WH wh-adverb 
Features for determiners 
<**CLB> clause boundary as it is introduced by which 
<Def> denite 
<Genord> general ordinal such as next 
<Indef> indenite such as an 
<Quant> quantier such as some 
ABS absolute form such as much 
ART article 
CENTRAL central determiner 
CMP comparative form 
DEM demonstrative determiner 
GEN genitive 
NEG negative form 
PL plural as required from few 
POST postdeterminer e.g. much 
PRE predeterminer e.g. all 
SG singular 
SG/PL singular or plural 
SUP superlative form 
WH wh-determiner such as whose 
Features for nouns 
<DER:bility> derived noun in -bility 
<DER:ble> derived noun in -ble 
<DER:er> derived noun in -er 
<DER:ing> derived noun in -ing 
<DER:ness> derived noun in -ness 
<DER:or> derived noun in -or 
<DER:ship> derived noun in -ship 
<NRare> word only rarely used as a noun 
<Proper> proper 
<-Indef> noun with no indenite article such as furniture 
<Title> title e.g. *professor 
GEN genitive case 
NOM nominative case 
PL plural 
SG singular 
SG/PL singular or plural 
Features for numerals 
<Fraction> fraction e.g. two-thirds 
CARD cardinal numeral 
ORD ordinal numeral 
SG singular e.g. one-eighth

D.2. TAGSETS FOR ENGLISH TEXT CORPORA 149 
Command Usage 
PL plural e.g. three-eighths 
Features for pronouns 
<**CLB> clause boundary e.g. introduced to who 
<Comp-Pron> compound pronoun e.g. something 
<Generic> generic pronoun e.g. one's 
<Interr> interrogative 
<NonMod> pronoun with no DET or premodier 
<Quant> quantitative pronoun 
<Re> reexive pronoun 
<Rel> relative pronoun 
ABS absolute form 
ACC accusative (objective) case 
CMP comparative form 
DEM demonstrative pronoun 
FEM feminine 
GEN genitive 
INDEP independent genitive form 
MASC masculine 
NEG negative form such as none 
NOM nominative 
PERS personal pronoun 
PL plural 
PL1 1st person plural 
PL2 2nd person plural 
PL3 3rd person plural 
RECIPR reciprocal pronoun e.g. each other 
SG singular 
SG/PL singular or plural 
SG1 1st person singular 
SG2 2nd person singular 
SG2/PL2 2nd person singular or plural 
SG3 3rd person singular 
SUP superlative form 
WH wh-pronoun 
Features for prepositions 
<CompPP> multi-word preposition e.g. in spite of 
Features for verbs 
<Arch> archaic form 
<DER:ate> derived verb in -ate 
<Rare> word only rarely used as a verb 
<Vcog> verb that takes a that-clause 
<SV> intransitive 
<SVO> monotransitive 
<SVOO> ditransitive 
<SVC/A> copular with adjective complement 
<SVC/N> copular with noun complement such as become 
<down/SVC/A> copular with A, phrasal verb e.g. fall down

150 APPENDIX D. TAGSETS 
Command Usage 
<out/SVC/A> copular with A, phrasal verb e.g. turn out 
<out/SVC/N> copular with N, phrasal verb e.g. turn out 
<up/SVC/A> copular with A, phrasal verb e.g. stand up 
<up/SVC/N> copular with N, phrasal verb e.g. end up 
<SVOC/A> complex trans. with adjective complement 
<SVOC/N> complex trans. with noun complement 
<as/SVOC/A> complex trans. with A, prepositional verb 
<for/SVOC/A> complex trans. with A, prepositional verb 
<into/SVOC/A> complex trans. with A, prepositional verb 
-SG1,3 other than 1st or 3rd person sg. 
-SG3 other than 3rd person sg. 
AUXMOD modal auxiliary 
IMP imperative 
INF innitive 
NEG negative 
PAST past tense 
PRES present tense 
SG1 1st person sg. 
SG1,3 1st or 3rd person sg. 
SG2 2nd person sg. 
SG3 3rd person sg. 
SUBJUNCTIVE subjunctive 
VFIN nite form

D.2. TAGSETS FOR ENGLISH TEXT CORPORA 151 
Constraint Grammar Syntactic Tags 
Command Usage 
@+FAUXV Finite Auxiliary Predicator 
@-FAUXV Nonnite Auxiliary Predicator 
@+FMAINV Finite Main Predicator 
@-FMAINV Nonnite Main Predicator 
@NPHR Stray NP 
@SUBJ Subject 
@F-SUBJ Formal Subject 
@OBJ Object 
@I-OBJ Indirect Object 
@PCOMPL-S Subject Complement 
@PCOMPL-O Object Complement 
@ADVL Adverbial 
@APP Apposition 
@N Title 
@DN> Determiner 
@NN> Premodifying Noun 
@AN> Premodifying Adjective 
@QN> Premodifying Quantier 
@GN> Premodifying Genitive 
@AD-A> Premodifying Ad-Adjective 
@<NOM-OF Postmodifying Of 
@<NOM-FMAINV Postmodifying Nonnite Verb 
@<AD-A Postmodifying Ad-Adjective 
@<NOM Other Postmodier 
@INFMARK> Innitive Marker 
@<P-FMAINV Nonnite Verb as Complement of Preposition 
@<P Other Complement of Preposition 
@CC Coordinator 
@CS Subordinator 
@O-ADVL Object Adverbial 
@NEG Negative particle "not" 
@DUMMY A word without a syntactic function, 
*the = The 
Sample Constraint Grammar Analysis 
"<*i>" 
"i" <*> <NonMod> PRON PERS NOM SG1 SUBJ @SUBJ 
"<see>" 
"see" <as/SVOC/A> <SVO> <SV> <InfComp> V PRES -SG3 VFIN @+FMAINV 
"<a>" 
"a" <Indef> DET CENTRAL ART SG @DN> 
"<bird>"
"bird" N NOM SG @OBJ 
"<$.>" 
Literature 
For a comprehensive introduction to ENGCG see [Karlsson et al 1995]. The 
material can be obtained by sending an emty email to engcgi@ling.helsinki..

152 APPENDIX D. TAGSETS 
D.3 Tagsets for German Text Corpora 
D.3.1 The Stuttgart-Tubingen Tagset 
The Stuttgart-Tubingen tagset is a mere PoS-tagset. It comes in two versions, 
a small and a large one. In the large tagset PoS information is 
augmented by morphlogical information such as features for person, number, 
gender, case, tense, mode, comparation, and inection type. In the 
following the inventary of the small version of the tagset is presented:

D.3. TAGSETS FOR GERMAN TEXT CORPORA 153 
Command Usage 
ADJA attributival adjektiv 
ADJD adverbial or predicative adjective 
ADV adverb 
APPR preposition ; circumposition left 
APPRART preposition with incorporated article 
APPO postposition 
APZR circumposition right 
ART denite or indenite article 
CARD cardinal 
FM foreign material 
ITJ interjection 
KOUI subordinating conjunction with zu and innitive 
KOUS subordinating conjunction with sentence 
KON coordinating conjunction 
KOKOM comaration particle, without sentence 
NN common noun 
NE proper noun Hans*Hamburg*HSV 
PDS substitutive demonstrative pronoun 
PDAT attributive demonstrative pronoun 
PIS substitutive indenit pronoun 
PIAT attributive indenit pronoun 
PIDAT attributive indenit pronoun mit Determiner 
PPER irreexive personal pronoun 
PPOSS substitutive possessive pronoun 
PPOSAT attributive possessive pronoun 
PRELS substitutive relative pronoun 
PRELAT attributive relative pronoun 
PRF reexive personal pronoun 
PWS substitutive interrogativpronoun 
PWAT attributive interrogativpronoun 
PWAV adverbial interrogative- or relative pronoun 
PROAV pronominal adverb 
PTKZU "zu" before innitive 
PTKNEG negation partikel 
PTKVZ abgetrennter verb prex 
PTKANT Antwortpartikel 
PTKA partikel occurring with adjective or adverb 
TRUNC lexical ellipsis 
VVFIN nite verb, main 
VVIMP imperative, main 
VVINF innitive, main 
VVIZU innitive with "zu", main 
VVPP past partiziple, main 
VAFIN nite verb, aux 
VAIMP imperative, aux 
VAINF innitive, aux

154 APPENDIX D. TAGSETS 
Command Usage 
VAPP past artiziple, aux 
VMFIN nite verb, modal 
VMINF innitive, modal 
VMPP past partiziple, modal 
XY non-word, Sonderzeichen 
$, comma 
$. sentence nal punctation 
$( sentence internal punctuation 
Annotation Example 
M"ogen VMFIN 
Puristen NN 
aller PIDAT 
Musikbereiche NN 
auch ADV 
die ART 
Nase NN 
r"umpfen VVINF 
, $, 
die ART 
Zukunft NN 
der ART 
Musik NN 
liegt VVFIN 
f"ur APPR 
viele PIDAT 
junge ADJA 
Komponisten NN 
im APPRART 
Crossover-Stil NN 
. $. 
Sie PPER 
gehen VVFIN 
gewagte ADJA 
Verbindungen NN 
und KON 
Risiken NN 
ein PTKVZ 
, $, 
versuchen VVFIN 
ihre PPOSAT 
M"oglichkeiten NN 
auszureizen VVIZU 
. $. 
Literature 
[Thielen and Schiller 1995]

Appendix E 
Optimization Theory, 
Wild-West Style 
E.1 Introduction 
Here we will discuss constrained optimization using Lagrange multiplyers. 
This requires that we rst go through the following necessary prerequisites: 
 R, the set of real numbers: 0;1; 42; 1
7;p2; e; ; : : : 
 Intervals: [1; 0]; ( 1
7;p2); [;1); : : : 
 Sequences and limits in R: e = limn!1(1 + 1n 
)n. 
 Rn = 
n factors 
z }| { R   R. 
 Functions f(x) from R to R (f : R 7! R): x+ 5; x2; sin x; ex. 
 Sequences and limits of functions in R 7! R: limx!x0 f(x). 
 Derivatives: df(x) 
dx = limh!0 
f(x+h)f(x) 
h . 
 Functions f(x1; : : :; xn) from Rn to R (f : Rn 7! R): x2 + y2 + z2. 
 Partial derivatives: @f(x1;:::;xn) 
@xi 
. Gradients: rf = ( @f 
@x1 
; : : : ; @f 
@xn 
). 
 Optimization in one dimension: 
max 
x2AR 
f(x) 
 The solutions: Stationary points and boundary points. 
 The zeros of the derivative: df 
dx = 0. 
 Optimization in several dimensions: 
max 
(x1;:::;xn)2ARn 
f(x1; : : :; xn) 
 The zeros of the partial derivatives: 8i 
@f 
@xi 
= 0. Or, equivalently, the 
zeros of the gradient: rf = 0 = (0; : : : ; 0). 
155

156 APPENDIX E. OPTIMIZATION THEORY, WILD-WEST STYLE 
 Constrained optimization in several dimensions: 
max 
(x1;:::;xn)2ARn 
f(x1; : : :; xn) 
gk(x1; : : :; xn) = 0 for k = 1; : : :;m 
 Lagrange multiplyers: r[f(x1; : : :; xn)Pm
k=1 kgk(x1; : : :; xn)] = 0. 
In addition to this, we need to recapitulate numerical analysis to see how 
to solve the resulting equations. Since this is clearly impossible within the 
given time, we will start from the end and work our way backwards. 
E.2 Constrained Optimization in Rn 
The basic problem that we want to solve is the following: 
max 
(x1;:::;xn)2ARn 
f(x1; : : :; xn) 
gk(x1; : : :; xn) = 0 for k = 1; : : :;m 
This means that we want to maximize the real-valued object function f of 
n real variables x1; : : :; xn where the vector x = (x1; : : :; xn) is restricted 
to the subset A of Rn, while observing m constraints of the form 
gk(x1; : : :; xn) = 0. 
Example: A typical example would be estimating the parameters 
vj; pjk; ajl 2 [0; 1] : j; k = 1; : : :;N ; l = 1; : : :;M of a 
hidden Markov model with N states and M signals. 
max 
(v1;:::;vN;p11;:::;pNN;a11;:::;aNM)2[0;1]n 
P(O) 
N X
j=1 
vj = 1 
N X
k=1 
pjk = 1 for j = 1; : : :;N 
M X
k=1 
ajl = 1 for j = 1; : : :;N 
Here the probability P(O) of the observed training data O is 
the object function, and the requirement that the various sets 
of probabilities sum to 1 constitutes the set of constraints. We 
have n = N +N N +N M = N(N +M +1) variables xi and 
m = 1+N + N = 2N + 1 constraints. 
Let us simplify this problem slightly to the case where we we have 
no constraints, i.e., where m = 0. As we shall soon see, the points of 
interest would then under suitable conditions be those where rf(x) = 0. 
Intuitively, this can be understood as follows: rf(x) is a vector in Rn 
indicating 
1. the direction in point x in which f(x) grows the most, 
2. and how fast it grows in this direction.

E.2. CONSTRAINED OPTIMIZATION IN RN 157 
If x is a maximum, f(x) doesn't grow at all in any direction, and rf(x) 
should thus be zero. Unfortunately, this is also true for a minimum, and 
for other so-called stationary points. In addition to this, f(x) may not be 
particularly well-behaved, and may not allow us to form rf(x), i.e., this 
quantity may be undened. We'll get back to this later. For the time being, 
maximizing f(x) will involve inspecting the points where rf(x) = 0. 
We have however not taken the constraints gk(x) = 0 into account. A 
clever way of doing this, due to the famous French mathematician Joseph 
Louis Lagrange (1736{1813), is to introduce so-called Lagrange multiplyers. 
Note that 
 maxx2ARn f(x) 
gk(x) = 0 for k = 1; : : :;m ,  maxx2ARn [f(x) Pm
k=1 kgk(x)] 
gk(x) = 0 for k = 1; : : :;m  
for any choice of real numbers k since gk(x) = 0. It turns out that the 
points of interest when solving the constrained optimizations problem are, 
again under suitable conditions, precisely the points where 
 r[f(x) Pm
k=1 kgk(x)] = 0 
gk(x) = 0 for k = 1; : : :;m ,  rf(x) = Pm
k=1 krgk(x) 
gk(x) = 0 for k = 1; : : :;m  
The parameters 1; : : :; m are determined by the new set of equations. 
Compared with the unconstrained optimization problem, we have increased 
the set of unknown variables from n to n + m by introducing these parameters, 
but we have also added m equations fgk(x) = 0 : k = 1; : : :;mg. 
We have thus given the system m extra degrees of freedom, but also added 
m extra constraints, a net change of zero degrees of freedom. 
Example: In the example of estimating the parameters vj; pjk; ajl 2 [0; 1] : j; k = 1; : : :;N ; l = 1; : : :;M of a hidden Markov model 
with N states and M signals, this amounts to the following 
set of equations (P stands for P(O)): 
@P 
@vj 
= 1 for j = 1; : : :;N 
@P 
@pjk 
= j+1 for j = 1; : : :;N ; k = 1; : : :;N 
@P 
@ajl 
= N+j+1 for j = 1; : : :;N ; l = 1; : : :;M 
N X
j=1 
vj = 1 
N X
k=1 
pjk = 1 for j = 1; : : :;N 
M X
l=1 
ajl = 1 for j = 1; : : :;N 
By multiplying each equation by the appropriate parameter, 
and summing over the appropriate index (j in the rst set of 
equations, k in the second one, and l in the third one), we can 
nd the values of 1; : : :; 2n+1 that satisfy the constraints: 
N X
j=1 
vj 
@P 
@vj 
= 1

158 APPENDIX E. OPTIMIZATION THEORY, WILD-WEST STYLE 
N X
k=1 
pjk 
@P 
@pjk 
= j+1 for j = 1; : : :;N 
M X
l=1 
ajl 
@P 
@ajl 
= N+j+1 for j = 1; : : :;N 
Thus 
vj = 
vj 
@P 
@vj 
PN
j=1 vj 
@P 
@vj 
for j = 1; : : :;N 
pjk = 
pjk 
@P 
@pjk 
PN
k=1 pjk 
@P 
@pjk 
for j = 1; : : :;N ; k = 1; : : :;N 
ajl = 
ajl 
@P 
@ajl 
PM
l=1 ajl 
@P 
@ajl 
for j = 1; : : :;N ; l = 1; : : :;M 
It is not easy to prove that the points satisfying this new set of equations 
are exactly the set of interesting points for the constrained optimization 
problem, and we will refrain from attempting the proof. We instead oer 
the following intuitive explanation, based on reasoning about surfaces and 
curves in Rn: 
rf(x), rf for short, is a normal to the surface fx 2 Rn : f(x) = Cg, 
f = C for short. Similarly, rgk is a normal to the surface gk = 0. Any 
vector normal to the intersection curve of the surfaces g1 = 0 and g2 = 0 
can be written as 1rg1 + 2rg2, where 1 and 2 are real numbers. 
Generalizing, any vector normal to the intersection curve of the surfaces 
fgk = 0 : k = 1; : : :;mg can be written as Pm
k=1 krgk, where 1; : : :; m 
are real numbers. 
Now comes the crucial bit: As we vary C, the surface f = C will hopefully 
cut the intersection curve of the surfaces fgk = 0 : k = 1; : : :;mg at some point(s) for some value(s) of C. Otherwise, the constrained optimization 
problem has no solutions. We want to maximize C. This will 
under suitable conditions happen when the surface f = C just barely touches 
this intersection curve. This in turn will happen when a normal of 
the surface f = C is also a normal to the intersection curve of the surfaces 
fgk = 0 : k = 1; : : :;mg. This means that rf is a normal both to 
the surface f = C and to the intersection curve. Since any normal to the 
intersection curve can be written as Pm
k=1 krgk, this will happen when 
rf = Pm
k=1 krgk. 
E.3 Numerical Analysis 
This is not the only thing there is to know about numerical analysis. 
Assume that we wish to calculate p2, but that some nerd has knicked 
our belt-strapped scientic calculator, and left us with a useless economic 
calculator which only lends itself to the four basic arithmetic operations 
addition, subtraction, multiplication and division (+;;, and ), and, 
most importantly, percentage calculations (%). We then need to calculate 
a numerical approximation of p2 using only these four basic arithmetic 
operations.

E.3. NUMERICAL ANALYSIS 159 
We realize that x = p2 must satisfy 
x2 = 2 and x > 0: (E.1) 
and that this uniquely determines x. 
The basic idea is to come up with a recurrence equation xk+1 = f(xk) 
that only involves addition, subtraction, multiplication and division, and 
that will allow us to calculate a better next estimate xk+1 of x from the 
current one xk. 
A rst attempt might be to note that 
x = 
2
x 
and simply let f(x) = 2=x. Thus 
xk+1 = 
2 
xk 
Unfortunately, we see that 
xk+1 = 
2 
xk 
= 
2
2 
xk1 
= xk1 
Thus, we won't get any closer to the value p2 this way. By instead adding 
x to both sides of Equation E.1, we nd that 
x2 + x = 2+x or x(x+ 1) = 2+x or x = 
2 + x 
x + 1 
or x = 1+ 
1 
x + 1 
Thus 
xk+1 = 1 + 
1 
xk + 1 
Let x1 = 1 and note that p2  1:41421. 
x1 = 1 = 1:00000 
x2 = 32 
= 1:50000 
x3 = 7
5 = 1:40000 
x4 = 17 
12  1:41667 
x5 = 41 
29  1:41379 
x6 = 99 
70  1:41428 
We see to our joy that this is rapidly approaching p2. 
But why is this? We have that 
j x2
k+1  2 j = j (
2 + xk 
xk + 1
)2  2 j = j 
4 + 4xk + x2
k  2  4xk  2x2
k 
(xk + 1)2 j = 
= 
1 
(xk + 1)2 j x2
k  2 j 
We also note that xk  1 ) xk+1 = 1+ 1 
xk+1 > 1. So if we start with 
x1 = 1, then xk > 1 for k = 2; 3; : : : This means that 1 
(xk+1)2 < 1 
(1+1)2 = 14 
. 
Thus 
j x2
k+1  2 j < 
1
4 j x2
k  2 j < (
1
4
)2 j x2
k1  2 j < (
1
4
)k j x21
 2 j = (
1
4
)k

160 APPENDIX E. OPTIMIZATION THEORY, WILD-WEST STYLE 
Finally noting that x2
k+1 2 = (xk+1  p2)  (xk+1 + p2), we see that 
j xk+1  p2 j < (
1
4
)k 1 
xk+1 + p2 
< (
1
4
)k 1 
1+ p2 
and that xk approaches p2 with geometric speed as k tends to innity. 
Example: In the example of estimating the parameters vj; pjk; ajl 2 [0; 1] : j; k = 1; : : :;N ; l = 1; : : :;M of a hidden Markov model 
with N states and M signals, we arrived at the following set 
of equations: 
vj = 
vj 
@P 
@vj 
PN
j=1 vj 
@P 
@vj 
for j = 1; : : :;N 
pjk = 
pjk 
@P 
@pjk 
PN
k=1 pjk 
@P 
@pjk 
for j = 1; : : :;N ; k = 1; : : :;N 
ajl = 
ajl 
@P 
@ajl 
PM
l=1 ajl 
@P 
@ajl 
for j = 1; : : :;N ; l = 1; : : :;M 
From this we can construct a set of recurrence equations with 
the nice property that either the next iteration of parameters 
will increase the value of P(O) (the probability of the training 
data) or the value of the parameters will be the same. Since we 
have a lot of subscript indices, we will let ^x denote xk+1 and x 
denote xk. 
^vj = 
vj 
@P 
@vj 
PN
j=1 vj 
@P 
@vj 
for j = 1; : : :;N 
^pjk = 
pjk 
@P 
@pjk 
PN
k=1 pjk 
@P 
@pjk 
for j = 1; : : :;N ; k = 1; : : :;N 
^ajl = 
ajl 
@P 
@ajl 
PM
l=1 ajl 
@P 
@ajl 
for j = 1; : : :;N ; l = 1; : : :;M

Bibliography 
[Abney 1997] Steven Abney. Part-of-Speech Tagging and Partial Parsing. 
In Steve Young and Gerrit Bloothooft (eds.), Corpus-Based Methods in 
Language and Speech Processing. Kluwer, Dordrecht, The Netherlands, 
1997. 
[Aho & Ullman 1972] Alfred V. Aho and Jerey D. Ullman. 1972. The 
Theory of Parsing, Translation and Computing. Prentice-Hall, New jersey. 
[Aho et al 1986] 1986. Alfred V. Aho, Ravi Sethi and Jerey D. Ullman. 
Compilers, Principles, Techniques and Tools, Addison-Wesley. 
[Alshawi & Carter 1994] Hiyan Alshawi and David Carter. 1994. \Training 
and Scaling Preference Functions for Disambiguation". In Computional 
Linguistics 20(4), pp. 635{648. 
[Altmann & Steedmann 1988] Gerry Altmann and Mark Steedman. 1988. 
\Interaction with context during human sentence processing". In Cognition 
30 . 
[Armstrong et al. 1995] Susan Armstrong, Graham Russel, Dominique Petitpierre 
and Gilbert Robert. An Open Architecture for Multilingual 
Text Processing. In Proceedings of the ACL Sigdat Workshop, pages 30 
{ 35, Dublin, Ireland, 1995. 
[Ash 1965] Robert Ash. 1965. Information Theory. John Wiley, New York. 
[Bahl et al 1989] Lalit R. Bahl, Peter F. Brown, Peter V. de Souza and 
Robert L. Mercer. 1992. \A Tree-Based Statistical Language Model 
for Natural Language Speech Recognition". In IEEE Transactions on 
Acoustics, Speech, and Signal Processing 36(7), pp. 1001{1008. 
[Baum 1972] L. E. Baum. 1972. \ An Inequality and Assiciated Maximization 
Technique in Statistical Estimation of Probabilistic Functions of 
Markov Processes. In Inequalities 3 , pp. 1{8. 
[Bellman 1957] R. E. Bellman. 1957. Dynamic Programming. Princeton 
University Press, Princeton, New York. 
[Bies et al. 1995] Ann Bies, Mark Ferguson, Karen Katz and Robert MacIntyre. 
Bracketing Guidelines for Treebank II Style Penn Treebank 
Project. Technical Report, Unniversity of Pennsylvania, 1995. 
[Black et al 1992] Ezra Black, Fred Jelinek, John Laerty, Robert Mercer 
and Salim Roukos. 1992. \Decision Tree Models Applied to the Labeling 
of Text with Part-of-Speech". In Proceedings of the DARPA Speech 
161

162 BIBLIOGRAPHY 
and Natural Language Workshop, pp. 117{121, Morgan Kaufmann, San 
Mateo, California. 
[Black et al 1993] Ezra Black, Fred Jelinek, John Laerty, David M. Magerman, 
Robert Mercer and Salim Roukos. 1993. \Towards History-based 
Grammars: Using Richer Models for Probabilistic Parsing". In Proceedings 
of the 28th Annual Meeting of the Association for Computational 
Linguistics, pp. 31{37, ACL. 
[Black et al. 1996] Ezra Black, Stephen Eubank, Roger Garside, Georey 
Leech, Hideki Kashioka and David Magerman. Beyond Skeleton Parsing: 
Producing a Comprehensive Large-Scale General-English Treebank 
With Full Grammatical Analysis. In The 16th International Conference 
on Computational Linguistics, pages 107 { 113, Kopenhagen, Denmark, 
1996. 
[Bod 1992] Rens Bod. 1992. \A computational model for language performance: 
Data Oriented Parsing". In Proceedings of the 14th International 
Conference on Computational Linguistics, pp. 855{859, ICCL. 
[Bod 1995a] Rens Bod. 1995. \The Problem of Computing the Most Probable 
Parse in Data-Oriented Parsing and Stochastic Tree Grammars". 
In Proceedings of the 7th Conference of the European Chapter of the Association 
for Computational Linguistics, pp. 104{111, ACL. 
[Bod 1995b] Rens Bod. 1995. Enriching Linguistics with Statistics: Performance 
Models of Natural Language. ILLC Dissertation Series 1995-14, 
Amsterdam. 
[Brants & Samuelsson 1995] Thorsten Brants and Christer Samuelsson. 
1995. \Tagging the Teleman Corpus". In Proceedings of the 10th Scandinavian 
Conference on Computational Linguistics, pp. 7{20, University 
of Helsinki. 
[Brill 1992] Eric Brill. 1992. A Simple Rule-Based Part of Speech Tagger. 
In Proceedings of the DARPA Speech and Natural Language Workshop, 
pp. 112{116, Morgan Kaufmann, San Mateo, California. 
[Brill 1993] Eric Brill. 1993. \Automatic grammar induction and parsing 
free text: A transformation-based approach". In Proceedings of the 31st 
Annual Meeting of the Association for Computational Linguistics, pp. 
259{265, ACL. 
[Briscoe & Carroll 1993] Ted Briscoe and John Carroll. 1993. \Generalized 
Probabilistic LR Parsing of Natural Language (Corpora) with 
Unication-Based Grammars". In Computational Linguistics 19(1), pp. 
25{59. 
[Brown et al 1990] Brown et al. 1990. \A statistical approach to machine 
translation". In Computational Linguistics 16(2), pp. 79{85. 
[Brown et al 1991] Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra and Robert Mercer. 1991. \Word-sense disambiguation 
using statistical methods". In Proceedings of the 29th Annual Meeting of 
the Association for Computational Linguistics, pp. 264{270, ACL.

BIBLIOGRAPHY 163 
[Brown et al 1993] Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra and Robert Mercer. 1993. \The Mathematics of Statistical 
Machine Translation: Parameter Estimation". In Computational Linguistics 
19(2), pp. 263{311. 
[Carroll 1994] John Carroll. 1994. \Relating Complexity to Practical Performance 
in Parsing with Wide-Coverage Unication Grammars". In 
Proceedings of the 32nd Annual Meeting of the Association for Computational 
Linguistics, pp. 287{294, ACL. 
[Charniak 1993] Eugene Charniak. 1993. Statistical Language Learning. 
MIT Press. 
[Chinchor et al 1993] Nancy Chinchor, Lynette Hirschman, David D. Lewis. 
1993. \Evaluating Message Understanding Systems: An Analysis of 
the Third Message Understanding Conference (MUC-3)". In Computational 
Linguistics 19(3), pp. 409{449. 
[Church 1988] Kenneth W. Church. 1988. \A Stochastic Parts Program 
and Noun Phrase Parser for Unrestricted Text". In Proceedings of the 
2nd Conference on Applied Natural Language Processing, pp. 136{143, 
ACL. 
[Church 1993] Kenneth W. Church. 1993. \Char align: A Program for 
Aligning Parallel Texts at the Character Level". In Proceedings of the 
31st Annual Meeting of the Association for Computational Linguistics, 
pp. 1{8, ACL. 
[Church 1994] Kenneth Ward Church. Unix for poets. In European Summer 
School on Language and Speech Communication. Corpus-Based Methods. 
Course Notes Vol. 1, Utrecht University, 1994. Research Institute for 
Language and Speech. 
[Church & Gale 1991] Kenneth W. Church and William A. Gale. 1991. 
\A Comparison of the Enhanced Good-Turing and Deleted Estimation 
Methods for Estimating Probabilities of English Bigrams". In Computer 
Speech and Language 19(5). 
[Collins 1996] Michael John Collins. A New Statistical Parser Based on 
Bigram Lexical Dependencies. In Proceedings of the 34th ACL, Santa 
Cruz, CA, 1996. 
[Collins 1997] Michael Collins. Three Generative, Lexicalized Models for 
Statistical Parsing. In Proceedings of the 35th Annual Meeting of the Association 
for Computational Linguistics and 8th Conference of the European 
Chapter of the Association for Computational Linguistics, Madrid, 
1997. 
[Cutting et al 1992] Douglass Cutting, Julian Kupiec, Jan Pedersen and 
Penelope Sibun. 1992. \A Practical Part-of-Speech Tagger". In Proceedings 
of the 3rd Conference on Applied Natural LanguageProcessing, pp. 
133{140, ACL. 
[Daelemans 1994] Walter Daelemans. 1994. \Memory-Based Lexical Acquisition 
and Processing". Lecture Notes in Articial Intelligence, Machine 
Translation and the Lexicon.

164 BIBLIOGRAPHY 
[Dalsgaard 1992] Paul Dalsgaard. 1992. \Phoneme label alignment using 
acoustic-phonetic features and Gaussian probability density functions". 
In Computer Speech and Language 6 . hReally?i 
[Dagan et al 1993] Ido Dagan, Kenneth W. Church and William A. Gale. 
1993. \Robust alignment of bilingual texts". In IEEE 93 . 
[Dagan et al 1993] Ido Dagan, Shaul Marcus and Shaul Markovitch. 1993. 
\ContextualWord Similarity and Estimation from Sparse Data". In Proceedings 
of the 31st Annual Meeting of the Association for Computational 
Linguistics, pp. 164{171, ACL. 
[Dagan et al 1994] Ido Dagan, Fernando Perreira and Lillian Lee. 1994. 
\Similarity-Based Estimation of Word Cooccurrence Probabilities". In 
Proceedings of the 32nd Annual Meeting of the Association for Computational 
Linguistics, pp. 272{278, ACL. 
[Dano 1975] Sven Dano. 1975. Nonlinear and Dynamic Programming. 
Springer, New York. 
[DeGroot 1975] Morris H. DeGroot. 1975. Probability and Statistics. 
Addison-Wesley, Reading, Massachusetts. 
[DeRose 1988] Steven J. DeRose. 1988. \Grammatical Category Disambiguation 
by Statistical Optimization". In Computational Linguistics 
14(1), pp. 31{39. 
[Dreyfuss & Law 1977] Stuart E. Dreyfuss and Averill M. Law. 1977. The 
Art and Theory of Dynamic Programming. Academic Press, New York. 
[Eisner 1996] Jason Eisner. Tree New Probabilistic Models for Dependency 
Parsing: An Exploration. In Proceedings of the 16th International Conference 
on Computational Linguistics, pages 340 { 345, Copenhagen, Denmark, 
1996. 
[Ford et al. 1982] Marilyn Ford, Joan Bresnan and Ronald M. Kaplan. A 
competence-based theory of syntactic closure. In Joan Bresnan (ed.), The 
mental representation of grammatical relations. MIT Press, Cambridge, 
MA., 1982. 
[Forney 1973] G. David Forney, Jr. 1973. \The Viterbi Algorithm". In 
Proceedings of the IEEE 61(3), pp. 268{278. 
[Francis & Kucera 1982] N. W. Francis and H. Kucera. Frequency Analysis 
of English Usage. Houghton Miin, Boston, 1982. 
[Frazier 1978] L. Frazier. 1978. On comprehending sentences: Syntactic 
parsing strategies. Doctoral dissertation, Univ. of Connetticut. 
[Furui 1989] Sadaoki Furui. 1989. Digital Speech Processing, Synthesis, 
and Recognition. Marcel Dekker, New York, New York. 
[Gale & Church 1993] William A. Gale and Kenneth W. Church. 1993. \A 
Program for Aligning Sentences in Bilingual Corpora". In Computational 
Linguistics 19(1), pp. 75{102. 
[Gale et al 1992] William A. Gale, Kenneth W. Church and David Yarowsky. 
1992. \Work on statistical methods for word sense disambiguation". 
In Proceedings from the AAAI Fall Symposium: Probabilistic Approaches 
to Natural Language, pp. 55{60.

BIBLIOGRAPHY 165 
[Good 1953] I. J. Good. 1953. \The Population Frequencies of Species 
and the Estimation of Population Parameters". In Biometrika 40 , pp. 
237{264. 
[Hearst 1991] Marty Hearst. 1991. \Toward noun homonym disambiguation 
using local context in large corpora". ???. 
[Herwijnen 1994] Eric van Herwijnen. 1994. Practical SGML, 2nd Ed. 
Kluwer, Boston/Dordrecht/London. 
[Hindle 1992] Donald Hindle. 1992. \An Analogical Parser for Restricted 
Domanins". In Proceedings of the DARPA Speech and Natural Language 
Workshop, pp. 150{154. 
[Hindle & Rooth 1993] Donald Hindle and Mats Rooth. 1993. \Structural 
Ambiguity and Lexical Relations". In Computational Linguistics 19(1), 
pp. 103{121. 
[Jelinek 1990] Fred Jelinek. 1990. \Self-Organizing Language Models for 
Speech Recognition". In Readings in Speech Recognition, pp. 450{506, 
Morgan Kaufmann, San Mateo, California. 
[Jelinek & Mercer 1980] Frederick Jelinek and Robert L. Mercer. 1980. 
\Interpolated Estimation of Markov Source Paramenters from Sparse 
Data". In Pattern Recognition in Practice, pp. 381{397. North Holland. 
[Jelinek et al 1992] Fred Jelinek, John Laerty, Robert Mercer. 1992. \Basic 
methods of probabilistic context free grammars". In Pietro Laface and 
Renato De Mori Speech Recognition and Understanding. Recent Advances, 
Trends, and Applications, volume f75 of NATO Advanced Science 
Institute Series, pp ??{??. Springer, Berlin. Proceedings of the NATO 
Advanced Study Institute, Cetraro, Italy, 1990. 
[Johansson et al. 1986] Stig Johansson, Eric Atwell, Roger Garside and 
Georey Leech. The Tagged LOB Corpus: User's Manual. Manual, 
Norwegian Computing Centre for the Humanities, Bergen, 1986. 
[Johnson 1991] Mark Johnson. 1991. \The Computational Complexity of 
GLR Parsing". In [Tomita (ed.) 1991], pp. 35{42. 
[Joshi et al 1975] Aravind Joshi, L. Levy and M. Takahashi. 1975. \Tree 
Adjunct Grammars". In Journal of the Computer and System Sciences 
10(1), pp. 136{163. 
[Karlsson 1990] Fred Karlsson. 1990. \Constraint Grammar as a Framework 
for Parsing Running Text". In Proceedings of the 13th International 
Conference on Computational Linguistics, Vol. 3 , pp. 168{173, ICCL. 
[Karlsson et al 1995] Fred Karlsson, Atro Voutilainen, Juha Heikkila and 
Arto Anttila (eds). 1995. Constraint Grammar. A Language-Independent 
System for Parsing Unrestricted Text, Mouton de Gruyter, Berlin / New 
York. 
[Katz 1987] Slava M. Katz. 1987. \Estimation of Probabilities from Sparse 
Data for the Language Model Component of a Speech Recognizer". In 
IEEE Transactions on Acoustics, Speech, and Signal Processing 35(3), 
pp. 400{401.

166 BIBLIOGRAPHY 
[Kay & Roscheisen 1993] Martin Kay and Martin Roscheisen. 1993. \Text- 
Translation Alignment". In Computational Linguistics 19(1), pp. 121{ 
142. 
[Kimball 1973] J. Kimball. 1973. \Seven principles of surface structure 
parsing in natural language". In Cognition 2 . 
[Kipps 1991] James R. Kipp. 1991. \GLR Parsing in Time O(n3)". In 
[Tomita (ed.) 1991], pp. 43{59. 
[Knuth 1965] Donald E. Knuth. 1965. \On the translation of languages 
from left to right". In Information and Control 8(6), pp. 607{639. 
[Konieczny et al. 1997] Lars Konieczny, Barbara Hemforth, Christioph 
Scheepers and Gerd Strube. The role of lexical heads in parsing: evidence 
from German. Language and Cognitive Porcesses, 12(2/3), 1997. 
[Kupiec 1992] Julian Kupiec. 1992. \An algorithm for estimating the parameters 
of unrestricted hidden stochastic context-free grammars". In 
Proceedings of the 14th International Conference on Computational Linguistics, 
pp. 387{393, ICCL. 
[Kupiec 1993] Julian Kupiec. 1993. \An Algorithm for Finding Noun 
Phrase Correspondences in Bilingual Corpora". In Proceedings of the 
31st Annual Meeting of the Association for Computational Linguistics, 
pp. 17{22, ACL. 
[Laerty et al. 1992] John Laerty, Daniel Sleator and Davy Temperley. 
Grammatical Trigrams: A Probabilistic Model of Link Grarmmar. In 
Proceedings of the AAAI Fall Symposion on Probabilistic Approaches to 
Natural Language, 1992. 
[Lehmann et al. 1996] Sabine Lehmann et al. TSNLP { Test Suites for 
Natural Language Processing. In Proceedings of the 16th International 
Conference on Computational Linguistics, pages 711 { 717, Copenhagen, 
Denmark, 1996. 
[Magerman 1995] David M. Magerman. 1995. \Statistical Decistion-Tree 
Models for Parsing". In Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics, pp. 276{283, ACL. 
[Magerman & Marcus 1990] David M. Magerman and Mitchell P. Marcus. 
1990. \Parsing a Natural Language Using Mutual Information Statistics". 
In Proceedings of the 9th National Conference on Artical Intelligence, 
pp. 984{989, AAAI/MIT Press. 
[Magerman & Marcus 1991] David M. Magerman and Mitchell P. Marcus. 
1991. \Pearl: A probabilistic chart parser". In Proceedings of the 5th 
Conference of the European Chapter of the Association for Computational 
Linguistics, pp. 15{20, ACL. 
[Magerman & Weir 1992] David M. Magerman and CarlWeir. 1992. \Probabilistic 
Prediction and Picky Chart Parsing". In Proceedings of the 
DARPA Speech and Natural Language Workshop, pp. 128{133, Morgan 
Kaufmann, San Mateo, California.

BIBLIOGRAPHY 167 
[Manning 1993] Christopher D. Manning. 1993. \Automatic Acquisition 
of a Large Subcategorization Dictionary from Corpora". In Proceedings 
of the 31st Annual Meeting of the Association for Computational Linguistics, 
pp. 235{242, ACL. 
[Marcus 1980] Mitchell P. Marcus. 1980. A Theory of Syntactic Recognition 
for Natural Language. MIT Press. 
[Marcus et al. 1994] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, 
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz and 
Britta Schasberger. The Penn Treebank: Annotating Predicate Argument 
Structure. In Proceedings of the Human Language Technology 
Workshop, San Francisco, March, 1994. Morgan Kaufmann. 
[Mark et al 1992] Kevin Mark, Michael Miller, Ulf Grenander and Steve 
Abney. 1992. \Parameter Estimation for Constrained Context Free 
Language Models". In Proceedings of the DARPA Speech and Natural 
Language Workshop, pp. 146{149, Morgan Kaufmann, San Mateo, California. 
[Merialdo 1994] Bernard Merialdo. 1994. \Tagging English Text with a 
Probabilistic Model". In Computational Linguistics 20(2), pp. 155{171. 
[Monachini and Calzolari 1994] Monica Monachini and Nicoletta Calzolari. 
Synopsis and Comparison of Morphosyntactic Phenomena Encoded in 
Lexicon and Corpora. Technical Report, EAGLES Lexicon Group, 1994. 
[Mood et al 1974] A. Mood, F. Greybill and D. Boes. 1974. Introduction 
to the Theory of Statistics 3rd Ed.. McGraw Hill. 
[MUC-3 1991] 1991. Proceedings of the Third Message Understanding Conference 
(MUC-3). Morgan Kaufmann, San Mateo, California. 
[Nadas 1985] Arthur Nadas. 1985. \On Turing's Formula for Word Probabilities". 
In IEEE Transactions on Aucostics, Speech, and Signal Processing 
33(6), pp. 1414{1416. 
[Osborne & Bridge 1994] Miles Osborne and Derek Bridge. 1994. \Learning 
unication-based grammars using the Spoken English Corpus". In 
Proceedings of the 2nd International Grammatical Inference Colloquim, 
Alicante, Spain. 
[Pearl 1988] Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems. 
Morgan Kaufmann, San Mateo, California. 
[Pereira & Schabes 1992] Fernando Pereira and Yves Schabes. 1992. 
\Inside-outside reestimation from partially bracketed corpora". In Proceedings 
of the DARPA Speech and Natural Language Workshop, pp. 
122{127, Morgan Kaufmann, San Mateo, California. 
[Pereira et al 1993] Fernando Pereira, Naftali Tishby and Lillian Lee. 1993. 
\Distributional Clustering of English Words". In Proceedings of the 31st 
Annual Meeting of the Association for Computational Linguistics, pp. 
183{190, ACL. 
[Pustejovsky 1992] James Pustejovsky. 1992. \The Acquisition of Lexical 
Semantic Knowledge from Large Corpora". In Proceedings of the 
DARPA Speech and Natural Language Workshop, pp. 243{248, Morgan 
Kaufmann, San Mateo, California.

168 BIBLIOGRAPHY 
[Rade & Rudemo 1975] Lennart Rade and Mats Rudemo. 1975. Sannolikhetsl
ara och statistik for teknisk hogskola (In Swedish). Biblioteksf
orlaget, Stockholm, Sweden. 
[Rabiner 1989] Lawrence R. Rabiner. 1989. \A Tutorial on Hidden Markov 
Models and Selected Applications in Speech Recognition". In Proceedings 
of the IEEE 77(2), pp. 257{295. 
[Resnik 1993] Philip Resnik. 1993. \Semantic classes and syntactic ambiguity". 
In Proceedings of the ARPA Workshop on Human Language 
Technology, Morgan Kaufmann, San Mateo, California. 
[Sampson1995] Georey Sampson. English for the Computer. The SUSANNE 
Corpus and Analytic Scheme. Clarendon Press, Oxford, 1995. 
[Samuelsson 1994] Christer Samuelsson. 1994. \Grammar Specialization 
through Entropy Thresholds". In Proceedings of the 32nd Annual Meeting 
of the Association for Computational Linguistics, pp. 88{195, ACL. 
[Samuelsson 1996] Christer Samuelsson. 1996. \Handling Sparse Data by 
Successive Abstraction". In Procs. 16th International Conference on 
Computational Linguistics, pp. 895{900, ICCL, 1996. 
[Santorini 1995] Beatrice Santorini. Part-of-Speech Tagging Guidelines for 
the Penn Treebank Project (3rd Revision, 2nd printing). Technical Report, 
University of Pennsylvania, 1995. 
[Schabes 1992] Yves Schabes. 1992. \Stochastic lexicalized tree-adjoining 
grammars". In Proceedings of the 14th International Conference on Computational 
Linguistics, pp. 426{432, ICCL. 
[Schuetze 1992] Hinrich Schuetze. 1992. \Word sense disambiguation with 
sublexical representations". In Proceedings of the 11th National Conference 
on Artical Intelligence. hReally?i 
[Schuetze 1993] Hinrich Schuetze. 1993. \Part-of-Speech Induction from 
Scratch". In Proceedings of the 31st Annual Meeting of the Association 
for Computational Linguistics, pp. 251{257, ACL. 
[Schuetze 1994] Hinrich Schuetze and Yoram Singer. 1994. \Part-of Speech 
Tagging Using a Variable Memory Markov Model". In Proceedings of the 
32nd Annual Meeting of the Association for Computational Linguistics, 
pp. 181{187, ACL. 
[Sima'an 1995] Khalil Sima'an. 1995. \An optimized algorithm for Data 
Oriented Parsing". In Proceedings of the 1st International Conference on 
Recent Advances in Natural Language Processing, pp. 45{54. 
[Sima'an 1996] Khalil Sima'an. 1996. \Computational Complexity of Probabilistic 
Disambiguations by means of Tree-Grammars". Forthcoming. 
[Skut et al. 1997] Wojciech Skut, Brigitte Krenn, Thorsten Brants and 
Hans Uszkoreit. An Annotation Scheme for Free Word Order Languages. 
In ANLP, Washington, 1997. 
[Smadja 1993] Frank Smadja. 1993. \Retrieving Collocations from Text: 
Xtract". ???.

BIBLIOGRAPHY 169 
[Stolcke & Segal 1994] Andreas Stolke and Jonathan Segal. 1994. \Precise 
N-Gram Probabilities from Stochastic Context-Free Grammars". In 
Proceedings of the 32nd Annual Meeting of the Association for Computational 
Linguistics, pp. 74{79, ACL. 
[Stolcke 1995] Andreas Stolcke. 1995. \An Ecient Probabilistic Contextfree 
Parsing Algorithm that Computes Prex Probabilities". In Computational 
Linguistics 21(2), pp. 165{202. 
[Su et al 1991] Keh-Yih Su, Jong-Nae Wang, Mei-Hui Su and Jing-Shin 
Chang. 1991. \GLR Parsing with Scoring". In [Tomita (ed.) 1991], pp. 
93{102. 
[Taraban & McClelland 1988] Roman Taraban and James L. McClelland. 
1988. \Constituent attachment and thematic role assignment in sentence 
processing: inuences of content-based expectations". In Journal 
of Memory and Language 27 . 
[Tatsuoka 1988] Maurice M. Tatsuoka. 1988. Multivariate Analysis, 2nd 
Ed. Macmillan, New York. 
[Tesniere 1959] L. Tesniere.  Elements de Syntaxe Structurale. Klincksieck, 
Paris, 1959. 
[Teufel 1995a] Simone Teufel. A Support Tool for Tagset Mapping. In 
Proceedings of the ACL Sigdat Workshop, pages 24 { 30, Dublin, Ireland, 
1995. 
[Teufel 1995b] Simone Teufel. A Typed German Incarnation of the 
EAGLES-TS Denition of Lexical Descriptions and Classication Guidelines. 
Technical Report, EAGLES Tagging Group, 1995. 
[Thielen and Schiller 1995] Christine Thielen and Anne Schiller. Ein kleines 
und erweitertes Tagset furs Deutsche. In Tagungsberichte des Arbeitstre
ens Lexikon + Text 17./18. Februar 1994, SchloHohentubingen. 
Lexicographica Series Maior, Tubingen, 1995. Niemeyer. 
[Tomita (ed.) 1991] Matsuru Tomita, editor. 1991. Generalized LR Parsing. 
Kluwer, Boston/Dordrecht/London. 
[Tzoukermann et al 1995] Evelyne Tzoukermann, Dragomir R. Radev and 
William A. Gale. Combining Linguistic Knowledge and Statistical Learning 
in French. In Proceedings of the ACL Sigdat Workshop, pages 51 { 
58, Dublin, Ireland, 1995. 
[Utsuro et al 1992] Takehito Utsuro, Yuji Matsumoto and Makoto Nagao. 
1992. \Lexical Knowledge Acquisition from Bilingual Corpora". In Proceedings 
of the 14th International Conference on Computational Linguistics, 
pp. 581{587, ICCL. 
[Voutilainen 1994] Atro Voutilainen. 1994. Three studies in surfacesyntactic 
analysis of running text. Doctoral Dissertation. Publications 
24, Department of General Linguistics, University of Helsinki. Yliopistopaino. 
[Weischedel et al 1993] Ralph Weischedel, Marie Metter, Richard Schwartz, 
Lance Ramshaw, Je Palmucci. 1993. \Coping with ambiguity 
and unknown words through probabilistic models". In Computational 
Linguistics 19(2), pp. 359{383.

170 BIBLIOGRAPHY 
[Wittemore et al 1992] Greg Wittemore, Kathleen Ferrara and Hans Brunner. 
1990. \Emprical study of predictive powers of simple attachment 
schemes for post-modier prepositional phrases". In Proceedings of the 
28th Annual Meeting of the Association for Computational Linguistics, 
ACL. 
[Wu 1995] Dekai Wu. 1995. \Stochastic Inversion Transduction Grammars, 
with Application to Segmentation, Bracketing, and Alignment of 
Parallel Corpora". In 14th International Joint Conference on Articial 
Intelligence, pp. 1328{1335, Morgan Kaufmann, San Mateo, California. 
[Yarowsky 1992] David Yarowsky. 1992. \Word sense disambiguation using 
statistical models of Roget's Categories trained on large corpora". In 
Proceedings of the 14th International Conference on Computational Linguistics, 
pp. 454{460, ICCL. 
[Young 1993] Steve Young. 1993. \The HTK Hidden Markov Model Toolkit: 
Design and Philosophy". ??. 
[Zipf 1935] G. K. Zipf. 1935. The Psychobiology of Language. Houghton 
Miin, Boston.

